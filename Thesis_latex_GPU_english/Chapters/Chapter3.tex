% Chapter Template

\chapter{Introduction to Domain Wall Dynamics and a Implementation with CUDA} % Main chapter title

\label{Introduction to DW Dynamics} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Introduction to DW Dynamics}}% Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
This chapter is a overview of the theory and experiments behind Dr. Cluadio's work "Domain Wall Dynamics under Nonlocal Spin-Transfer Torque". This is a quantitatively test the effects of spin-diffusion, on real Domain Wall (DW) structures, by numerically implementing the Zhang-LI model into a NiFe soft nanostrip \cite{claudio}. The implementation takes advantage of the highly parallel process capabilities of the GPU.


\section{Theory}



%

We study spin-diffuse effect within a continuously variable magnetization distribution, integrating with micromagenectis with diffuse model of Zhang and LI \cite{claudio}

Spin-transfer torque is a torque that exerts on a magnetization by conduction electron spins, in other words the angular momentum transferred from spins to magnetic  moment \cite{zhang}.

This has simulated reaserch into domain wall (DW) dynamics, particularly those resulting from interactions with current passing through the DW via the phenomenon of spin momemntum transfer (SMT) \cite{handbookspin}

Some application include racetrack technology by fellow IBM scientific Parkin \cite{racetrack}



Contrarily to charge, spin accumulate in metals, The associated diffusion current flows in all directions, giving rise to nonlocal effects, Beyond transport properties, conduction electrons spin resonance and spin pumping provide further testimonies for non-locality in spin transport. These works all refer  to samples consisting in piecewise uniform layers or blocks, magnetic or not. Of special significance to the present work in the non-collinear geometry where a spin current with polarization transverse to the magnetization exists, whose absorption in the vicinity of the surface of a magnetic layer creates a torque on the magnetization, known as spin transfer torque (SFF), 


%Within a nanostip wire 

We Quantitatively test the effects of spin diffusion, on real Domain walls structures, this is done by numerically solve the Zhang-Li model \cite{zhang} into micro-magnetics.
The Zhang Li model refers to:

which is the following equation.

Base on the work of Dr. Claudio \cite{claudio}

At first we investigate the steady-sate velocity regime of DWs in NiFe soft nanostrips. applying current desities similar to those reported in experiments. The results that we are going to obtain 

Experimentally measured spin-diffusion parametres are used, we want to the solution of. 

\begin{equation}
 \frac{\partial \delta \vec{m} }{\partial t} =  D\bigtriangleup \delta \vec{m} + \frac{1}{\tau_{sd}} \vec{m} \times \delta  \vec{m} - \frac{1}{\tau_{sf}}\delta \vec{m} - u \partial_{x}  \vec{m}
\end{equation}


The sample that is considerate is a 300 nm wide and 5 nm tick NiFe soft nanostrip. This dimensions are widely used for experimental use.


Advances in spintronics recognized by 2007 Nobel Prize in Physics have ennable over the last decade advances in computer memory, in hard drives, this is a metal based structures which utilize magnetoresisite effects to save and read data from a magnetic disk. \cite{handbookspin} 


Base on this study numeric applications have been unfold. A interesting application using spintronics is new design for a new memory disk drive called racetrack memory by  Parkin in 2008\cite{racetrack}


Therefore, a simulatenous solution of the diffusive Zhang and Li model togethr with the magnetization dynamics equation has uncovered a qualitativiely new feature of the spin-trasnfer torque effec in the presence of spin diffusion.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\section{Domain Wall Dynamics on the GPU}


The implementation of the GPU of Dr. Claudio is based on launching several kernels into a single GPU node.


\subsection{Rungge and Kutta}


The basic structure is computational solve rungge and kutta of for other.

There exist several computational numeric methods to solver such equations, methods like euler, Midpoint Method and Runge-Kutta integrator method can solve this equations. The RG4 this method is used for the simulation because its numerically more accurate when compared to the others.

The RG4 method differs widely from the Euler method and the Midpoint method. The euler method is the simplest, the derivative at the starting point of each interval is extrapolated to find the next function value, see figure \ref{fig:euler}. The method is only has first order accuracy while RG4 its fourth order integrator.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{Figures/euler.png}
		\rule{35em}{0.5pt}
	\caption[Euler Method]{Euler Method, Is the simplest approximate to solver differential equation or numerically solve equations.}
	\label{fig:euler}
\end{figure}

RK4 goes as follows:

\begin{equation} \label{eq:kg4}
y_{n+1} = y_{n} + 1/6 K_{1} + 1/3 K_{2} +1/3 K_{3} + 1/6 K_{4} 
\end{equation}
where

\begin{align*}
K_{1} &= h \dot f(x_{n}, y_{n}) \\
K_{2} &= h \dot f(x_{n} + h/2, y_{n} + k_{1}/2) \\
K_{3} &= h \dot f(x_{n} + h/2, y_{n} + k_{2}/2) \\
K_{4} &= h \dot f(x_{n} + h, y_{n} + k_{3})
\end{align*}

As the equations shows, each step the derivative is evaluated four times, once at the initial point, twice at trial midpoints, and once at a trial endpoint. From these four values, the final value is calculated, just like the equation is shown \ref{eq:kg4}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/rk4.png}
		\rule{35em}{0.5pt}
	\caption[Fourth-order Runge and Kutta Method]{Fourth-order Runge and Kutta method, Each step the derivative is evaluated four times. }
	\label{fig:kutta}
\end{figure}

\cite{numerical}



%-----------------------------------
%	SUBSECTION 2
%-----------------------------------


\subsection{Kernels}

The GPU implementation. the application reads

At inicialize the applications first it allocates all the CUDA arraries, and C arries.

To allocate a big chunk of memory in the Device 

\begin{lstlisting}[frame=none]
cudaMalloc
\end{lstlisting}
And C with

\begin{lstlisting}[frame=none]
malloc
\end{lstlisting}

In the initialization function it also reads the magenetizacion data from a especific file in this specific case from ``upVW-magn-2.5nm.data''


The initial values for the simulation are

\begin{lstlisting}[frame=none]
#define NX	480		
//Number of cells along direction y
#define NY	120
//Number of cells along direction z
#define NZ	1

//Size of calculation box
#define TX	1200.0
#define TY	300.0
#define TZ	5.0

//Diffusion parameters
#define u_const	1  		//nm/ns
#define D  	1.0e3		//nm^2/ns
#define tau_sd_const 	1.0e-3		//ns
#define tau_sf_const 	25.0e-3		//ns
#define unitsfactor 1e-3	//needed to scale integer arguments to real value

//Threads and array sizes parameteres
#define XTHREADS_PERBLOCK	32
#define	YTHREADS_PERBLOCK	32
\end{lstlisting}


The calculations are divided into two parts, the CPU code and GPU code. Most of the code is on the GPU. On the CPU only minor  process are taken place, like I/O to a .data.
On GPU is were all the computation is happening and simulation.

\subsection{CPU}
In the $initial_calculations$ functions it calculates the terms on magnetization components


the read magnetization data.
This function basically reads data from a .dat file and allocates the memory for each blocks, it reads

The file is divide into two blocks of data, the first block of 57600 rows are the coordinate X and the coordinate Y. Then the next 57600 rows by 3 columns are the magnetization data. Base on the information read the matrices of data is created.

here two data sets are created. The coordinates point data $(x,y)$ and the magnetization data $(x, y, z)$.

Afther this initilization data, the next step is to send this data, that is actually read on the CPU (host) to de GPU(device). 


First we print the Initial and final coordinates that read, this is to ensure that the values ared sucuefully.


\subsection{GPU}


Array are created on the on the Host and sento the Device using

In the type it can be either cudaMemcpyHostToDevice or cudaMemcpyDeviceToHost, depeding, if the memory thats is being copied is sent to the host or to the device.

\begin{lstlisting}[frame=none]
cudaMemcpy(dst, src, size_in_bytes, type);
\end{lstlisting}

after initialization the coordinates points an the magnetization data in the device are done, does values are sent to the GPU, with the function cudaMemcpy() and value set to cudaMemcpyHostToDevice.

In the Initialization of the calculations most of the arrays are filled up with values base on the data read from the .dat magenetization.

\begin{lstlisting}[frame=none]
__global__ void gsource(double *sm_out, double *matrix_in, double u, int grid_width);

__global__ void gm_x_source(double *tempx, double *tempy, double *tempz,
							 double *mx, double *my, double *mz,
							 double *sm_x, double *sm_y, double *sm_z,
							 int grid_width);
\end{lstlisting}


The function gsource

Makes the following calculation of the $double *matrix\_in$ or m


\begin{equation} \label{eq:gsource}
out[i] = (m[i-2] - 8.0*m[i-1] + 8.0*m[i+1] - m[i+2]) * \dfrac{u}{12 * deltaX }
\end{equation}

where

$$deltaX = \frac{TX}{NX}$$

This calculation is done for the arrays read from the .dat file, for dev\_mx, dev\_my and dev\_mz and are saved in a temporary arrays dev\_sm\_x, dev\_sm\_y, and dev\_sm\_z.

The method.

$gm\_x\_source$ calculates the coss producto of the array $m_{xyx}$ and $sm_{xyz}$, this is done twice.

THis data is saved on the arrays $dev_sm_{xyz}$,

Afther launching this two kernels the initial setup is done, the next step is the actual simulation using runge and kutta integrator.


\subsection{KG4}

As seed  in Runge and Kutta section, this method is implementation to numerically solve the differential equation. Intuitivelly the implementation on CUDA code is done with 4 kernls, where each kernel calculates respectivelly the order of the integrator. In the last term calculation is where all the magic occurr, the sum of the previous 3 calculated terms.

\begin{lstlisting}[frame=none]
__global__ void gterm1_RK1( . . .);
__global__ void gterm2_RK2( . . .);
__global__ void gterm3_RK3( . . .);
__global__ void gterm4_RK4( . . .);
\end{lstlisting}

Between each term calculation of RG4 laplacian calculation kernels are launched.

\begin{lstlisting}[frame=none]
__global__ void glaplacianx( . . . );
__global__ void glaplacianyboundaries( . . . );
__global__ void glaplaciany( . . . );
\end{lstlisting}

The final kernel is launched $void gterm4_RK4()$ obtain the array $deltam_{xyz}$, which is the final result of the RK4 integrator. This array is sent to the last step.

\subsection{effective values}

When the rg4 integretaor is done effective values are calculatated, this values sirve the porpese of calculation  the.


\begin{lstlisting}[frame=none]
__global__ void gm_x_sm( . . . );
__global__ void gu_eff( . . . );
__global__ void gu_eff_beta_eff( . . . );
__global__ void gbeta_eff( . . . );
__global__ void gbeta_diff( . . . );
\end{lstlisting}

The last kernel $ void gbeta\_diff( . . . );$ is where the two final arrays are obtain,
which then are sent to the CPU for the final calculation.

The final calculation is just the sum of all the elements of $beta_diff_num$ and $beta_diff_den$, there divided.
This final single values tells us...


This is the final step of the simulations this is where $beta\_diff$ is obtained. 

The final data is saved
\subsection{time}

When the simulation is done, it will repeat the process intil the values converges.


\section{Validation}

The validate the code, that is obtained from the simulation

Once obtain the results from the simulation, the results are saved into two seperated data sets. .eff and .spin. depending of the configuration of the application is possible to obtain the uVW or the. Because CUDA framework is highly parallel system is farly easy to obtain errenois data from the calculations, even setting up the threads per block incorrectly is possible to get data set that a wrong, or results that don't diverge. It is necessary that when finishing making changes to the code validating the results with a valid data set is done.

The validation is done by checking the output the simulation with a valid data set, the output of the validation application tells us the error factor of the current data with the valid set. So for each data set there is a threshold value, that can tell if the that is close enough to the results. A example of the validation performed.


\section{Help Kernels}


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Data Flow}


The initial data flow of the kernels goes as follow, Fi
