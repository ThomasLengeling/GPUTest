% Chapter Template

\chapter{Implementation of Domain Wall Dynamics under Nonlocal STT} % Main chapter title

\label{Implementation of Domain Wall Dynamics under Nonlocal STT} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Implementation of Domain Wall Dynamics under Nonlocal STT}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

The following chapter is the study of the heterogeneous computing implementation of the Domain Wall Dynamics under Nonlocal Spin-Transfer Torque. We use the massively parallel capabilities of a single GPU to numerically solve a mathematical equation, known as the Zhang-Li model. The numerical method used for the solution is method known as Finite Differences in the Time Domain (FDTD) whose integration is done using the 4th order Runge-Kutta. The integration is done on a 3D grid space which outputs the magnetization data of the Vortex Wall, Asymmetric Transverse Wall and a single value, the effective beta.

\section{Simulation}

The simulation consists in integrating the Zhang-Li model \ref{eq:zhang} using the 4th order integrator Runge-Kutta, which is done for a 3D grid space of 57,600 cells. The sample considered is a 300nm wide in y direction and 5nm thick in z direction. Furthermore, the sample is a soft nanostrip composed of NiFe, a material and size widely used in experiments. Using this size the asymmetric transverse wall \ref{fig:atw}, and the vortex wall have nearly equal energies. The numerical mesh size is 3 x 3 x 5 $nm^3$, and the calculation box has a length (x direction) of 1,200 or 3,172 nm. The table \ref{tab:mesh} illustrates mesh information and calculation box for the simulation \cite{claudio}. In addition, the table \ref{tab:drk} shows the constant values for the numerical solution of the equation \ref{eq:zhang} such as $\mu$, $D_{0}$, $\tau_{sd}$ and $\tau_{sf}$.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
Mesh size & value & calculation box & value \\
\hline
Cell NX & 480 & Box TX  & 1200.0   \\
\hline
Cell NY & 120 & Box TY  & 300.0  \\
\hline
Cell NZ &	1 & Box TZ  & 5.0   \\
\hline
\end{tabular}
\caption{Mesh size and calculation box}
\label{tab:mesh}
\end{table}

The simulation and code is divided into main two calculations elements, the host part and the device part. The figure \ref{fig:flow} illustrates the data flow between the host and the device, but as well as a general description of the simulation. Based on the data flow figure, the simulation starts by reading the magnetization values from a data file. Then, the data set is used to calculate the initial magnetization matrices for the simulation. Afterwards, the simulation begins with the RK4 integration, hence, numerically solves the Zhang-Li model \ref{eq:zhang}. The simulation is configured to integrate 50,000 times the Zhang-Li Model before calculating the final effective Beta value. The simulation stops if the effective beta or convergence to 1.0e-9. The figure \ref{fig:flow} shows the control flow of the simulation. But also illustrates the workload on the host and the device. As we can see the heavy computation operations are done on the GPU side. While on the CPU only minor intense computation are done such as I/O data, memory allocation and final beta variation. Finally, \ref{log:rk4} is the pseudo-code of the RK4 integration.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
Diffusion parameters& Value & Runge - Kutta 4th & Value \\
\hline 
$\mu$ & 1 &  time step (dt) &   $25.0e^{-6}$   \\
\hline
$D_{0}$ & $1.0e^{3}$ nm mm$^2$/ns  & tmax  & 1.0  \\
\hline
$\tau_{sd}$ & $1.0e^{-3}$ ns  & beta difference & $1.0e^{-9}$ \\
\hline
$\tau_{sf}$ & $25.0e^{-3}$ ns  & Iterations & 50,000 \\
\hline
\end{tabular}
\caption{Diffusion parameters and Runge-Kutta 4th}
\label{tab:drk}
\end{table}

\subsection{Data allocation and threads}

The first section of the implementation begins with allocation data into several matrices for both host memory and device memory. The initial magnetization data can be either self generated by the application or by reading a file which contains information related to the magnetization. In both cases the data are divided into two blocks of information. Blocks have 57600 (480 x 120) rows of information The first 57600 rows contains initial magnetization x, y coordinates. The next block of 57600 rows is the initial magnetization in x, y, and z. The data being read is stored on a three temporary 2D matrices. Therefore, corresponds to x, y, and z coordinates. Next, the three matrices are flattende into three continuous memory blocks, as shown in figure \ref{fig:flaten}. The device code \ref{lst:flatten} flattens the 2d index into a single linear 1D index.

\begin{lstlisting}[language=C++, label={lst:flatten}, caption={Kernel Flatten from a 2 value index to a single value index.}]	
    int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
     // map the two 2D indices to a single linear, 1D index
    int index = j * grid_width + i; 
\end{lstlisting}


To ensure optimal memory allocation on the GPU side is best to assign square matrices to the device. Using square memory sizes the GPU allocates efficiently the memory into the SMs and threads. The calculation for this operation is done by using the first two operations in the code \ref{lst:blocks}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.55\textwidth]{Figures/flow.png}
		\smallskip
	\caption[Control flow]{Control flow of the simulation for the CPU and GPU.}
	\label{fig:flow}
\end{figure}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.93\textwidth]{Figures/block.png}
		\smallskip
	\caption[Grid layout]{Memory allocation in terms of the blocks per threads and grids.}
	\label{fig:block}
\end{figure}


\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l | }
\hline
Matrix size X & 480 & Device allocation X & 512\\
\hline
Matrix size Y & 120 & Device allocation Y & 128 \\
\hline
\end{tabular}
\caption{Matrix allocation size}
\label{tab:cuda}
\end{table}

The magnetization data is stored on three matrices x, y, and z, each one of them with a capacity of 56,700 values, in other words 480 times 120. Base on the information we want to calculate the optimal number of grids that will ensure a complete use of the hardware resources. The number of blocks per grid corresponds dividing the dimensions of the array by the number of threads. The last two operations in \ref{lst:blocks}.

\begin{lstlisting}[language=C++, label={lst:blocks}, caption={Device capacity calculation and number of block per grid}]	
NXCUDA = (int)powf(2,ceilf(logf(NX)/logf(2)));
NYCUDA = (int)powf(2,ceilf(logf(NY)/logf(2)));

//Setup optimum number of blocks
XBLOCKS_PERGRID = (int)ceil((float)NX/(float)XTHREADS_PERBLOCK); 
YBLOCKS_PERGRID = (int)ceil((float)NY/(float)YTHREADS_PERBLOCK);
\end{lstlisting}

Depending on the hardware properties, each GPU is able to allocate different number of threads per block and as well as different shared memory sizes. The Shared memory in this implementation relies on the number of threads per block. In addition, the number of blocks depends on the input matrix and the number of threads. Examine figure \ref{fig:block}. More information about the optimal number of threads per block read can be found in the final chapter \ref{Optimization Results}. 

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l |}
\hline
 & Fermin & Kepler \\
\hline
Threads per block X  & 16 & 32 \\
\hline
Threads per block Y  & 16 & 32 \\
\hline
Number of blocks X & 30 & 15 \\
\hline
Number of blocks Y & 8 & 4 \\
\hline
Shared memory & 16 * 16 & 32 * 32 \\
\hline
\end{tabular}
\caption{Threads, blocks size}
\label{tab:threads}
\end{table}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.55\textwidth]{Figures/flaten.png}
		\smallskip
	\caption[2D Flatten array]{Converting 2D array to a single continuous block of memory}
	\label{fig:flaten}
\end{figure}


\subsection{Initial Calculations}

Before applying the RK4, we need to evaluate static matrices that will not change over the curse of the RK4 algorithm \ref{log:rk4}. The initial calculations are for the therm $ \delta \vec{m}$ of the equation \ref{eq:zhang} (chapter \ref{Implementation of Domain Wall Dynamics under Nonlocal STT}). The term is the non-equilibrium spin density domain wall at rest \cite{claudio}. The Following equations evaluates the first steps of the magnetization data \ref{lst:init}.

\begin{lstlisting}[language=C++, label={lst:init}, caption={Initial calculations}]
	//Compute x, y and z component of source term
    gsource << <blocks, threads >> >(...);
    gsource << <blocks, threads >> >(...);
    gsource << <blocks, threads >> >(...);

    //Project source term on magnetization components by computing
    //a cross product twice
    gm_x_source << <blocks, threads >> >(...);
    gm_x_source << <blocks, threads >> >(...);
\end{lstlisting}

The kernel \textit{gsource} from \ref{lst:init} are evaluated only once,  resulting with the matrix \textit{sm}, which is used to compute the Zhang-Li model in the following RK4 integration. 

Mentioned before the matrices for CUDA are square matrices of 512 x 128. However, the data set is 480 x 120. Therefor, we need to limit the number of threads inside every kernel. To limit the executing of the extra threads a simple $if$ within each kernel solves the issue \ref{lst:if}. Nonetheless, due to the boundary condition of the FDTD method we need to add a small shift of 2 indices in the x direction.

\begin{lstlisting}[language=C++, label={lst:if}, caption={Laplacian X using global memory}]
    if (i > 1 && i < NX + 2 && j >= 0 && j < NY){
    	//calculations
    }
\end{lstlisting}

\subsection{Numerical Methods}

The following steps is where all the computational intense operations are performed. The algorithm \ref{log:rk4} demonstrates the operations necessary for RK4 integration process. In addition, the operations are evaluated once per iteration, composed by four time steps of the RK4, furthermore, solving the Zhang-Li model using  FDTD. The algorithm exists when $b_{eff}$ reaches an numeric error of $1.0e^{-9}$.

\begin{algorithm}[H]
 \KwData{deltam, sfrelax, sdex, sm, laplacian}
 \KwResult{deltam}
 data initialization\;
 \While{$b_{eff} < 1.0e^{-9}$}{
  Runge and Kutta 4th\;
  \For{$i = 1; i <= 4; i+=1$ }{  
  sdex $\gets$ crossProduct(deltam, mag); calculate cross product \\
  FDTD with boundary condition\\
  laplacian $\gets$ laplacianXYBoundary(deltam);\\
  evaluate Zhang-Li model \\
  zhangLi  $\gets$ solveZhang(sfrelax, sdex, Laplacian, sm); \\
  
  RK4 evaluation \\
  rkterm(i) $\gets$  rktime(i, solveZhangLi, dt); \\
  
  \eIf {$i == 4$}{
    deltam $\gets$ rk4(zhangLi, tmp, dt, rkterm(1),rkterm(2), rkterm(3),rkterm(4))
    }{
	deltam $\gets$ rk4(zhangLi, tmp, dt)\\
   }
  evaluate RK4 term\\
  deltam $\gets$ rk4(zhangLi, tmp, dt) \\
  sfrelax $\gets$ relaxation(deltam, tau) \\
  
  \If {$i == 4$}{
    tmp $\gets$ copy(rkterm(4));\\
   }
  }
  $b_{eff}$ = calculate(temp, beta);
 }
 \label{log:rk4}
 \caption{Runge and Kutta 4th integration implementation}
\end{algorithm}

\subsubsection{Finite differences in the time domain}

The finite differences method requires the domain of interest to be broken down into small regions. Such subdivision of space is known as mesh, grid or cell division. The cell division is illustrated in the table \ref{tab:mesh} from the previous section. In addition, the number of threads per block of the CUDA code implementation is based on the FDTD cell division. The separation is illustrated in table \ref{tab:cuda} and \ref{tab:threads}, which are based on the equation \ref{eq:nn} from chapter \ref{Introduction to Domain Wall Dynamics under Nonlocal STT}. To numerically solve the Zhang-Li model \ref{eq:zhang} we need to determinate and evaluate the first and second derivate. Therefore accomplish using the FDTD method. The proposed solution is based on the seconds nearest neighbors. The basic idea is shown on the figure \label{fig:laplacian}. Moreover, the equation \ref{eq:nn} is evaluated for the x, y and z coordinates.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/laplacian.png}
		\smallskip
	\caption[Laplacian block calculation]{Laplacian XY block calculation and boundaries condition}
	\label{fig:laplacian}
\end{figure}

The figure \ref{fig:laplacian} demonstrates the calculation of the nearest neighbors expansion by evaluation a neighborhood of $[-2, 2]$ values in the x direction. In addition, the index begins at $i = 2$ and finishes at $NX + 2$. The same occurs for the Laplacian in the Y direction, however, begins at $j = 2$ and finish at $NY -2$. The implementation \ref{lst:lpay} uses the equation from chapter 3 \ref{eq:nn} as previous mention. The calculation \ref{lst:lpay} is done for each three x, y and z coordinates. 

\begin{lstlisting}[language=C++, label={lst:lpay}, caption={Laplacian X using global memory}]
int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
int j = blockIdx.y * blockDim.y + threadIdx.y;
// map the two 2D indices to a single linear, 1D index
int idx = j * grid_width + i;

lapy[idx] = - deltam[idx + 2] / 12.0 + 4.0 * deltam[idx + 1] / 3.0
			- 5.0 * deltam[idx] / 2.0
			- deltam[idx - 2] / 12.0 + 4.0 * deltam[idx - 1] / 3.0;	
\end{lstlisting}


However, the calculation of the nearest neighbors only works for values of points inside a grid of [-2, 2] for x, y, and z coordinates. In addition, we need to calculate the boundaries values for those points that are in the edge of the grid, such as the beginning points and the end points. In other words, close to the magnetization boundaries. To solve the boundaries issue we use two square matrices matrices from chapter \ref{Introduction to Domain Wall Dynamics under Nonlocal STT}.

 The matrices \ref{eq:matrix4} and \ref{eq:matrix3} are used for evaluating the values near the matrix borders. The matrices boundaries at $j = 0$ and $j = NY - 1$ for all values in the $i$ cell, the matrix \ref{eq:matrix3} is evaluates. For the condition $j = 1$ and $j = NY - 2$ for all $i$ values, we used the matrix \ref{eq:matrix4}. The code \ref{lst:lap} demonstrate how boundary condition was implementation. To obtain the correct values, first the laplacian in the y direction is calculated, then the boundary condition in the x direction. 

\begin{lstlisting}[language=C++, label={lst:lap}, caption={Evaluation of Laplacian X, Y with boundary condition}]
__global__ void glaplaciany(...){} //Compute laplacian in Y direction

__global__ void glaplacianBoundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries Equation 3.10
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries Equation 3.9
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries Equation 3.9
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries Equation 3.10
    }
}
__global__ void glaplacianx(...){} //Compute laplacian in X direction
\end{lstlisting}

The three CUDA kernels \textit{glaplaciany}, \textit{glaplacianx} and \textit{glaplaciany} are evaluated for each x, y and z coordinate. In addition, the three coordinate sum up to 172,800 cell points to be calculated.

\subsubsection{Zhang and Li Model}

The Zhang-Li Model \ref{eq:zhang} is solved using code \ref{lst:rkcuda}. Furthermore, the Zhang-Li equation is used as function $f$ for the RK4 integration process \ref{eq:rk4}. The first term of the equation \textit{sfrelax} is calculated in the relaxation process at the end of the RK4 integrator \ref{fig:sim}. Then the term \textit{sdex} is computed in the Cross Product process, which is done before the RK4 calculation. The matrix \textit{sm} is only calculated once at the initial calculations process, kernel \textit{gsource} in the code \ref{lst:init}. Finally, the \textit{lapl} matrix is evaluated in the Laplacian process. Moreover, computing the Laplacian for x, y coordinate of the CUDA memory grid. The same process evaluates the Laplacian boundary condition, using the FDTD method.

\begin{lstlisting}[language=C++, label={lst:zhangcuda}, caption={Runge and Kutta 4th Terms}]
		sfrelax[idx] = -deltam[idx] / tau_sf;
		sdex[idx] = -(deltam[idx] * m[index] - deltam[idx] * m[idx]) / tau_sd;
		
		//Evaluate Zhang - Li Method
        solveZhangLi[idx] = sfrelax[idx] + sdex[idx] + lapl[idx] - sm[idx];
\end{lstlisting}

Once completed the evaluation of the Zhang-Li Model (listing \ref{lst:zhangcuda}), The matrix \textit{solveZhangLi} is assigned to each one of the RK4 terms evaluation.

\subsubsection{Runge and Kutta}
 
Intuitively the equation \ref{eq:rk4} is implemented by using four CUDA kernels, each kernel calculates the step of the integration. The first part of \ref{lst:rkcuda} computes the Runge and Kutta's 1st, 2nd, and 3rd  term \ref{eq:rksplit}. The final fourth term, is the sum of the previous 3 terms, the last two lines of code in \ref{lst:rkcuda} show the calculation.

\begin{lstlisting}[language=C++, label={lst:rkcuda}, caption={Runge and Kutta 4th Terms}]
     rk1[idx] = dt * deltam[idx];  // Terms k1, k2, k3
     deltam[idx] = temp[idx] + 0.5 * rk1[idx];
     
     rk4[idx] = dt * deltam[idx]; //final k4
     deltam[idx] = temp[idx] + (rk1[idx] + 2.0 * (rk2[idx] + rk3[idx])
                                  + rk4[idx]) / 6.0;
\end{lstlisting}

On each RK4 step the Zhang-Li model is evaluated, Moreover, numerically solving the model with the FDTD method including the magnetization boundaries condition.

The integration is group up in two \textit{for} cycles (listing\ref{lst:rk4}). The inner \textit{for} cycle evaluates the RK4 for the x, y, z coordinate. The outer for cycle evaluates four times the Runge and Kutta integration.

\begin{lstlisting}[language=C++, label={lst:rk4}, caption={Summarize of Runge and Kutta 4th Integration}]	

for(int term = 0; term < 4; term++)
	for(int coord = 0; coord < 3; coord++)
    	gsd_exchange<<<blocks, threads>>>(term, coord);
    	glaplacianx<<<blocks, threads>>>(term, coord);
    	glaplacianyboundaries<<<blocks, threads>>>(term, coord);
    	glaplaciany<<<blocks, threads>>>(term, coord);
    	gsolution<<<blocks, threads >>>(term, coord);
    	gterm_RK4<<<blocks, threads >>>(term, coord);
}
\end{lstlisting}

 Consequently the simulation is set to 50,000 iterations of the RK4 integretor, as the data flow illustrates  \ref{fig:flow}. However, is possible for the simulation diverge, and yield to inconsistent results.
 
\subsection{Calculate effective beta}

The RK4 algorithm \ref{log:rk4} will finish until the beta evaluation reaches the value of $1.0e^{-9}$. The final step of the simulation determinate the effective beta, which tells use the energy configuration of the system. But more importantly the spin diffusion. Furthermore, the DW velocity the diffusion non-adiabatic term $\beta$.

The kernels \ref{lst:beta} are launched only when the RK4 integration is done evaluating. The effective beta kernels computes a single value, the effective beta.

\begin{lstlisting}[language=C++, label={lst:beta}, caption={Calculate effective beta}]
    gm_x_sm << <blocks, threads >> >(...); //Calculate
    gu_eff << <blocks, threads >> >(...);  //Calculate 
    gu_eff_beta_eff << <blocks, threads >> >(...); 	//Calculate
    gbeta_eff << <blocks, threads >> >(..); 	//Calculate
    gbeta_diff << <blocks, threads >> >(...);  //Calculate             
\end{lstlisting}


We do not know for how long we need to integrate the system. However, it will stop until the local energy reaches the minimum configuration. Hence, the effective beta diverges to $1^{-9}$. The simulation will stop when the effective beta reaches the minimum. Then the magnetization data is written. Depending on the application configuration is possible to write either the magnetization results for the VW or for the ATW. The magnetization data is written into two separated data files, which contains the effective data .eff and the spin accumulation data .spin.

\section{Validation}

Because CUDA framework is a highly parallel system is fairly easy to obtain erroneous data from the calculations, even setting up the threads per block incorrectly is possible to get a data set that is wrong, or results that don't diverge. When making changes to the code, it is necessary to validate the new code.

The validation is done by comparing the output of the simulation with a valid data set. The output of the validation application tells us the error factor of the current data with the valid set. So for each data set there is a threshold value, that can tell if the that is close enough to the results. An example of the validation performed.

According to our results, new code shouldn't produce errors in the spin.dat data greater than $7.0^{-17}$, in other words valid code don't lead to differences greater than the precision expected from computations with double precision $1.0^{-16}$ in the case of eff data the errors are in the order of $1.0^{e-11}$ and no greater than $6^{-11}$. For the diffuse beta variation the precision expected to be within the double precision range of $1^{-16}$. 

The initial implementation results were done using the GeForce GT 650M, with 384 CUDA core at 745 MHz and 2GB GDDR5 of memory. The final simulation with a correct validation data outputs the following results \ref{tab:results}. The 1.00x speedup is used for comparing optimization results in the final chapter \ref{Optimization Results}.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
\multicolumn{4}{|c|}{GeForce GT 650M  384 CUDA} \\
\hline
Data set & Simulation time & Speedup & Diffuse beta  \\
\hline
upVW magnetization & 377590.3ms & 1.00x & 4.848728452719814e-02 \\
\hline
ATWpm magnetization & 377409.2ms & 1.00x & 4.054674178687585e-02 \\
\hline
\end{tabular}
\caption{Calculation results}
\label{tab:results}
\end{table}


\vspace{4.0em}

To conclude, the simulating at its core uses the RK4 for integration which uses as integration function the Zhang-li model equation \ref{eq:zhang}. In addition, to solve such differential equations of the Zhang-Li model the FDTD method is evaluated. For each iteration the RK4 \ref{eq:rk4} evaluates fourth times the Zhang-Li equation. Moreover, each RK4 term evaluates numerically the Laplacian \ref{eq:nn} with boundary conditions \ref{eq:matrix3} \ref{eq:matrix4}. We showed the procedure of numerically solving the equations from Chapter \ref{Implementation of Domain Wall Dynamics under Nonlocal STT}.



