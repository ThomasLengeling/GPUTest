% Chapter Template

\chapter{Introduction to Domain Wall Dynamics and a Implementation with CUDA} % Main chapter title

\label{Introduction to DW Dynamics} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Introduction to DW Dynamics}}% Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
This chapter is a overview of the theory and experiments behind Dr. Cluadio's work "Domain Wall Dynamics under Nonlocal Spin-Transfer Torque". This is a quantitatively test the effects of spin-diffusion, on real Domain Wall (DW) structures, by numerically implementing the Zhang-LI model into a NiFe soft nanostrip \cite{claudio}. The implementation takes advantage of the highly parallel process capabilities of the GPU.


\section{Theory}

Spintronics is a new type of electronics that exploits the spin degree of freedom of an electron in addition to its charge. It is expected that electronics technology and devices will be faster, compacter and more energy-saving \cite{spinz}

\cite{ferro}

An abrupt in magnetization at the boundary of two anti-aligned domains is not a favorable condition. Domain walls form between suc domains as means of minimizing the energy of the two anti-aligned domains. Domains walls are transitions layers in which the magnetization changes gradually from on magnetization to another.  The gradual change prevents the large increase in exchange energy that would accompany an abrupt change in the magnetization angle. \cite{spindomain}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/vortex.png}
		\rule{35em}{0.5pt}
	\caption[Domain Wall - Vortex]{Domain Wall - Vortex}
	\label{fig:vortex}
\end{figure}

The study spin-diffuse effect within a continuously variable magnetization distribution, integrating with micromagenectis with diffuse model of Zhang and LI \cite{claudio}

Numerical Methods
\cite{methods}


Spin-transfer torque is a torque that exerts on a magnetization by conduction electron spins, in other words the angular momentum transferred from spins to magnetic  moment \cite{zhang}.

This has simulated research into domain wall (DW) dynamics, particularly those resulting from interactions with current passing through the DW via the phenomenon of spin momentum transfer (SMT) \cite{handbookspin}





Contrarily to charge, spin accumulate in metals, The associated diffusion current flows in all directions, giving rise to nonlocal effects, Beyond transport properties, conduction electrons spin resonance and spin pumping provide further testimonies for non-locality in spin transport. These works all refer  to samples consisting in piecewise uniform layers or blocks, magnetic or not. Of special significance to the present work in the non-collinear geometry where a spin current with polarization transverse to the magnetization exists, whose absorption in the vicinity of the surface of a magnetic layer creates a torque on the magnetization, known as spin transfer torque (SFF), 


%Within a nanostip wire 

We Quantitatively test the effects of spin diffusion, on real Domain walls structures, this is done by numerically solve the Zhang-Li model \cite{zhang} into micro-magnetics.
The Zhang Li model refers to:

which is the following equation.

Base on the work of Dr. Claudio \cite{claudio}

At first we investigate the steady-sate velocity regime of DWs in NiFe soft nanostrips. applying current densities similar to those reported in experiments. The results that we are going to obtain 

Experimentally measured spin-diffusion parameters are used, we want to the solution of. 

\begin{equation}
 \frac{\partial \delta \vec{m} }{\partial t} =  D\bigtriangleup \delta \vec{m} + \frac{1}{\tau_{sd}} \vec{m} \times \delta  \vec{m} - \frac{1}{\tau_{sf}}\delta \vec{m} - u \partial_{x}  \vec{m}
\end{equation}


The sample that is considerate is a 300 nm wide and 5 nm tick NiFe soft nanostrip. This dimensions are widely used for experimental use.

Therefore, a simultaneous solution of the diffusive Zhang and Li model together with the magnetization dynamics equation has uncovered a qualitatively new feature of the spin-transfer torque effect in the presence of spin diffusion.


Advances in spintronics recognized by 2007 Nobel Prize in Physics have enable over the last decade advances in computer memory, in hard drives, this is a metal based structures which utilize magnetoresisite effects to save and read data from a magnetic disk. \cite{handbookspin} 

Some application include racetrack technology by the IBM fellow scientific Parkin \cite{racetrack}

Base on this study numeric applications have been unfold. A interesting application using spintronics is new design for a new memory disk drive called racetrack memory by  Parkin in 2008\cite{racetrack}


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\section{Domain Wall Dynamics on the GPU}


The implementation of the GPU of Dr. Claudio is based on launching several kernels into a single GPU node.


The differential evaluation in one-dimention,
The Second order Taylor expansion readily yields expressions for the first and seconds central derivates

$$ \dfrac{df}{dx} = \dfrac{f_{i+1} - f_{i-1} }{2a}$$

and

$$ \dfrac{d^{2}f}{dx^{2}} = \dfrac{f_{i+1} - 2f_{i}+f_{i-1} }{a^2}$$

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.4\textwidth]{Figures/taylor.png}
		\rule{35em}{0.2pt}
	\caption[Sampled at regular intervals a, Taylor expansion]{Sampled at regular intervals a}
	\label{fig:taylor}
\end{figure}

Taylor expasion of the function $f(x)$ around $x=x_i$ yields where $f^{(k)}(x_i) = f(x)$ if $k=0$

$$f(x) = \sum\limits_{k=0}^{\infty} \dfrac{(x-x_i)^k}{k!}f^{(k)}(x_i) = \sum\limits_{k=0}^{\infty} \dfrac{(x-x_i)^k}{k!}f^{(k)}$$

Applying the previous equation to nearest and next neares neigborar to grid pint i and tructation tht the 4th order yields a set of four equations:

\begin{align}
\begin{bmatrix}
    -2a & \dfrac{(-2a)^2}{2!} & \dfrac{-(2a)^3}{3!} & \dfrac{(-2a)^4}{4!}\\
    -a & \dfrac{(-a)^2}{2!} & \dfrac{(-a)^3}{3!} & \dfrac{(-a)^4}{4!}\\
    a & \dfrac{(a)^2}{2!} & \dfrac{(a)^3}{3!} & \dfrac{(a)^4}{4!}\\
    2a & \dfrac{(2a)^2}{2!} & \dfrac{(2a)^3}{3!} & \dfrac{(2a)^4}{4!}
\end{bmatrix}
\begin{bmatrix}
    f_i^{(1)}  \vphantom{ \dfrac{d^4}{d} }\\
    f_i^{(2)}  \vphantom{ \dfrac{d^4}{d} } \\
    f_i^{(3)}  \vphantom{ \dfrac{d^4}{d} } \\
    f_i^{(4)}  \vphantom{ \dfrac{d^4}{d} }
\end{bmatrix}
=
\begin{bmatrix}
    f_{i-2} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i-1} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i+1} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i+2} - f_{i}    \vphantom{ \dfrac{d^4}{d}} 
\end{bmatrix}
\end{align}

The set of linear equations provide numerical estimates for the first, second, third and fourth derivatives of $f$ at any given point $i$.

The general form of the first and second derivate based on second nearest neighbors expansion reads:

\begin{align*}
f^{(1)}_i &= \dfrac{f_{i-2}-8f_{i-1} + 8f_{i+1} - f_{i+2}}{12a} \\
f^{(2)}_i &= \dfrac{f_{i-2}+16f_{i-1} -30f_{i} + 16f_{i+1} - f_{i+1}}{12a^2}
\end{align*}


\begin{align}
\begin{bmatrix}
    -2a & \dfrac{(-2a)^2}{2!} & \dfrac{-(2a)^3}{3!} & \dfrac{(-2a)^4}{4!}\\
    -a & \dfrac{(-a)^2}{2!} & \dfrac{(-a)^3}{3!} & \dfrac{(-a)^4}{4!}\\
    a & \dfrac{(a)^2}{2!} & \dfrac{(a)^3}{3!} & \dfrac{(a)^4}{4!}\\
    1 & \dfrac{(3a)}{2} & \dfrac{(3a/2)^3}{3!} & \dfrac{(3a/2)^4}{4!}
\end{bmatrix}
\begin{bmatrix}
    f_i^{(1)}  \vphantom{ \dfrac{d^4}{d} }\\
    f_i^{(2)}  \vphantom{ \dfrac{d^4}{d} } \\
    f_i^{(3)}  \vphantom{ \dfrac{d^4}{d} } \\
    f_i^{(4)}  \vphantom{ \dfrac{d^4}{d} }
\end{bmatrix}
=
\begin{bmatrix}
    f_{i-2} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i-1} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i+1} - f_{i}    \vphantom{ \dfrac{d^4}{d}} \\
    f_{i+2}(x_R)   \vphantom{ \dfrac{d^4}{d}} 
\end{bmatrix}
\end{align}
     

\cite{methods}



Modern numerical algorithms for the solution of ordinary differential equations are also base on the method of the Taylor series. Each algorithm such as Runge-Kutta method are constructed so they give an expression depending of the parameter $(h)$, in other works the step as an approximate solution of the first terms of the Taylor series.

\subsection{Runge and Kutta}

There exist several computational numeric methods to solver such equations, methods like Euler, Midpoint Method and Runge-Kutta integrator method can solve this equations. The RG4 this method is used for the simulation because its numerically more accurate when compared to the others.

The RG4 method differs widely from the Euler method and the Midpoint method. The euler method is the simplest, the derivative at the starting point of each interval is extrapolated to find the next function value, see figure \ref{fig:euler}. The method is only has first order accuracy while RG4 its fourth order integrator.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{Figures/euler.png}
		\rule{35em}{0.5pt}
	\caption[Euler Method]{Euler Method, Is the simplest approximate to solver differential equation or numerically solve equations.}
	\label{fig:euler}
\end{figure}

RK4 goes as follows:

\begin{equation} \label{eq:kg4}
y_{n+1} = y_{n} + 1/6 K_{1} + 1/3 K_{2} +1/3 K_{3} + 1/6 K_{4} 
\end{equation}
where

\begin{align*}
K_{1} &= h \dot f(x_{n}, y_{n}) \\
K_{2} &= h \dot f(x_{n} + h/2, y_{n} + k_{1}/2) \\
K_{3} &= h \dot f(x_{n} + h/2, y_{n} + k_{2}/2) \\
K_{4} &= h \dot f(x_{n} + h, y_{n} + k_{3})
\end{align*}

As the equations shows, each step the derivative is evaluated four times, once at the initial point, twice at trial midpoints, and once at a trial endpoint. From these four values, the final value is calculated, just like the equation is shown \ref{eq:kg4}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/rk4.png}
		\rule{35em}{0.5pt}
	\caption[Fourth-order Runge and Kutta Method]{Fourth-order Runge and Kutta method, Each step the derivative is evaluated four times. }
	\label{fig:kutta}
\end{figure}

\cite{numerical}

%The basic structure is computational solve rungge and kutta of for other.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------


\subsection{Kernels}

The GPU implementation. the application reads

At initialize the applications first it allocates all the CUDA and c arrays.

To allocate a big chunk of memory in the Device 

\begin{lstlisting}[frame=none]
cudaMalloc
\end{lstlisting}
And C with

\begin{lstlisting}[frame=none]
malloc
\end{lstlisting}

In the initialization function it also reads the magenetizacion data from a especific file in this specific case from ``upVW-magn-2.5nm.data''


The initial values for the simulation are

\begin{table}[h]
\centering
\begin{tabular}{| l | l |}
\hline   
NX     & 480                                            \\
\hline   
NY     & 120                                            \\
\hline   
NZ     & 1                                              \\
\hline   
TX     & 1200.0                                         \\
\hline   
TY     & 300.0                                          \\
\hline   
TZ     & 5.0                                            \\
\hline   
u      & 1                                              \\
\hline   
D      & 1.0e\textasciicircum 3 nm\textasciicircum 2/ns \\
\hline   
tau sd & 1.0e\textasciicircum -3 ns                     \\
\hline   
tau sf & 25.0e-3 ns      \\
\hline   
\end{tabular}
\end{table}


\subsection{Threads}

The number of threads that are allocated within each kernel is a 2d grid.

\begin{table}[h]
\centering
\begin{tabular}{| l | l |}
\hline   
Threads per block  X   & 32       \\
\hline   
Threads per block Y     & 32         \\
\hline
\end{tabular}
\end{table}

Depending on the hardware configuration, each GPU can allocate different threads per block. To make a homogeneous grid space for each GPU a simple calculation is made.

\begin{lstlisting}[frame=none]
NXCUDA = (int)powf(2,ceilf(logf(NX)/logf(2)));
printf("NXCUDA = %i\n",NXCUDA);
NYCUDA = (int)powf(2,ceilf(logf(NY)/logf(2)));
printf("NYCUDA = %i\n",NYCUDA);
if((int)powf(2,ceilf(logf(NZ)/logf(2))) < 1)
	NZCUDA = 1;
else
        NZCUDA = (int)powf(2,ceilf(logf(NZ)/logf(2)));
//printf("NZCUDA = %i\n",NZCUDA);

//Setup optimum number of blocks
XBLOCKS_PERGRID = (int)ceil((float)NX/(float)XTHREADS_PERBLOCK); 
printf("XBLOCKS_PERGRID = %i\n",XBLOCKS_PERGRID);

YBLOCKS_PERGRID = (int)ceil((float)NY/(float)YTHREADS_PERBLOCK); 
\end{lstlisting}


The calculations are divided into two parts, the CPU code and GPU code. Most of the code is on the GPU. On the CPU only minor  process are taken place, like I/O to a .data.
On GPU is were all the computation is happening and simulation.

\subsection{CPU}
In the $initial_calculations$ functions it calculates the terms on magnetization components


the read magnetization data.
This function basically reads data from a .dat file and allocates the memory for each blocks, it reads

The file is divide into two blocks of data, the first block of 57600 rows are the coordinate X and the coordinate Y. Then the next 57600 rows by 3 columns are the magnetization data. Base on the information read the matrices of data is created.

here two data sets are created. The coordinates point data $(x,y)$ and the magnetization data $(x, y, z)$.

Afther this initilization data, the next step is to send this data, that is actually read on the CPU (host) to de GPU(device). 


First we print the Initial and final coordinates that read, this is to ensure that the values ared sucuefully.


\subsection{GPU}


Array are created on the on the Host and sento the Device using

In the type it can be either cudaMemcpyHostToDevice or cudaMemcpyDeviceToHost, depeding, if the memory thats is being copied is sent to the host or to the device.

\begin{lstlisting}[frame=none]
cudaMemcpy(dst, src, size_in_bytes, type);
\end{lstlisting}

after initialization the coordinates points an the magnetization data in the device are done, does values are sent to the GPU, with the function cudaMemcpy() and value set to cudaMemcpyHostToDevice.

In the Initialization of the calculations most of the arrays are filled up with values base on the data read from the .dat magenetization.

\begin{lstlisting}[frame=none]
__global__ void gsource(double *sm_out, double *matrix_in, double u, int grid_width);

__global__ void gm_x_source(double *tempx, double *tempy, double *tempz,
							 double *mx, double *my, double *mz,
							 double *sm_x, double *sm_y, double *sm_z,
							 int grid_width);
\end{lstlisting}


The function gsource

Makes the following calculation of the $double *matrix\_in$ or m


\begin{equation} \label{eq:gsource}
out[i] = (m[i-2] - 8.0*m[i-1] + 8.0*m[i+1] - m[i+2]) * \dfrac{u}{12 * deltaX }
\end{equation}

where

$$deltaX = \frac{TX}{NX}$$

This calculation is done for the arrays read from the .dat file, for dev\_mx, dev\_my and dev\_mz and are saved in a temporary arrays dev\_sm\_x, dev\_sm\_y, and dev\_sm\_z.

The method.

$gm\_x\_source$ calculates the coss producto of the array $m_{xyx}$ and $sm_{xyz}$, this is done twice.

THis data is saved on the arrays $dev_sm_{xyz}$,

Afther launching this two kernels the initial setup is done, the next step is the actual simulation using runge and kutta integrator.


\subsection{KG4}

As seed  in Runge and Kutta section, this method is implementation to numerically solve the differential equation. Intuitivelly the implementation on CUDA code is done with 4 kernls, where each kernel calculates respectivelly the order of the integrator. In the last term calculation is where all the magic occurr, the sum of the previous 3 calculated terms.

\begin{lstlisting}[frame=none]
__global__ void gterm1_RK1( . . .);
__global__ void gterm2_RK2( . . .);
__global__ void gterm3_RK3( . . .);
__global__ void gterm4_RK4( . . .);
\end{lstlisting}

Between each term calculation of RG4 laplacian calculation kernels are launched.

\begin{lstlisting}[frame=none]
__global__ void glaplacianx( . . . );
__global__ void glaplacianyboundaries( . . . );
__global__ void glaplaciany( . . . );
\end{lstlisting}

The final kernel is launched $void gterm4_RK4()$ obtain the array $deltam_{xyz}$, which is the final result of the RK4 integrator. This array is sent to the last step.

\subsection{effective values}

When the rg4 integretaor is done effective values are calculatated, this values sirve the porpese of calculation  the.


\begin{lstlisting}[frame=none]
__global__ void gm_x_sm( . . . );
__global__ void gu_eff( . . . );
__global__ void gu_eff_beta_eff( . . . );
__global__ void gbeta_eff( . . . );
__global__ void gbeta_diff( . . . );
\end{lstlisting}

The last kernel $ void gbeta\_diff( . . . );$ is where the two final arrays are obtain,
which then are sent to the CPU for the final calculation.

The final calculation is just the sum of all the elements of $beta_diff_num$ and $beta_diff_den$, there divided.
This final single values tells us...


This is the final step of the simulations this is where $beta\_diff$ is obtained. 

The final data is saved
\subsection{time}

When the simulation is done, it will repeat the process intil the values converges.


\section{Validation}

The validate the code, that is obtained from the simulation

Once obtain the results from the simulation, the results are saved into two seperated data sets. .eff and .spin. depending of the configuration of the application is possible to obtain the uVW or the. Because CUDA framework is highly parallel system is farly easy to obtain errenois data from the calculations, even setting up the threads per block incorrectly is possible to get data set that a wrong, or results that don't diverge. It is necessary that when finishing making changes to the code validating the results with a valid data set is done.

The validation is done by checking the output the simulation with a valid data set, the output of the validation application tells us the error factor of the current data with the valid set. So for each data set there is a threshold value, that can tell if the that is close enough to the results. A example of the validation performed.


\section{Help Kernels}


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Data Flow}


The initial data flow of the kernels goes as follow, Fi
