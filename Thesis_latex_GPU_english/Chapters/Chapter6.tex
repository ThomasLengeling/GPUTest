% Chapter Template

\chapter{Conclusions and future work} % Main chapter title

\label{Conclusions and future work} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 6. \emph{Conclusions and Future Work}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

GPUs definitely have a place in the world of computational physics and other similar applications. Their use allows to carry out teh same work with less energy and more science with less resources. They make HPC computing clusters affordable for small research groups. The true limit test of this new technology will be if it is actually used to advance new science. In the field of computational physics studies, this pushes the barrier of what is computationally feasible, from speedups of 1.5x to 20x using GPUs\cite{applications}.

Acceptance has been slow due to many factors, GPUs are sometimes seen as a fad or a niche. The specialized skill set and effort required for GPU programming along with the risk of spending money to setup a GPU cluster, raises a concern for productivity and viability of this technology. Adopting this technology requires abandoning legacy codes and smart optimizations that have been developed over the years. A wrong choice may result in wasted time and effort.
What is certain is at the moment, it is the overall direction of the industry towards higher parallelism. Single threaded performance has reached a local limit, where all types of processors are seeking more performance out of parallelism. This means that a large portion of the work needed to parallelize a code for a certain parallel architecture will most probably be applicable to another parallel architecture as well. From the literature and my experiences, one can observe that in order to achieve good results in programming with GPUs it is necessary to take a heterogeneous approach to coding. That refers to adopting multi-threaded CPUs and concurrent GPU type algorithms.

Spintronics, in particular involves designing new magnetic materials for spin-devices and modeling and understanding of spin-transport at molecular and atomic scale. Manipulating magnetic domain walls to store and transfer information is envisioned to enable high-density, low-power, non-volatile, and non-mechanical memory. Promising for future systems for example the racetrack memory by Parkin at IBM, where DWs can be moved by applied magnetic fields and/or by currents via the spin transfer torque effects such as the simulation proposed by this research. However, most of the technologies and experiments  are still in development. Furthermore, there are several obstacles to be overcome to enable these technologies.

Using computer simulation is possible to predict the outcome of the theoretical approach. In this when reproducing the effects of spin diffusion by numerically implementing the Zhang-Li Model into micormagnetics, we apply a current to a regime of DWs in a NiFe soft nanostrips. Furthermore, it provid the theoretical experiments with high precision on relative inexpensive computers. By using the highly parallel capabilities of the GPU it was possible to dramatically reduce the computation time of the simulation from around 400s on the non-threaded CPU version to 41s on a GPU. Therefore, it is now possible to execute ten times more simulations on the same time frame.

Through the optimization we achieved a maximum speedup of 8.0x. The result did not occur on the newest device, the K20m, but on the mid-range GPU, the GeForce GTX 580, which has more clock cycles (GHz) than the others cards. The newest device, the K20m, is ten times more expensive than the 580 card. The Tesla cards are mainly designed for server used, multiple users, while the GeForce is designed for high performance graphics, high workload on a single user. The  optimization approach was focused on giving more workload on the GPUs, more performance per SM and increasing the work of threads per block. Lastly, the GTX 580 is not the newest GeForce card available on the market. The new GeForce 980 is expected to have a 40$\%$ increase in performance over the 500 GTX series. 

The simulations were performed on a relative small data set. Moreover, it is possible to increase the data set, in other words, a bigger magnetization data. This overall will reduce the execution time by a factor of 8.0, increasing the number of simulations in the same time frame. Future test are likely to be performed on a newer GPU architectures, such as the Maxwell, for example, on the new GeForce 980 or on the Tesla K80.

The current thread is to push the hardware capabilities and performance along with Mooers' Law, despite these issues there are some trends in the hardware industry that will make working with GPU easier and more widespread within a HPC context. The following are some examples:

\begin{description}
  \item[3D Memory] \hfill \\
 A possibility is stacking DRAM chips into dense modules with wide interfaces, and this brings them inside the same package as the GPU. This lets GPUs get data from memory more quickly – boosting throughput and efficiency – allowing us to build more compact GPUs that put more power into smaller devices. The results are: several times greater bandwidth, more than twice the memory capacity and quadrupled energy efficiency.
  
  \item[NVLink] \hfill \\
 Today’s computers are constrained by the speed at which data can move between the CPU and GPU. NVLink puts a fatter pipe between the CPU and GPU which, allows data to flow at more than 80GB per second, compared to the 16GB per second available now.
 
 \item[Pascal Module] \hfill \\ 
  NVIDIA has designed a module to house Pascal GPUs with NVLink. At one-third the size of the standard boards used today, they will put the power of GPUs into more compact form factors than ever before.
  
   \item[Mobile and Embedded Devices] \hfill \\ 
   With the new Tegra K1 it is possible to do supercomputing on the level of mobile devices, achieving upto 1 TFlops of performance. Embedded systems with CUDA capabilities have the possibility to integrate high performance algorithms to such small platforms. 
   
   \item[Cloud Computing] \hfill \\ 
   NVIDIA is pushing the limits of brining computer graphics to the cloud. The idea is for everybody to have access to high quality computer graphics.
 
  \end{description}
  
\vspace{3.5em}


Using heterogenous computing is possible to dramatically decrease computational time on CPU applications that are not feasible with the current CPU paradigm. They also offer room for new types of computational simulations in a reasonable time frame. 
To conclude, I offer my personal perspective on GPU computing and I think the importance of using accelerator hardware is an economic and environmental issue and part of the future. The environmental aspect of doing computing is often overlooked, but an ever increasing important one to be considered. As heavy computer users we will have to take responsibility for our electricity use. The benefit of less energy use is clear with more computational power. These ideas will provide definite benefits for future use.
