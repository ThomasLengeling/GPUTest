% Chapter Template

\chapter{Optimization Results} % Main chapter title

\label{Optimization Results} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Optimization Results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter is the results of the CUDA code implementation launched on a single GPU device. The test were performed on various GPUs architectures, which, has different hardware characteristics. The application is analyzed by the NVIDIA's Visual Profiler. In addition the CUDA kernels were evaluated in performance, execution time, occupancy and concurrent kernels. Furthermore, the results, are analyzed and optimized using the schemes from chapter 3. Lastly the code is executed remotely on the supercomputer ``Piritakua'' at the Department of Multidisciplinary Studies Yuriria, University of Guanajuato.

\section{Supercomputer ``Piritakua''}

The experiments are carried out using the supercomputer “Piritakua”. The massive GPU cluster was design and built by Dr. Claudio from the University of Guanajuato Multidisciplinary Studies Yuriria. The GPU cluster is located at a small town of Mexico, Yuriria. The supercomputer at the Front-end has a eight core Intel Xeon at 2.4 Ghz, at the back-end several GPU are connected. All of GPUs CUDA version 5.0 was installed. Furthermore, the GPUs node are; one NVIDIA Tesla K20, two Tesla M2070 and a GeForce GTX 580 

The cluster has a GNU$ \ $LINUX distribution installed, the CentOS 64 bits version 6.4. CentOS stands for Community Enterprise Operating System which is free operating system and one of the most popular GNU$ \ $Linux distribution for web servers and as well is supported by RHEL (Red Hat Enterprise Linux) \cite{centos}. The specifications of the front-end cluster are \ref{tab:cpus}.

\begin{table}[h]
\centering
\begin{tabular}{ | p{7.1cm}  | l | l | l |}
  \hline
  Processor & Number & Cores & RAM  \\
  \hline
  Server Dell Intel Xeon E5620 2.4 GHz & 1 & 8 & 12 GB \\
  \hline
  Server HP Proliant SL 350s Gen3 Intel Xeon X5650 2.67 GHz & 2 & 24 & 32 GB \\
  \hline
   Server HP Proliant SL 250s Gen8 Intel Xeon E5-2670 2.60 GHz & 3 & 48 & 104 GB \\
   \hline
   CPU Xeon Phi  5110p & 1 & 8 & 8 GB\\
   \hline
   CPU Xeon Phi 7120p  & 1 & 8 & 16 GB\\
   \hline
  \end{tabular}
      \caption{CPU specifications}
  \label{tab:cpus}
  \end{table}
  
 The CUDA Code was launched on only two CPUs, a laptop with a eight core intel i7-3630QM and a high-end CPU Xeon Phi 7120p from the cluster. In addition the Xeon Phi was used for all the experiments for the Cluster's GPUs. The Xeon Phi 720p is capable of achieving f 1.2 teraflops of double precision floating point instructions with 352 GB/sec memory bandwidth at 300 W. The code was executed on laptop to show the performance comparison between a lightweight GPU and a server based GPU.

When accessing ``Piritakua'' remotely is possible to use all the GPUs nodes available on the cluster. The specifications of the GPU connected to the back-end are as follow, CC stands for compute capability.

\begin{table}[h]
\centering
  \begin{tabular}{ |  l  |  l  |  l  |  l  |  l  | l | l | l |l | }
    \hline
    Model & Core& RAM& DP GF& SP GF& Bandwidth& GHz& CC & Power\\
    \hline
    Tesla K20m & 2496 & 5GB & 1,170 & 3,520 & 208GB/s & 0.73 & 3.5 & 225W \\
   \hline
    Tesla M2070 & 448 & 6GB & 515 & 1,030 & 150GB/s & 1.15 &  2.0 & 225W\\
   \hline
     Tesla C2050 & 448 & 2.5GB & 512 & 1,030 & 144GB/s & 1.15  & 2.0 & 238W \\
   \hline
      GeForce 580 & 512 & 1.5GB & 520 & 1,154 & 192.2GB/s & 1.5 & 2.0 & 244W \\
   \hline
   GeForce 670mx & 960 & 3GB & 520 & 1,154 & 67.2GB/s & 0.6 & 3.0 &  - \\
   \hline
  \end{tabular}
    \caption{GPU technical specifications}
  \label{tab:gpus}
  \end{table}
  
  The code was launched on all Piritakua's GPUs and on external GeForce GTX 670m, located on a laptop. The "m" stands for the mobil graphic cards. In addition the 670m card is design for less power usage, but with high graphics power, it even has more cores than some Tesla models, However, this types of cards has way more less Bandwidth than standard versions. The 670m card was used as comparison between laptop GPUs and high-end desktop/servers GPUs.
     
\subsection{Architecture Differences}

  Architecture dependent technical differences of NVIDIA GPUs. During the CUDA development a lot of internal features have been improved, but most paradigms for the programmer stayed the same. For example a streaming processor can now handle 2048 threads at a time, but the maximum block size stayed at 1024. This results in a 100$\%$ theoretical occupancy for block sizes of 1024 compared to 66$\%$ of Fermi. Another example is the use of Shared Memory. Maxwell has 64KB dedicated Shared Memory. The maximum amount of Shared Memory per Block is 48KB for all three architectures \cite{hoermanngpu}.
  
  There are two GPU architectures that CUDA implementation was launched, the Fermi and the Kepler. The Tesla K20m and the GeForce 670mx are based on the ``Kepler'' GPU architecture. The Tesla M2070, M2050 and the GeForce GTX 580 on the Fermi architecture. The Kepler architecture newer than the Fermi. More information about the architectures in the table \ref{tab:arch}. The Maxwell architecture wasn't uses for benchmarking test. However, it is showed for future reference and analysis.
  
\begin{table}[h]
\centering
  \begin{tabular} { | l | l  | l | l | l  |  l  | l |}
    \hline
    Name & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} &  \multicolumn{2}{|c|}{Maxwell} \\
    \hline
    Compute Capability & 2.0 & 2.1 & 3.0 & 3.5 & \multicolumn{2}{|c|}{5.0}\\
   \hline
    Single Precision Operation per Clock/SM & 32 & 48 & \multicolumn{2}{|c|}{192} & \multicolumn{2}{|c|}{128}\\
   \hline
    Double Precision Operation per Clock/SM & $4/16^1$ & 4 & 8 & $8/64^2$ & \multicolumn{2}{|c|}{$1^3$}\\
   \hline
    Max Number of Threads per SM / SM & \multicolumn{2}{|c|}{16} & \multicolumn{4}{|c|}{32}\\
   \hline
    Max Number of Registers per Thread/SM & \multicolumn{3}{|c|}{1536} & \multicolumn{3}{|c|}{2048}\\
   \hline
       Max Number of Threads per Block & \multicolumn{6}{|c|}{1024}\\
   \hline
   Active Thread Blocks per SM / SM & \multicolumn{2}{|c|}{8} & \multicolumn{2}{|c|}{16} & \multicolumn{2}{|c|}{32}\\
   \hline
   Max Warps per Multiprocessor/ SM & \multicolumn{2}{|c|}{48} & \multicolumn{4}{|c|}{64}\\
   \hline
   Registers / SM & \multicolumn{2}{|c|}{32K} & \multicolumn{4}{|c|}{64K}\\
   \hline
   Level 1 Cache & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Shared Memory / SM & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Warp Size & \multicolumn{6}{c|}{32}  \\
   \hline
  \end{tabular}
  \caption{GPU Architecture Specifications}
  \label{tab:arch}
  \end{table}
 
\section{Optimization}

The CUDA code was launched on each one of "Piritakua"'s GPUs. As we know the supercomputer has different GPUs, as well as several different  architectures. Furthermore the initial results in time execution of the implementation can been seen in figure \ref{fig:iniresults}.

Using the NVIDIA's Visual Profiler we obtain kernel metrics of the Tesla K20m. The output is organized by kernel performance throughout the  simulation.  As we can notice the laplacian evaluation; $glaplacinay$, $gLaplacianx$ and  $gLaplacianYBoundaries$ uses up-to 44.37$\%$ of the overall simulation. The $gsolution$ kernels which solves Zhang and Li model \ref{eq:zhang} uses 14.04$\%$. The terms calculation for the RK4 integration only uses a minor part of the overall simulation. However the $gSolution$, $gsdExchange$ and laplacian calculation are part of the RK4 integration, which overall is about 99$\%$.

 The optimization focus on giving the GPUs as much work as possible, using at the fullest the GPU hardware capabilities. In addition reducing the performance time of most time consuming kernels in the table \ref{tab:nvprof}.
\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | l |}
    \hline
    Time$\%$& Time & Calls & Avg & Min & Max & Kernel \\
    \hline
    23.50 & 3.6s & 26521 & 137.5us & 96.0us & 597.1us&$gLaplaciany$ \\
    \hline
    17.04 & 2.6s & 26521 & 99.7us & 57.0us & 561.1us&$gSolution$ \\
    \hline
    16.75 & 2.6s & 26522 & 98.0us & 62.8us & 400.6us&$gLaplacianx$ \\
     \hline
      13.37 & 2.0s & 26522 & 78.2us & 40.8us & 453.8us&$gsdExchange$\\
      \hline
    7.22 & 1.1s & 26522 & 42.2us & 23.4us & 326.0us &$gsfRelaxation$\\
       \hline
    6.22 & 965.2ms & 6630 & 145.6us & 79.2us & 722.6us &$gTerm4RK4$\\
       \hline
    4.12 & 640.3ms & 26522 & 24.1us & 21.8us  &138.7us &$gLaplacianYBoundaries$\\
       \hline
    3.41  & 529.2ms & 6630 & 79.8us & 41.6us  & 478.8us & $gTerm2RK4$\\
       \hline
    3.36 & 520.8ms & 6630 & 78.5us & 41.5us & 372.2us & $gTerm3RK4$\\
       \hline
    3.35 & 519.5ms & 6631 & 78.3us & 41.1us & 372.2us & $gTerm1RK4$ \\
   \hline
  \end{tabular}
  \caption{Kernel executing and time int the Tesla K20}
  \label{tab:nvprof}
  \end{table}
 
  \begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/gpu_initial.png}
		\rule{35em}{0.2pt}
	\caption[Initial GPU results]{Initial results of the implementation running on several different GPU nodes}
	\label{fig:iniresults}
\end{figure}

As we can see in figure \ref{fig:iniresults} the GeForce GTX 580 is the card with the least amount of time execution, and the Tesla K20m the fastest amongst the Tesla Cards. 


--------include some initial performance metrics with the profiler

The following sections are the optimization techniques or methods applied to the application. In addition demonstrating the comparison between the initial implementation and the modified version. Five versions of the code are describe; Branching, Occupancy, Concurrent Kernels, Shared Memory and Structure of Arrays. Branching refers how the threads are executing and their executing flow in the application. Occupancy number of threads per devices being used. Concurrent Kernels execution several kernels at once. Shared Memory, using as much shared memory as possible. Finally Structure of Arrays modifying the memory allocation in the device.

 \subsection{Branching}
 
 CUDA follows the Single Instruction Multiple Thread architecture. This means that there are running several threads executing the same code. Each thread can operate on its own data and has its own address counter . They are free to use each data dependent path. But also each thread is executing the same operation at the same time. When a thread within a warp branches differently the other threads get deactivated\cite{hoermanngpu}. This can be described by the following code \ref{lst:branch} and the illustration \ref{fig:threads}.

\begin{lstlisting}[language=C++, label={lst:branch}, caption={Threads Branching}]
__global__ void kernel(int* out){
idx = threadIdx.x;
int result;
if(idx == 0){
	result = foo();
} else {
	result = bar();
	out[idx] = result;
}
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.51\textwidth]{Figures/threads.png}
		\rule{35em}{0.2pt}
	\caption[he execution flow]{The execution flow of a branching code, with warp size 8. Black arrows are active threads, and the grey ones are disabled.}
	\label{fig:threads}
\end{figure}

The branching problem occurred in the section where boundary condition for laplacian was being analyzed \ref{lst:branchlap}. Only a single kernel was used to checking bounding. In addition a bottleneck occur. The implementation gets the job done. However, a minor part of the threads are working, which is a waste of computation resources.

\begin{lstlisting}[language=C++, label={lst:branchlap}, caption={Branching problem in the laplacian boundary condition evaluation}]
__global__ void glaplaciany(...); //Compute laplacian in Y direction
__global__ void glaplacianx(...); //Compute laplacian in X direction

__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
  	}
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
}
\end{lstlisting}

To solve the branching issue is to include more work on the laplacian boundaries calculation \ref{lst:newcde}.
\begin{lstlisting}[language=C++, label={lst:newcde}, caption={More workload on a single kernel execution}]
__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
    glaplaciany(...); //Compute laplacian in Y direction
	glaplacianx(...);  //Compute laplacian in X direction
}
\end{lstlisting}

This technique was applied to all parts of the code, eliminated inactive threads that were not performing computational operations to active ones. The results of the modified version consult the Optimization Results section. 

\subsection{Concurrent Kernels}

Initially each kernel was launched in the default steam zero. The figure \ref{fig:streams} illustrates such result using the NVIDIA's Visual Profiler. Each kernel that is being launched cannot run simultaneously because the next kernel launches needs to compute data, in other words the kernels are not independent from each other.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.72\textwidth]{Figures/ini_steams.png}
		\rule{35em}{0.2pt}
	\caption[Initial Streams]{Kernels running in the default 0 Stream.
}
	\label{fig:streams}
\end{figure}

 Kernels by default cannot run in parallel with others kernels. Furthermore CUDA doesn't provide an automatic parallel kernel executing. In addition the programmer needs to tell the CUDA compiler that some portion of the code or kernel can run in parallel. However, the compiler cannot always execute concurrent kernels, depends on the hardware capabilities and as well the number of threads per block and the number of SM available. If the compiler finds available space to run another kernel simultaneously it will do so. 

For example, the $gsolution$ \ref{lst:concurrent} kernel computes the Zhang an Li model for x, y, z coordinates, which uses extensively the global memory of the device. To achieve concurrent kernels the streams need to access memory blocks that are pinned to a stream. So each memory block corresponding x, y, z are mapped to 3 streams. Furthermore, all the matrices corresponding to the coordinate x are mapped to the stream 1, y to stream 2 and z to stream 3.

\begin{lstlisting}[language=C++, label={lst:concurrent}, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel.}]
deltamX[index] = sfrelaxX[index] + sdexX[index] + laplX[index] - smX[index];
deltamY[index] = sfrelaxY[index] + sdexY[index] + laplY[index] - smY[index];
deltamZ[index] = sfrelaxZ[index] + sdexZ[index] + laplZ[index] - smZ[index];
\end{lstlisting}
 
The CUDA code \ref{lst:concurrent} is divided into a single kernel \ref{lst:consingle}. In addition this new generic kernel can be launch in parallel. Instead of running one big kernel, three individual kernels are launched simultaneous. Dividing each kernel is possible to implement shared memory though each kernel which otherwise wasn't possible. 

\begin{lstlisting}[language=C++, label={lst:consingle}, caption={Evaluation of individual coordinates of the Zhang and Li model}]
int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
int j = blockIdx.y * blockDim.y + threadIdx.y;
int index = j * NXCUDA_CONST + i;

if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
	deltam[index] = sfrelax[index] + sdex[index] + lapl[index] - sm[index];
\end{lstlisting}

This same method is applied to every kernel that can be separated into three kernels calls. Some kernels cannot be separate such as the cross product, because the product uses pinned memory block from the other streams.  The figure \ref{fig:concurrent} shows the results of concurrent kernels in the Tesla K20.

\begin{lstlisting}[language=C++, caption={Evaluate Zhang and Li model.}]
for (int i  = 0; i < 3; i++)
	gsolution<<<blocks, threads, 0, stream[i]>>>(spinAccXYZ[i]->getDev_deltam(),
												 spinAccXYZ[i]->getDev_sfrelax(), 
												 spinAccXYZ[i]->getDev_sm(), 
											 	 spinAccXYZ[i]->getDev_sdex(),
											 	 spinAccXYZ[i]->getDev_lapl());
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.75\textwidth]{Figures/concurent.png}
		\rule{35em}{0.2pt}s
	\caption[Streams kernels Tesla K20]{Concurrent kernels in the Tesla K20 using NVIDIA's Visual Profiler.}
	\label{fig:concurrent}
\end{figure}

Concurrent kernels demonstrates a very promising technique to achieve a huge performance increment in the application. However, there are some downsides to this implementation; correctly synchronize kernels, waiting time and hardware resources \cite{practices}. The timeline of the application \ref{ref:waittime} illustrates the waiting time between kernels execution. However, the waiting time are very small time steps between 0.01ms and 0.01ms, but waiting time occurs for each step of the RK4, appears approximate 45,00 times. Furthermore, currently branching the kernel execution process can eliminate this issue.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{Figures/waittime.png}
		\rule{35em}{0.2pt}
	\caption[Waiting time concurrent kernels]{Waiting time between each concurrent kernel execution}
	\label{fig:waittime}
\end{figure}


\subsection{Shared Memory}

As we know shared memory is faster than global memory. However, shared memory is very limited. To be able to implement shared memory in the kernels, we needed the kernels separated in their x, y and z coordinate as mentioned in the previous section. In addition this allows us to implement shared memory across each kernel, otherwise wouldn't be possible.

The idea behind shared memory is to reduce the amount of global memory calls, which has about 400-600 clock cycles, while the shared memory only 1-32 cycles \ref{fig:memory}. The share memory implementation is accomplish by allocating the data from the thread block into a temporary array, which is the shared memory.In addition the kernel can make calculation on this temporary array and write the values onto the global memory. The code of such implementation is illustrated in \ref{lst:shared}. As we know there is no guaranty that threads will execute at the same order. Using $\_\_syncthreads()$ will wait until all threads have completed there task, in this case loading global memory into shared memory. Then we performed operations on the shared memory, and finally save the values in the global memory. Shared memory was used in all the occasions were kernels used extensively global memory.

\begin{lstlisting}[language=C++, label={lst:shared}, caption={Shared memory}]
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
int index = j * NXCUDA_CONST + i;

if (i > 1 && i < NX + 2 && j >= 0 && j < NY){
     int cacheIdx = threadIdx.y * blockDim.x + threadIdx.x;
     __shared__ double deltamS[THREADS_SHARED * THREADS_SHARED];

	 //load memory into shared memory
     deltamS[cacheIdx] = operationGlobal(globalMemory);
     __syncthreads();

	 //copy back the shared memory to global memory
     deltam[index]  = deltamS[cacheIdx];
}
\end{lstlisting}

To calculate the laplacian we need to access a great amount of global memory which is located near the value of interest. In this case in region of 4x4.  The figure \ref{fig:shared} illustrates which part the block is for allocating shared memory and global memory. The global memory is used for the boundary conditions of the block, while the shared memory for all the values inside the block.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.3\textwidth]{Figures/shared.png}
		\rule{35em}{0.2pt}
	\caption[Shared Memory Strategy]{Shared Memory Strategy for Laplacian evaluation }
	\label{fig:shared}
\end{figure}

The code \ref{lst:lapsh} demonstrate how calculate the Laplacian with the equation \ref{eq:nn} with the usage of shared memory. As we can see first we load all the global memory onto a a temporary array, the shared memory. The we performed the calculation on the shared memory as mentioned before.

\begin{lstlisting}[language=C++, label={lst:lapsh}, caption={Laplacian evaluating using shared memory with boundaries condition}]
if (i >= 0 && i < NX && j >= 0 && j < NY){
    __shared__ double lapS[ THREADS_SHARED * THREADS_SHARED];
    lapS[sIdx] = deltam[Index];
    __syncthreads();

    if (threadIdx.x >= 2 && i < threadIdx.x - blockDim.x -2){ //shared
       lapy[idx] = - lapS[sIdx + 2] / 12.0 + 4.0 * lapS[sIdx + 1] / 3.0
			  	   - 5.0 * lapS[sIdx] / 2.0
			  	   - lapS[sIdx - 2] / 12.0 + 4.0 * lapS[sIdx - 1] / 3.0;
	else{ //global memory
		lapy[idx] = - deltam[idx + 2] / 12.0 + 4.0 * deltam[idx + 1] / 3.0
			  		- 5.0 * deltam[idx] / 2.0
			  		- deltam[idx - 2] / 12.0 + 4.0 * deltam[idx - 1] / 3.0;
	}
}
\end{lstlisting}

This technique look very promising and reducing global memory. However, great amount of time is spent on loading data onto the shared memory array, The results of such implementation is in the following section, optimization results.

\subsection{Structure of Arrays, SAO}

AoS and SoA refer to "Array of Structures" and "Structure of Arrays" respectively. These two terms refer to two different ways of laying out your data in memory. This is illustrated in figure \ref{fig:aos} and \ref{fig:sao} respectively. AOS, grouping properties of an object together and making an array of those objects in memory, whereas a structure of arrays would be a single structure in which you make an array for each property. The structure of arrays can allow for better cache utilization, easier to access continues data, making better use of each read you make from memory, providing a more effective route to memory. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/aos.png}
		\rule{35em}{0.2pt}
	\caption[Array of structures (AOS)]{AOS memory layout }
	\label{fig:aos}
\end{figure}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/soa.png}
		\rule{35em}{0.2pt}
	\caption[Structure of Arrays (SAO)]{SAO memory layout}
	\label{fig:sao}
\end{figure}

The initial implementation the  x, y, z data was allocated in separated blocks. Furthermore when accessing blocks of the the same coordinates, the register access the data as the figure\ref{fig:aos}.

\begin{lstlisting}[language=C++, caption={AOS implementation}]
deltam_x = (double **)calloc(NYCUDA, sizeof(double *));
deltam_y = (double **)calloc(NXCUDA, sizeof(double *));
deltam_z = (double **)calloc(NXCUDA, sizeof(double *));
\end{lstlisting}

To solve this issue a custom class GPUMatrix was programmed, which contained all the matrices for the device. Moreover, the classes allocated the data for each Matrix and free the memory automatically when the simulation is over. The was allocated in a form that easier for the device to access common elements, for example when doing operations only on the x coordinate, the kernel physically access matrices that are near by.

\begin{lstlisting}[language=C++, caption={SOA implementation}]
    GPUMatrix<T> *dev_deltam;
    GPUMatrix<T> *dev_sdex; //Exchange term
    GPUMatrix<T> *dev_sfrelax;
    GPUMatrix<T> *dev_m; 
\end{lstlisting}

-- add a little more explanation

\subsection{Occupancy}
 
Firstly, we increase the use of constant memory in the device, this helps redundant evaluations of variables which speedups the operations per clock cycle in the device. In addition  constant memory changes can be appreciate in the code \ref{lst:constant}. Also  matrix calculation for the boundary conditions \ref{eq:matrix3} and \ref{eq:matrix3} were implemented using constant Memory.
 
 \begin{lstlisting}[language=C++,  label={lst:constant}, caption={Constant Memory changes}]
 gsource << <blocks, threads >> >(u_val, dev_sm_z, dev_mz, NXCUDA);
 
 sfrelax_y[index] = -deltam_y[index] / tau_sf;
     
 DELTAX = (double)TX / (double)NX;
\end{lstlisting}


-----include occupancy results using The Visual Profiler.
 
The different numbers of threads per block and as well the number of blocks per grid can dramatically increase or decrease the performance of the application. The table \ref{tab:threads} illustrate the different threads per block configuration on the GeForce GTX 580. The initial configuration for the Fermin and the Kepler was 32x32 threads per block for global memory and 16x16 threads per block for the shared memory. We found, that the optimal configuration for the Fermin cards was 16x16 threads per block and as well for the shared memory and for the Kepler cards was 32x32 threads per block for both memory types.

\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | }
    \hline
    Threads & Shared & speedup & time & Occupancy & Occupancy Shared \\
    \hline
     8x8 &  8x8 & 7.217x & 52318.3  &  & \\
    \hline
     16x16 & 8x8 & 7.625x & 49517.3 &  & \\
    \hline
    16x16 & 16x16 & 7.978x & 47329.2 &  & \\
    \hline
    32x32 & 16x16 & 7.356x & 51333.4 &  & \\
    \hline
    32x32 & 32x32 & \multicolumn{4}{|c|}{Failed}\\
    \hline
  \end{tabular}
  \caption{Threads per block configuration and occupancy on the Fermin architecture}
  \label{tab:threads}
  \end{table}


\section{Optimization results}

This section is the overview of the optimization results compared with each version of the code. The figure \ref{tab:time} and \ref{tab:speed} illustrates the the time execution and the speedup respectively. The final version of the code is the Occupancy. Moreover the greatest performance occurred on the GeForce GTX 580 Card with a 8.0x speedup.

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy \\
    \hline
    Tesla K20m & 107322.7 & 101513.4 & 97106.0 & 90201.7 & 68988.2 & 66456.0\\
   \hline
    Tesla M2070 & 110912.3 & 103212.4 & 130754.1 & 97343.4 & 73938.1 & 70299.3\\
    \hline
    Tesla C2050 & 109635.1 & 101212.4 & 128516.6 & 96762.0 & 72964.5 & 69358.1\\
   \hline
    GeForce GTX 580 & 70002.7 & 68712.2 & 76481.9 & 68567.1 & 51603.7 & 47213.2\\
   \hline
    GeForce 650m & 244372.9 & 237371.9 & 227237.8 & 279804 & 181217.4 & 174419\\
   \hline
  \end{tabular}
    \caption{GPU Optimization time}
  \label{tab:time}
  \end{table}
  
  \begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy\\
    \hline
    Tesla K20m & 3.517x & 3.718x & 3.888x & 4.186x & 5.473x & 5.682x\\
   \hline
    Tesla M2070 & 3.403x & 3.534x & 2.888x & 3.879x & 5.107x & 5.371x\\
    \hline
    Tesla C2050 & 3.442x & 3.571x & 2.938x & 3.902x & 5.175x & 5.444x\\
   \hline
    GeForce GTX 580 & 5.391x & 5.521x & 4.937x & 5.551x & 7.317x & 8.0x\\
   \hline
    GeForce 670MX & 1.544x & 1.598x & 1.662 & 1.349x & 2.084 & 2.163x\\
   \hline
  \end{tabular}
    \caption{Speedup performance}
  \label{tab:speed}
  \end{table}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.48\textwidth]{Figures/gpuOptimization.png}
		\rule{35em}{0.2pt}
	\caption[Overall time simulation]{Overall time simulation}
	\label{fig:sao}
\end{figure}

The initial Visual Profiler test \ref{fig:iniresults} illustrated us that the Laplacian evaluating obtain about half of the overall simulation time. However, in the final optimization \ref{fig:final} the Laplacian was reduced from 44.37$\%$ to 26.24 $\%$ execution time. The same occur for the Runge and Kutta term evaluation. Furthermore, the $gsdExchangeFull$ incremented from 13.37$\%$. 23.35$\%$, which is not necessary good. The increment is due to shift in stream operators, the  $gsdExchangeFull$ is being process in the default stream zero, while the others on stream one through three. 

---include final thread occupancy in the table.

\begin{figure}[htbp]
	\centering
	  \begin{tabular} { |  l  |  l | l  | l | l |}
	      \hline
	    Time $\%$& Time & Calls & Avg & Kernel \\
    \hline
   35.24 & 21.33s & 216330 & 98.5us & $gSolTermRK4Relaxation$ \\
   \hline
   26.24 & 15.88s & 288441 & 55.0us & $glaplacianXYBroundaries$\\
   \hline
   23.35 & 14.13s & 160000 & 88.3us & $gsdExchangeFull$ \\
   \hline
   15.17 & 9.18s & 72108 & 127.2us & $gSolTerm4RK4Relaxation$\\ 
   \hline
    \end{tabular}
	\caption[Optimization results with the Profiler]{Final optimization results using NVIDIA's profiler, on the Tesla K20m}
	\label{fig:final}
\end{figure}
    
The Tesla K20m was the only GPU which in every code modification it did not lose performance over the course of the optimization process. However,  the other GPUs drop performance in the stream optimization stage. This is where each kernel was divided into three separated kernels. Doing this we were able to calculate the x, y, z coordinates independently. In addition, this enable room to implement shared memory across the kernels. As the figure \ref{tab:gpus} illustrates the Tesla K20m is the only GPU card with CC of 3.5. This is important to mention, because the card has access to Hyper-Q. Moreover, this improvement can synchronize automatically the kernels for them to launch in a concurrent manner. 

The SOA optimization overall improved dramatically the performance of the application, obtaining a 1.2x - 2.0x speed up in all GPU cards \ref{fig:speedup}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/speed.png}
		\rule{35em}{0.2pt}
	\caption[Optimization speedup overview]{Optimization speedup}
	\label{fig:speedup}
\end{figure}


We could expected the newest card Tesla k20m would obtain the most speedup and lest time overall, certainly because it has more CUDA cores, the highest compute capabilities. Furthermore, if falls behind the GeForce 580 with about 2.5x speedup difference. However, the Tesla K20 only had a difference of 0.3x speedup compared with the Teslas Cards. The GeForce \ref{tab:gpus} compared with the others GPUs specifications has most Processor clock (GHz), which can make more mathematical calculations per cycle.

  \vspace{3.5em}

Finally, various techniques and practices from chapter \ref{Heterogeneous Performance Analysis and Practices} we used to archive speedup and an increase performance. We incremented the used of constant memory, shared memory, changed the memory allocating access, analyzed thread branching and finally analyzed kernel occupancy. The highest performance of all GPUS did not occur in the newest NVIDIA card, the K20m, which is the most expensive of all the GPUs. The actual improvement occurred on the GeForce 580 (the more GHz of all GPUs) with a 2.32x speed up difference from the Tesla K20m.
  
