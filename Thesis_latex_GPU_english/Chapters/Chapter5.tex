% Chapter Template

\chapter{Optimization Results} % Main chapter title

\label{Optimization Results} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Optimization Results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter is the results of the CUDA code implementation launched on several different GPUs nodes. The test are performed on various GPUs architectures, which, has different hardware characteristics. Each GPU node is analyzed using the NVIDIA's Visual Profiler, in addition the CUDA kernels are evaluated in performance; throughput, bandwidth, executing and parallel time. Furthermore the results, are analyzed and optimized using the schemes from chapter 3. Lastly the code is executed remotely on the supercomputer ``Piritakua'' at the Department of Multidisciplinary Studies Yuriria, University of Guanajuato

\section{Supercomputer ``Piritakua''}

The experiments are carried out using the supercomputer “Piritakua”. The massive GPU cluster was design and built by Dr. Claudio from the University of Guanajuato Multidisciplinary Studies Yuriria. The GPU cluster is located at a small town of Mexico, Yuriria. The supercomputer at the Front-end has a eight core Intel Xeon at 2.4 Ghz, at the back-end several GPU are connected, one NVIDIA Tesla K20, two Tesla M2070 and a GTX 580.

A GNU$ \ $LINUX distribution installed on the system, the CentOS 64 bits version 6.4. CentOS stands for Community Enterprise Operating System which is free operating system and one of the most popular GNU$ \ $Linux distribution for web servers and as well is supported by RHEL (Red Hat Enterprise Linux). \cite{centos} 

The specifications of the front-end cluster.

\begin{tabular}{ | p{7.1cm}  | l | l | l |}
  \hline
  Processor & Number & Cores & RAM  \\
  \hline
  Server Dell Intel Xeon E5620 2.4 GHz & 1 & 8 & 12 GB \\
  \hline
  Server HP Proliant SL 350s Gen3 Intel Xeon X5650 2.67 GHz & 2 & 24 & 32 GB \\
  \hline
   Server HP Proliant SL 250s Gen8 Intel Xeon E5-2670 2.60 GHz & 3 & 48 &104 GB \\
   \hline
   CPU Xeon Phi  5110p & 1 & 8 & 8 GB\\
   \hline
   CPU Xeon Phi 7120p  & 1 & 8 & 16 GB\\
   \hline
  \end{tabular}

 The CUDA Code was launched on only two CPUs, a laptop with a eight core intel i7-3630QM and a high-end CPU Xeon Phi 7120p from the cluster. In addition the Xeon Phi was used for all the experiments for the Cluster's GPUs. The Xeon Phi 720p is capable of achieving f 1.2 teraflops of double precision floating point instructions with 352 GB/sec memory bandwidth at 300 W.

When accessing ``Piritakua'' remotely is possible to use all the GPUs available on the cluster.
 The specifications of the GPU connected to the back-end are as follow, CC stands for compute capability.

  \begin{tabular}{ |  l  |  l  |  l  |  l  |  l  | l | l |}
    \hline
    Model & Cores & RAM & DP & SP & Bandwidth & CC \\
    \hline
    Tesla K20m & 2496 & 5GB & 1.17 Tflops & 3.52 Tflops & 208 GB/s & 3.5 \\
   \hline
    Tesla M2070 & 448 & 6GB & 515 Gflops & 1030 Gflops & 150 GB/s & 2.0 \\
   \hline
     Tesla C2050 & 448 & 2.5GB & 512 Gflops & 1030 Gflops & 144 GB/s & 2.0  \\
   \hline
      GeForce 580 & 512 & 1.5GB & 520 Gflops & 1,154 Gflops & 192.2 GB/s & 2.0  \\
   \hline
   GeForce 670MX & 960 & 3GB & 520 Gflops & 1,154 Gflops & 67.2 GB/s & 3.0  \\
   \hline
  \end{tabular}

   The code was launched on all Piritakua's GPUs and on external GeForce GTX 670m, located on a laptop. The "m" stands for the mobil graphic card version. In addition the 670m card is design for less power usage, but with high graphics power, it even has more cores than some Tesla models, however this types of cards has way more less Bandwidth than standard versions.

  
\subsection{Architecture Differences}

  Architecture dependent technical differences of NVIDIA GPUs. During CUDA development a lot of internal features have been improved, but most paradigms for the programmer stayed the same. For example a streaming processor can now handle 2048 threads at a time, but the maximum block size stayed at 1024. This results in a 100$\%$ theoretical occupancy for block sizes of 1024 compared to 66$\%$ of Fermi. Another example is the use of Shared Memory. Maxwell has 64KB dedicated Shared Memory. The maximum amount of Shared Memory per Block is 48KB for all three architectures. \cite{hoermanngpu}
  
  There are two GPU architectures that code was launched, the Fermi and the Kepler. The Tesla K20m is base on ``Kepler'' GPU architecture and the Tesla M2070, M2050 and GeForce GTX 580 on the Fermi architecture. The Kepler is a newer architecture than the Fermi. The big difference between the is the number of CUDA cores per SM. Roughly summarized as:
  
\begin{table}[h]
\centering
  \begin{tabular} { | l | l  | l | l | l  |  l  | l |}
    \hline
    Name & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} &  \multicolumn{2}{|c|}{Maxwell} \\
    \hline
    Compute Capability & 2.0 & 2.1 & 3.0 & 3.5 & \multicolumn{2}{|c|}{5.0}\\
   \hline
    Single Precision Operation per Clock/SM & 32 & 48 & \multicolumn{2}{|c|}{192} & \multicolumn{2}{|c|}{128}\\
   \hline
    Double Precision Operation per Clock/SM & $4/16^1$ & 4 & 8 & $8/64^2$ & \multicolumn{2}{|c|}{$1^3$}\\
   \hline
    Max Number of Threads per SM / SM & \multicolumn{2}{|c|}{16} & \multicolumn{4}{|c|}{32}\\
   \hline
    Max Number of Registers per Thread/SM & \multicolumn{3}{|c|}{1536} & \multicolumn{3}{|c|}{2048}\\
   \hline
       Max Number of Threads per Block & \multicolumn{6}{|c|}{1024}\\
   \hline
   Active Thread Blocks per SM / SM & \multicolumn{2}{|c|}{8} & \multicolumn{2}{|c|}{16} & \multicolumn{2}{|c|}{32}\\
   \hline
   Max Warps per Multiprocessor/ SM & \multicolumn{2}{|c|}{48} & \multicolumn{4}{|c|}{64}\\
   \hline
   Registers / SM & \multicolumn{2}{|c|}{32K} & \multicolumn{4}{|c|}{64K}\\
   \hline
   Level 1 Cache & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Shared Memory / SM & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Warp Size & \multicolumn{6}{c|}{32}  \\
   \hline
  \end{tabular}
  \end{table}
  
\subsection{Experiment metrics}

The initial implementation was launched on several GPU nodes as well as several times on the same node. The implementation was launched 10 times on each node, For example the following table are the results of the initial implementation launched on the NVIDIA Tesla K20m.

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l  |  }
    \hline
    number & time  \\
    \hline
    1 &  64846.0 \\
   \hline
    2 & 59010.4 \\
   \hline
    3 & 67332.4\\
   \hline
    4 & 68171.8 \\
   \hline
    5 & 61818.0 \\
   \hline
    6 & 65111.3\\
    \hline
    7 &  67346.4\\
    \hline
    8 & 65666.6 \\
    \hline
    9 & 67343.3 \\
    \hline
    10 & 65681.8      \\
   \hline
  \end{tabular}
  \end{table}
  
As we can see each time the code is executed, different time results. The order in which operations are evaluated can significantly affect the final value of a floating point computation; t
It's possible that the first issued block might be issued to different multiprocessor each time, and if there are an uneven number of blocks across multiprocessors, then the execution order could vary slightly between runs \cite{cook}.

As we know there is no guaranty that threads will execute at the same order. To accomplish such task thread synchronization is needed.

\subsection{Validation}



\section{Results}

The CUDA code was launched on each GPU of the "Piritakua" supercomputer. As we know the supercomputer has different GPU, as well as several architectures and different number of CUDA cores.


mention CUDA versions. as well as the compiler used for the test.


\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l  |  l  | l |}
    \hline
    Node & initial Avg time & 1st optimization & 2nd optimization \\
    \hline
    Tesla K20m &  64846.0 & 55 & 33\\
   \hline
    Tesla M2070 & 64846.0 & 55 & 33\\
   \hline
    GeForce GTX 580 & 64846.0 & 55 & 33\\
   \hline
    GeForce 670m & 64846.0 & 55 & 33\\
   \hline
  \end{tabular}
  \end{table}
  
  

  
  
\subsection{Initial Test}

The initial implementation was launched on several GPU nodes, as previous mentioned.  As we know. Se figure.\ref{fig:iniresults}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/initial_results.png}
		\rule{35em}{0.2pt}
	\caption[Initial GPU results]{Initial results of the implementation running on several different GPU nodes}
	\label{fig:iniresults}
\end{figure}

\subsubsection{Visual profiler}

The visual profiler.

The visual profiler was used on Laptop with GeForce GTX 670m with the intel eight core i7-3630QM.


the

command

 nvprof –o nvprof.log ./command
 
 
 \subsection{Branching}
 
 CUDA follows the Single Instruction Multiple Thread architecture. This means that there are running several threads executing the same code. Each thread can operate on its own data and has its own address counter. They are free to use each data dependent path. But also each thread is executing the same operation at the same time. When a thread within a warp branches differently the other threads get deactivated. This can be described by the following listing and illustrated by \cite{hoermanngpu}.
 
 \ref{fig:threads}
 
\begin{lstlisting}[language=C++, caption={CPU Vector Addition}]
__global__ void kernel(int* out){
 
idx = threadIdx.x;
int result;

if(idx == 0){
	result = foo();
} else {
	result = bar();
	out[idx] = result;
}
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/threads.png}
		\rule{35em}{0.2pt}
	\caption[he execution flow]{The execution flow of a branching code, with warp size 8. Black arrows are active threads, and the grey ones are disabled.
}
	\label{fig:threads}
\end{figure}


This problem ocurr when analysing the boundary condition for the matrices, only a single kernel was checking boundaring. Which a bottleneck ocurr. Bec

This implementation gets the job done, however a minor part of the threads are working, which is a waste of computation. 
\begin{lstlisting}[language=C++, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel}]

//Compute laplacian in Y direction
__global__ void glaplaciany(...);

//Compute laplacian in X direction
__global__ void glaplacianx(...);

__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0)
    {
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1)
    {
  		// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2)
    {
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1)
    {
        // Update Laplacian Boundaries
    }
}
\end{lstlisting}

Solving the issue is including more work on the laplacian boundaries calculation. 

\begin{lstlisting}[language=C++, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel}]

__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0)
    {
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1)
    {
  		// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2)
    {
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1)
    {
        // Update Laplacian Boundaries
    }
    
    //Compute laplacian in Y direction
    glaplaciany(...);

	//Compute laplacian in X direction
	glaplacianx(...);
    
}
\end{lstlisting}

The optimization

\subsection{Occupancy}
 
 
 
 Increase the used of constant memory in the device with values that only change during run-time for example the code int 
 
 
 \begin{lstlisting}[language=C++, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel}]
 gsource << <blocks, threads >> >(u_val, dev_sm_z, dev_mz, NXCUDA);
  
 sfrelax_y[index] = -deltam_y[index] / tau_sf;
     
 DELTAX = (double)TX / (double)NX;
\end{lstlisting}
 
 

\subsection{Concurrent Kernels}

Initially each kernel was launched in the default steam 0, the figure \ref{fig:streams} illustrates such result in the Visual Profiler. Each kernel that is being launched cannot run simultaneously because the next kernel launches needs to compute data, in other words the kernels are not independent from each other.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/ini_steams.png}
		\rule{35em}{0.2pt}
	\caption[Initial Streams]{Kernels running in the default 0 Stream.
}
	\label{fig:streams}
\end{figure}


The gsolution kernel computes the Zhang an Li model for x, y, z coordinates using extensively the global memory from the device. The kernel which is launched in the default stream cannot run in parallel with others kernels. To manage concurrency the kernel is divided into three kernels which computes individual each coordinate. 

\begin{lstlisting}[language=C++, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel}]
__global__ void gsolution(double *sfrelax_x, double *sfrelax_y, double *sfrelax_z,
                          double *sm_x, double *sm_y, double *sm_z,
                          double *sdex_x, double *sdex_y, double *sdex_z,
                          double *lapl_x, double *lapl_y, double *lapl_z,
                          double *deltam_x, double *deltam_y, double *deltam_z,
                          int grid_width)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
  	int j = blockIdx.y * blockDim.y + threadIdx.y;
    
  	int index = j * grid_width + i;

  	if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
  	{
  		deltam_x[index] = sfrelax_x[index] + sdex_x[index] + lapl_x[index]
                          - sm_x[index];
     	deltam_y[index] = sfrelax_y[index] + sdex_y[index] + lapl_y[index]
                          - sm_y[index];
     	deltam_z[index] = sfrelax_z[index] + sdex_z[index] + lapl_z[index]
                          - sm_z[index];
  }
}
\end{lstlisting}

To achieve concurrent kernels the streams need to access memory blocks that are pinned to a stream. So each memory block corresponding x, y, z are mapped to 3 streams, furthermore all the matrices corresponding for example x are mapped to stream 1. 


\begin{lstlisting}[language=C++, caption={Evaluation of individual coordinates of the Zhang and Li model}]
__global__ void gsolution(double *deltam,
					      double *sfrelax, double *sm, double *sdex, double *lapl)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
 
    int index = j * NXCUDA_CONST + i;

    if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
    {
        deltam[index] = sfrelax[index] + sdex[index] + lapl[index] - sm[index];
    }
}
\end{lstlisting}


This same method is applied to every kernel that can be separated into three kernels calls. Some kernels cannot be separate such as the cross product, because the product uses pinned memory block from the other streams. Instead of running one big kernel, three individual kernels are launched simultaneous. The figure \ref{fig:concurrent} shows the results of concurrent kernels in the Tesla K20.

\begin{lstlisting}[language=C++, caption={Evaluation of individual coordinates of the Zhang and Li model}]
for (int i  = 0; i < 3; i++)
{
gsolution<<<blocks, threads, 0, stream[i]>>>(spinAccXYZ[i]->getDev_deltam(),
											 spinAccXYZ[i]->getDev_sfrelax(), 
											 spinAccXYZ[i]->getDev_sm(), 
											 spinAccXYZ[i]->getDev_sdex(),
											 spinAccXYZ[i]->getDev_lapl());
}
\end{lstlisting}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/concurent.png}
		\rule{35em}{0.2pt}
	\caption[Streams kernels Tesla K20]{Concurrent kernels in the Tesla K20
}
	\label{fig:concurrent}
\end{figure}


\subsubsection{Results}

graphic.

k20  6.0 speed up.

However


\subsection{Shared Memory}

As we know shared memory is faster than global memory, however shared memory is very limited. 

To increase occupancy

\begin{lstlisting}[language=C++, caption={Evaluation of individual coordinates of the Zhang and Li model}]


\end{lstlisting}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{Figures/shared.png}
		\rule{35em}{0.2pt}
	\caption[Shared Memory Strategy]{Shared Memory Strategy }
	\label{fig:shared}
\end{figure}

http://www.bu.edu/pasi/files/2011/07/Lecture31.pdf


\subsubsection{Results}


\subsection{Structure of Arrays, SAO}


AoS and SoA refer to "Array of Structures" and "Structure of Arrays" respectively. These two terms refer to two different ways of laying out your data in memory. This is illustrated in figure \ref{fig:aos} and \ref{fig:sao} respectively. AOS, grouping properties of an object together and making an array of those objects in memory, whereas a structure of arrays would be a single structure in which you make an array for each property. The structure of arrays can allow for better cache utilization, easier to access continues data, making better use of each read you make from memory, providing a more effective route to memory. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/aos.png}
		\rule{35em}{0.2pt}
	\caption[Array of structures (AOS)]{AOS memory layout }
	\label{fig:aos}
\end{figure}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/soa.png}
		\rule{35em}{0.2pt}
	\caption[Structure of Arrays (SAO)]{SAO memory layout}
	\label{fig:sao}
\end{figure}


The initial implementation the  x, y, z data was allocated in separated blocks. Furthermore when accessing blocks of the the same coordinates, the register access the data as the figure\ref{fig:aos}.

\begin{lstlisting}[language=C++, caption={AOS implementation}]
deltam_x = (double **)calloc(NYCUDA, sizeof(double));
deltam_y = (double **)calloc(NXCUDA, sizeof(double *));
deltam_z = (double **)calloc(NXCUDA, sizeof(double *));
\end{lstlisting}

To solve this issue, first a custom class named GPUMatrix was programmed. Moreover it allocated the data for each coordinate and free the memory automatically when the simulation is done. The allocation of the memory is ralated to the operations that are being done by the kernels for faster memory access. 

\begin{lstlisting}[language=C++, caption={SOA implementation}]

    GPUMatrix<T> *dev_deltam;
    GPUMatrix<T> *dev_sdex; //Exchange term
    GPUMatrix<T> *dev_sfrelax;
    GPUMatrix<T> *dev_m; 
\end{lstlisting}

\subsubsection{Results}



\section{Optimized}



Summarize the data

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Shared 2 \\
    \hline
    Tesla K20m & 0 & 0 & 0 & 0 & 0 & 0\\
   \hline
    Tesla M2070 & 107468.0 & 0 & 130754.1 & 97343.4 & 73938.1 & 0\\
    \hline
    Tesla C2050 & 106303.6 & 0 & 128516.6 & 96762.0 & 72964.5 & 0\\
   \hline
    GeForce GTX 580 & 69341.9 & 0 & 76481.9 & 70567.1 & 51603.7 & 0\\
   \hline
    GeForce 670m & 0 & 0 & 0 & 0 & 0 & 0\\
   \hline
  \end{tabular}
  \end{table}
  
  
  seed-up
  \begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Shared 2 \\
    \hline
    Tesla K20m & 0 & 0 & 0 & 0 & 0 & 0\\
   \hline
    Tesla M2070 & 3.512x & 0 & 2.888x & 3.879x & 5.107x & 0\\
    \hline
    Tesla C2050 & 3.550x & 0 & 2.938x & 3.902x & 5.175x & 0\\
   \hline
    GeForce GTX 580 & 5.443x & 0 & 4.937x & 5.351x & 7.317x & 0\\
   \hline
    GeForce 670m & 0 & 0 & 0 & 0 & 0 & 0\\
   \hline
  \end{tabular}
  \end{table}
  
  
  \vspace{3.2em}


Finally, 
  
