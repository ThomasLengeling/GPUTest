% Chapter Template

\chapter{Optimization Results} % Main chapter title

\label{Optimization Results} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Optimization Results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title


In this chapter we obtain the results of the CUDA code implementation launched on a single GPU device. The tests were performed on various GPUs architectures. The application is analyzed on various stages using NVIDIA's Visual Profiler. In addition, the CUDA kernels were evaluated in performance, execution time, occupancy and concurrent kernels. Furthermore, the results, are analyzed and optimized using the schemes from Chapter \ref{Heterogeneous Performance Analysis and Practices}. The code is executed remotely on the supercomputer Piritakua at the Department of Multidisciplinary Studies Yuriria, University of Guanajuato. The last section is the overview of all the optimizations results achieved on the simulation.

\section{Supercomputer Piritakua}

The experiments are carried out using the supercomputer Piritakua. The massive GPU cluster was design and built by Dr. Claudio from the University of Guanajuato at Yuriria's Multidisciplinary Studies. The cluster is located at Yuriria, a small town in the center of Mexico. Piritakua at its front-end has an eight core Intel Xeon at 2.4 Ghz, at the back-end several GPU are connected. All of GPUs CUDA version 5.0 was installed. Furthermore, the GPUs node are; one NVIDIA Tesla K20, two Tesla M2070 and a GeForce GTX 580 

The cluster has the CentOS ( Community Enterprise Operating System) 64 bits as operating system. The OS is a free operating system and one of the most popular GNU$ \ $Linux distribution for web servers and as well is supported by RHEL (Red Hat Enterprise Linux) \cite{centos}. The specifications of cluster are \ref{tab:cpus}.

\begin{table}[h]
\centering
\begin{tabular}{ | p{7.1cm}  | l | l | l |}
  \hline
  Processor & Number & Cores & RAM  \\
  \hline
  Server Dell Intel(R) Xeon(R) E5620 2.4 GHz & 1 & 8 & 12 GB \\
  \hline
  Server HP Proliant SL 350s Gen3 Intel(R) Xeon(R) X5650 2.67 GHz & 2 & 24 & 32 GB \\
  \hline
   Server HP Proliant SL 250s Gen8 Intel Xeon E5-2670 2.60 GHz & 3 & 48 & 104 GB \\
   \hline
  \end{tabular}
      \caption{CPU technical specifications}
  \label{tab:cpus}
  \end{table}

The host code was executed on only two types of CPUs. On an eight core Intel i7-3630QM and on a high-end eight core Intel Xeon CPU E5620. In addition, the Xeon was used in all the simulation test, except when testing on the GeForce 670mx. Lastly, the code was executed on laptop to show the performance comparison between a lightweight GPU and a server based GPU.

The device code was executed using the CUDA Toolkit 5.5 version. The main advantage of the 5.5 version are improvements in MPI(Message Passing Interface) and HyperQ. The MPI implementation was not used for the current simulation. However, we obtain benefits from using the HyperQ when applying concurrent kernels to the implementation. 

When accessing Piritakua remotely is possible to use all the GPUs nodes available on the cluster. The specifications of the GPU connected to the back-end are as follow, CC stands for compute capability.

\begin{table}[h]
\centering
  \begin{tabular}{ |  l  |  l  |  l  |  l  |  l  | l | l | l |l | }
    \hline
    Model & Core& RAM& DP GF& SP GF& Bandwidth& GHz& CC & Power\\
    \hline
    Tesla K20m & 2496 & 5GB & 1,170 & 3,520 & 208GB/s & 0.73 & 3.5 & 225W \\
   \hline
    Tesla M2070 & 448 & 6GB & 515 & 1,030 & 150GB/s & 1.15 &  2.0 & 225W\\
   \hline
     Tesla C2050 & 448 & 2.5GB & 512 & 1,030 & 144GB/s & 1.15  & 2.0 & 238W \\
   \hline
      GeForce 580 & 512 & 1.5GB & 520 & 1,154 & 192.2GB/s & 1.5 & 2.0 & 244W \\
   \hline
   GeForce 670mx & 960 & 3GB & 520 & 1,154 & 67.2GB/s & 0.6 & 3.0 &  - \\
   \hline
  \end{tabular}
    \caption{GPU technical specifications}
  \label{tab:gpus}
  \end{table}
  
  The code was launched on all Piritakua's GPUs and on the GeForce GTX 670m. The "m" stands for the mobil graphic cards. In addition the 670m card is design for less power usage, but high graphics power, it has more cores than some Tesla models, However, they have less Bandwidth than standard versions. The 670m card was used as comparison between laptop GPUs and high-end desktop/servers GPUs.
     
\subsection{Architecture Differences}

  NVIDIA's GPU are constantly being improved over the years, nonetheless, most programming paradigms stayed the same. The are currently three main architectures; Fermi, Kepler and Maxwell. For example, a streaming processor can now handle 2048 threads at a time, but the maximum block size stayed at 1024. The results in a 100$\%$ theoretical occupancy for block sizes of 1024 compared to 66$\%$ of Fermi type. Another example is the use of Shared Memory. Maxwell has 64KB dedicated Shared Memory. The maximum amount of Shared Memory per Block is 48KB for all three architectures \cite{hoermanngpu}.
  
  There are two GPU architectures where the implementation was launched, the Fermi and the Kepler. The Tesla K20m and the GeForce 670mx are based on the ``Kepler'' GPU architecture. The Tesla M2070, M2050 and the GeForce GTX 580 on the Fermi architecture. The Kepler architecture newer than the Fermi. More information about the architectures in the table \ref{tab:arch}. The Maxwell architecture was not used for the current simulation, however, is showed for future reference.
  
\begin{table}[h]
\centering
  \begin{tabular} { | l | l  | l | l | l  |  l  | l |}
    \hline
    Name & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} &  \multicolumn{2}{|c|}{Maxwell} \\
    \hline
    Compute Capability & 2.0 & 2.1 & 3.0 & 3.5 & \multicolumn{2}{|c|}{5.0}\\
   \hline
    Single Precision Operation per Clock/SM & 32 & 48 & \multicolumn{2}{|c|}{192} & \multicolumn{2}{|c|}{128}\\
   \hline
    Double Precision Operation per Clock/SM & $4/16^1$ & 4 & 8 & $8/64^2$ & \multicolumn{2}{|c|}{$1^3$}\\
   \hline
    Max Number of Threads per SM / SM & \multicolumn{2}{|c|}{16} & \multicolumn{4}{|c|}{32}\\
   \hline
    Max Number of Registers per Thread/SM & \multicolumn{3}{|c|}{1536} & \multicolumn{3}{|c|}{2048}\\
   \hline
       Max Number of Threads per Block & \multicolumn{6}{|c|}{1024}\\
   \hline
   Active Thread Blocks per SM / SM & \multicolumn{2}{|c|}{8} & \multicolumn{2}{|c|}{16} & \multicolumn{2}{|c|}{32}\\
   \hline
   Max Warps per Multiprocessor/ SM & \multicolumn{2}{|c|}{48} & \multicolumn{4}{|c|}{64}\\
   \hline
   Registers / SM & \multicolumn{2}{|c|}{32K} & \multicolumn{4}{|c|}{64K}\\
   \hline
   Level 1 Cache & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Shared Memory / SM & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Warp Size & \multicolumn{6}{c|}{32}  \\
   \hline
  \end{tabular}
  \caption{GPU Architecture Specifications}
  \label{tab:arch}
  \end{table}
 
\section{Optimization}

The cluster has two  main architecture types Fermi and Kepler. Furthermore, the initial results of 1.00x speedup were tested on a Kepler architecture, the GeForce GT 650M. The graph \ref{fig:iniresults} illustrates the executing time for all the GPUs in the server.

We used NVIDIA's Visual Profiler to obtain kernel metrics of the Tesla K20m, table \ref{tab:nvprof}. The output is organized by kernel importance and performance during the simulation. The Laplacian kernel evaluation; {\listf glaplacinay}, {\listf gLaplacianx} and  {\listf gLaplacianYBoundaries} uses upto 44.37$\%$ of the overall simulation. The $gsolution$ kernel, which solves Zhang-Li model \ref{eq:zhang} consumes up-to 14.04$\%$. The RK4 integration only exhaust a minor part of the overall simulation. However, the {\listf gSolution}, {\listf gsdExchange} and Laplacian calculation are part of the RK4 integration, which overall is about 99$\%$.

The throughput was not review on the current application. Since only two stages of the simulation transfer of the CPU data occurs, on the initial stage where CPU data is sent to the GPU, and the final stage where is sent back to CPU.

 The optimization focus is to give the GPUs as much work as possible, using at the fullest the GPU hardware capabilities. In addition, reducing the overall performance time of each kernel, eliminating the computational hover-head process on the highest consumed kernel \ref{tab:nvprof}.
 
\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | l |}
    \hline
    Time$\%$& Time & Calls & Avg & Min & Max & Kernel \\
    \hline
    23.50 & 3.6s & 26521 & 137.5us & 96.0us & 597.1us& {\listf gLaplaciany} \\
    \hline
    17.04 & 2.6s & 26521 & 99.7us & 57.0us & 561.1us& {\listf gSolution} \\
    \hline
    16.75 & 2.6s & 26522 & 98.0us & 62.8us & 400.6us& {\listf  gLaplacianx} \\
     \hline
      13.37 & 2.0s & 26522 & 78.2us & 40.8us & 453.8us& {\listf gsdExchange} \\
      \hline
    7.22 & 1.1s & 26522 & 42.2us & 23.4us & 326.0us & {\listf gsfRelaxation} \\
       \hline
    6.22 & 965.2ms & 6630 & 145.6us & 79.2us & 722.6us & {\listf gTerm4RK4}  \\
       \hline
    4.12 & 640.3ms & 26522 & 24.1us & 21.8us  &138.7us & {\listf gLaplacianYBoundaries} \\
       \hline
    3.41  & 529.2ms & 6630 & 79.8us & 41.6us  & 478.8us & {\listf gTerm2RK4} \\
       \hline
    3.36 & 520.8ms & 6630 & 78.5us & 41.5us & 372.2us & {\listf gTerm3RK4} \\
       \hline
    3.35 & 519.5ms & 6631 & 78.3us & 41.1us & 372.2us & {\listf gTerm1RK4} \\
   \hline
  \end{tabular}
  \caption{Kernel time executing and on the Tesla K20}
  \label{tab:nvprof}
  \end{table}
 
  \begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/gpu_initial.png}
		\smallskip
	\caption[Initial GPU results]{Initial implementation benchmarking on several different GPUs nodes, with a initial speedup of 1.0x}
	\label{fig:iniresults}
\end{figure}

The figure \ref{fig:iniresults} illustrates the GeForce GTX 580 is the card with the least amount of execution time, and the Tesla K20m the fastest amongst the Tesla Cards. 


The following sections are the optimization techniques and methods applied to the application. In addition, comparing the performance between the initial implementation and the modified versions. The optimization is breakdown into five steps; Branching, Occupancy, Concurrent Kernels, Shared Memory and Structure of Arrays. Branching, refers how kernels and threads are executed in the application. Occupancy, number of threads per devices being used. Concurrent Kernels, execution several kernels at once. Shared Memory, using as much shared memory as possible. Finally, Structure of Arrays, modification of the memory allocation in the device.

 \subsection{Branching}
 
 CUDA follows the Single Instruction Multiple Thread architecture, meaning, threads execute the same code. Each thread is able to operate on its own data and has its individual address counter. Moreover, threads are free to use a different path. Each thread launches the same operation at the same time. However, they have to wait for all the threads in the kernel to finish their task. In other words, some threads can finish their job before a groups of threads are executing their tasks. Therefore, a thread within a warp/block branches differently the other threads get deactivated \cite{hoermanngpu}. The method is described in the following code \ref{lst:branch} and  illustrated in the figure \ref{fig:threads}.

\begin{lstlisting}[language=C++, label={lst:branch}, caption={Branching threads}]
__global__ void kernel(int* out){
idx = threadIdx.x;
int result;
if(idx == 0){
	result = foo();
} else {
	result = bar();
	out[idx] = result;
}
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/threads.png}
		\smallskip
	\caption[he execution flow]{The execution flow of a branching code, with warp size 8. Black arrows are active threads, and the grey ones are disabled.}
	\label{fig:threads}
\end{figure}

The branching problem occurred in the section where boundary condition for Laplacian was being analyzed \ref{lst:branchlap}. Only a single kernel was used to check the boundary condition. In addition, a bottleneck occurred. The implementation gets the job done with only one kernel. However, a minor part of the threads are only working, which is a waste of computation resources and energy.

\begin{lstlisting}[language=C++, label={lst:branchlap}, caption={Branching problem in the Laplacian boundary condition evaluation}]
__global__ void glaplaciany(...); //Compute Laplacian in Y direction
__global__ void glaplacianx(...); //Compute Laplacian in X direction

__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
  	}
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
}
\end{lstlisting}

To solve the branching issue, we include more work on the laplacian boundary condition kernel. The new kernel evaluates the boundary condition in a single kernel. Therefore, eliminating branching threads, more importantly, reducing global memory calls (code \ref{lst:newcde}).

\begin{lstlisting}[language=C++, label={lst:newcde}, caption={More workload on a single kernel execution}]
__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
    glaplaciany(...); //Compute Laplacian in Y direction
	glaplacianx(...);  //Compute Laplacian in X direction
}
\end{lstlisting}

The technique was applied to all parts of the code. Therefore, eliminated inactive threads. Moreover, activating threads for more computational process. The technique increased the occupancy percentage of active threads within the kernels. The results of the modified version please read the final section of the chapter.

\subsection{Concurrent Kernels}

Initially, each kernel was launched on the default steam zero. Therefore, every kernel was consequently launched in a serial way. The figure \ref{fig:streams} illustrates such results using the NVIDIA's Visual Profiler. Each kernel that is being launched is not able to run simultaneously. Because, each kernel needs previous data to compute the next data. In other words, the kernels are not independent from each other. Therefore, we change the implementation to be able to launch parallel kernels.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/ini_steams.png}
		\smallskip
	\caption[Initial Streams]{Kernels running on the default Stream zero.
}
	\label{fig:streams}
\end{figure}

 Kernels by default cannot run in parallel with others kernels. Furthermore, CUDA doesn't provide an automatic parallel kernel executing. In addition, the programmer needs to tell the CUDA compiler that some portion of the code or kernel should be run in parallel. However, the compiler does not always know when to use concurrent kernels, it depends on the hardware capabilities and as well the number of threads per block and the number of SM available. If the compiler finds available space to run another kernel simultaneously, it will do so. 

For example, the {\listf gsolution} \ref{lst:concurrent} kernel computes the Zhang and Li model for x, y, z coordinates, which extensively uses the global memory of the device. To accomplish concurrent kernels, the streams should be able to access memory blocks that are pinned to a specific stream. Therefore, each memory block corresponding to x, y, z coordinate are mapped to three independent streams. Furthermore, all the matrices corresponding to the coordinate x are mapped to the stream 1, coordinate y to stream 2 and coordinate z to stream 3.

\begin{lstlisting}[language=C++, label={lst:concurrent}, caption={Evaluation of x, y, z coordinates of the Zhang and Li model in a single kernel.}]
deltamX[index] = sfrelaxX[index] + sdexX[index] + laplX[index] - smX[index];
deltamY[index] = sfrelaxY[index] + sdexY[index] + laplY[index] - smY[index];
deltamZ[index] = sfrelaxZ[index] + sdexZ[index] + laplZ[index] - smZ[index];
\end{lstlisting}
 
The CUDA code \ref{lst:concurrent} is divided into a single kernel \ref{lst:consingle}. In addition, the new generic kernel is launched parallel with the others kernels. Instead of running one big kernel, three individual kernels are launched simultaneously. Dividing each kernel is now possible to implement shared memory through each kernel, otherwise wasn't possible. 

\begin{lstlisting}[language=C++, label={lst:consingle}, caption={Evaluation of individual coordinates of the Zhang and Li model}]
int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
int j = blockIdx.y * blockDim.y + threadIdx.y;
int index = j * NXCUDA_CONST + i;

if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
	deltam[index] = sfrelax[index] + sdex[index] + lapl[index] - sm[index];
\end{lstlisting}

This same method was applied to every kernel that was possible to separate into three kernels calls. Some kernels were not viable to be separated, such as the cross product. Because, the cross product uses pinned memory block from the other streams. The figure \ref{fig:concurrent} illustrates the results of concurrent kernels in the Tesla K20.

\begin{lstlisting}[language=C++, caption={Evaluate Zhang and Li model.}]
for (int i  = 0; i < 3; i++)
	gsolution<<<blocks, threads, 0, stream[i]>>>(spinAccXYZ[i]->getDev_deltam(),
												 spinAccXYZ[i]->getDev_sfrelax(), 
												 spinAccXYZ[i]->getDev_sm(), 
											 	 spinAccXYZ[i]->getDev_sdex(),
											 	 spinAccXYZ[i]->getDev_lapl());
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/concurent.png}
		\smallskip
	\caption[Streams kernels Tesla K20]{Concurrent kernels in the Tesla K20 using NVIDIA's Visual Profiler.}
	\label{fig:concurrent}
\end{figure}

In chapter \ref{Implementation of Domain Wall Dynamics under Nonlocal STT} we mentioned that the input data set 480 x 120 is mapped to a CUDA square grid of 512 x 128. Because the mapping of the extra threads, is possible to execute a portion of kernels concurrently.

Concurrent kernels demonstrate a very promising technique to achieve a huge performance increment in the current simulation. In theory is possible to have multiple kernels executing at the same. However, there are some downsides to the implementation; correctly synchronize kernels, waiting time and hardware resources are among the  problems \cite{practices}. The timeline of the application \ref{fig:waittime} illustrates the waiting time between kernels execution. However, the waiting time are very small time steps between 0.01ms and 0.01ms, but waiting time occurs for each step of the RK4, appears approximate 45,00 times. Furthermore, branching the kernel execution process should eliminate the issue.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/waittime.png}
		\smallskip
	\caption[Waiting time in concurrent kernels]{Waiting time between each concurrent kernel execution}
	\label{fig:waittime}
\end{figure}


\subsection{Shared Memory}

Shared memory is faster than global memory(read Chapter \ref{Heterogeneous Performance Analysis and Practices} for more reference), however, shared memory is very limited. To be able to implement shared memory in the kernels, we needed the kernels to be separated in their x, y and z coordinate, as mentioned in the previous section. In addition, this allows us to implement shared memory across each kernel, otherwise wouldn't be possible. Shared memory was applied in all the kernels were global memory was used extensively. Eliminating the number clock cycles per thread.

The idea behind shared memory is to reduce the amount of global memory calls, which has about 400-600 clock cycles, while the shared memory only 1-32 clock cycles \ref{fig:memory}. The shared memory implementation is accomplish by allocating the data from the thread block into a temporary array, in other words the shared memory. In addition, the kernel is able to performed calculations on the temporary array and write the values onto the global memory. The implementation code is illustrated in the listing \ref{lst:shared}. There is no guaranty that threads will execute at the same order. Using {\listf \_\_syncthreads()} will wait until all threads have completed their task, in this case loading global memory into the shared memory array. Chapter \ref{Heterogeneous Performance Analysis and Practices} section thread synchronization, has more information about thread synchronization and shared memory. Once all the operations on the shared memory array are finish. The final part is to write the shared memory values back to the the global memory.

\begin{lstlisting}[language=C++, label={lst:shared}, caption={Shared memory}]
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
int index = j * NXCUDA_CONST + i;

if (i > 1 && i < NX + 2 && j >= 0 && j < NY){
     int cacheIdx = threadIdx.y * blockDim.x + threadIdx.x;
     __shared__ double deltamS[THREADS_SHARED * THREADS_SHARED];

	 //load memory into shared memory
     deltamS[cacheIdx] = operationGlobal(globalMemory);
     __syncthreads();

	 //copy back the shared memory to global memory
     deltam[index]  = deltamS[cacheIdx];
}
\end{lstlisting}

To calculate the Laplacian, we need to access a great amount of global memory, therefore, located near the value of interest. In this case a region of 4x4 grid. The figure \ref{fig:shared} illustrates what part of the block is used for allocating shared memory and global memory. The global memory is used for the boundary conditions of the block, while the shared memory for all the values inside the block.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.4\textwidth]{Figures/shared.png}
		\smallskip
	\caption[Shared Memory Strategy]{Shared Memory Strategy for Laplacian evaluation }
	\label{fig:shared}
\end{figure}

The code \ref{lst:lapsh} demonstrates to how calculate the Laplacian from the equation \ref{eq:nn} with the implementation of shared memory. First we load all the global memory into a temporary array, the shared memory. Then we performed the calculation on the shared memory as mentioned before. Lastly return data to the global memory.

\begin{lstlisting}[language=C++, label={lst:lapsh}, caption={Laplacian evaluating using shared memory with boundaries condition}]
if (i >= 0 && i < NX && j >= 0 && j < NY){
    __shared__ double lapS[ THREADS_SHARED * THREADS_SHARED];
    lapS[sIdx] = deltam[Index];
    __syncthreads();

    if (threadIdx.x >= 2 && i < threadIdx.x - blockDim.x -2){ //shared
       lapy[idx] = - lapS[sIdx + 2] / 12.0 + 4.0 * lapS[sIdx + 1] / 3.0
			  	   - 5.0 * lapS[sIdx] / 2.0
			  	   - lapS[sIdx - 2] / 12.0 + 4.0 * lapS[sIdx - 1] / 3.0;
	else{ //global memory
		lapy[idx] = - deltam[idx + 2] / 12.0 + 4.0 * deltam[idx + 1] / 3.0
			  		- 5.0 * deltam[idx] / 2.0
			  		- deltam[idx - 2] / 12.0 + 4.0 * deltam[idx - 1] / 3.0;
	}
}
\end{lstlisting}

The current approach seems very promising for reducing global memory. However, great amount of time is spent on loading data onto the shared memory array, In consequence delaying threads executing, resulting a decrease in performance. Fast allocating shared memory data is the optimal solution to ensure the optimal use of this type of memory. The results of such implementation are in the following section, optimization results.

\subsection{Structure of Arrays, SAO}

AoS and SoA refer to "Array of Structures" and "Structure of Arrays" respectively. These two terms refer to two different ways of laying out your data in memory. This is illustrated in figure \ref{fig:aos} and \ref{fig:sao} respectively. AOS, grouping properties of an object together and making an array of those objects in memory, whereas a structure of arrays would be a single structure in which you make an array for each property. The structure of arrays can allow for better cache utilization, easier to access continues data, making better use of each read you make from memory, providing a more effective route to memory. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/aos.png}
		\smallskip
	\caption[Array of structures (AOS)]{AOS memory layout }
	\label{fig:aos}
\end{figure}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/soa.png}
		\rule{35em}{0.2pt}
	\caption[Structure of Arrays (SAO)]{SAO memory layout}
	\label{fig:sao}
\end{figure}

The initial implementation the  x, y, z data was allocated in separated blocks. Furthermore when accessing blocks of the the same coordinates, the register access the data as the figure\ref{fig:aos}.

\begin{lstlisting}[language=C++, caption={AOS implementation}]
deltam_x = (double **)calloc(NYCUDA, sizeof(double *));
deltam_y = (double **)calloc(NXCUDA, sizeof(double *));
deltam_z = (double **)calloc(NXCUDA, sizeof(double *));
\end{lstlisting}

To solve the issue, a custom class GPUMatrix was programmed. The class contained all the matrices for the device. Moreover, the classes allocated the data for each Matrix and free the memory automatically when the simulation is over. The was allocated in a structure that easier for the device to access common elements. For example, evaluating operations only on the x coordinate, the kernel physically access matrices that are near by. Eliminating unassary shift in registers, or hierarchy memory access.

\begin{lstlisting}[language=C++, caption={SOA implementation}]
    GPUMatrix<T> *dev_deltam;
    GPUMatrix<T> *dev_sdex; //Exchange term
    GPUMatrix<T> *dev_sfrelax;
    GPUMatrix<T> *dev_m; 
\end{lstlisting}

The current approach eliminates unnecessary data shift in registers, and is able to stack more values per registers. In theory more computational time per threads. The results of such implementation is illustrate on the last section of the chapter. With the new implementation the code become more readable for future improvements. 

\subsection{Occupancy}
 
Firstly, we increased the use of constant memory in the device, eliminating redundant evaluations of variables and operations. The results are increase in performance and more computational workload on each thread. In addition, constant memory modifications are illustrated in the code \ref{lst:constant}. Matrix calculation for the boundary conditions \ref{eq:matrix3} and \ref{eq:matrix3} were implemented using constant Memory, reducing unnecessary calls in the device.
 
 \begin{lstlisting}[language=C++,  label={lst:constant}, caption={Constant Memory changes}]
 gsource << <blocks, threads >> >(u_val, dev_sm_z, dev_mz, NXCUDA);
 
 sfrelax_y[index] = -deltam_y[index] / tau_sf;
     
 DELTAX = (double)TX / (double)NX;
\end{lstlisting}
 
The different numbers of threads per block and as well the number of blocks per grid can dramatically increase or decrease the performance of the application. The table \ref{tab:ocu} illustrate the different threads per block configuration on the GeForce GTX 580. Using NVIDIA's Profiler is possible to obtain the occupancy percentage of threads the device. The initial configuration for the Fermi and the Kepler was 32x32 threads per block for global memory and 16x16 threads per block for the shared memory. We found, that the optimal configuration for the Fermi cards was 16x16 threads per block and as well for the shared memory and for the Kepler cards was 32x32 threads per block for both memory types. 

\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | }
    \hline
    Threads & Shared & speedup & time & Occupancy \\
    \hline
     8x8 &  8x8 & 7.217x & 52318.3  & 56.6\% \\
    \hline
     16x16 & 8x8 & 7.625x & 49517.3 & 86.6\% \\
    \hline
    16x16 & 16x16 & 7.978x & 47329.2 & 100.0\% \\
    \hline
    32x32 & 16x16 & 7.356x & 51333.4 & 66.6\% \\
    \hline
    32x32 & 32x32 & \multicolumn{3}{|c|}{Failed}\\
    \hline
  \end{tabular}
  \caption{Threads per block configuration and occupancy on the Fermi architecture}
  \label{tab:ocu}
  \end{table}


\section{Optimization results}

This section is a overview of the optimization results compared with the initial CUDA implementation. Each version of the code is compared with the first test results. The figure \ref{fig:gpuop} and \ref{fig:speedup} illustrates the the time execution and the speedup respectively for the tables \ref{tab:time} and \ref{tab:speed}. The final version of the code is the Occupancy. Moreover, the greatest performance occurred on the GeForce GTX 580 Card with 8.0x speedup \ref{fig:speedup}.

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy \\
    \hline
    Tesla K20m & 107322.7 & 101513.4 & 97106.0 & 90201.7 & 68988.2 & 66456.0\\
   \hline
    Tesla M2070 & 110912.3 & 103212.4 & 130754.1 & 97343.4 & 73938.1 & 70299.3\\
    \hline
    Tesla C2050 & 109635.1 & 101212.4 & 128516.6 & 96762.0 & 72964.5 & 69358.1\\
   \hline
    GeForce GTX 580 & 70002.7 & 68712.2 & 76481.9 & 68567.1 & 51603.7 & 47213.2\\
   \hline
    GeForce 650m & 244372.9 & 237371.9 & 227237.8 & 279804 & 181217.4 & 174419\\
   \hline
  \end{tabular}
    \caption{GPU Optimization time}
  \label{tab:time}
  \end{table}
  
  The table \ref{tab:time} displays the overall executing time for all the version of the code, original, constant, streams, shared memory, SAO and Occupancy. We can see compared the initial time and the final there is a difference between 40s and 50s time decrease. The time reduction is relatively low. There is no big difference in waiting 100s or 66s to a simulation to complete. However, if we increase the data set to five decimal points, the simulation can take up-to a couple of hours or days. Finally, the speedup comparison in table \ref{tab:speed} and figure \ref{fig:speedup} illustrates how much performance increase we could obtain in a new simulation using the new implementation.
  
  \begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy\\
    \hline
    Tesla K20m & 3.517x & 3.718x & 3.888x & 4.186x & 5.473x & 5.682x\\
   \hline
    Tesla M2070 & 3.403x & 3.534x & 2.888x & 3.879x & 5.107x & 5.371x\\
    \hline
    Tesla C2050 & 3.442x & 3.571x & 2.938x & 3.902x & 5.175x & 5.444x\\
   \hline
    GeForce GTX 580 & 5.391x & 5.521x & 4.937x & 5.551x & 7.317x & 8.0x\\
   \hline
    GeForce 670MX & 1.544x & 1.598x & 1.662x & 1.349x & 2.084x & 2.163x\\
   \hline
  \end{tabular}
    \caption{GPU Speedup performance}
  \label{tab:speed}
  \end{table}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.98\textwidth]{Figures/gpuOptimization.png}
		\smallskip
	\caption[Overall simulation time]{Overall simulation time}
	\label{fig:gpuop}
\end{figure}

The table \ref{fig:final} illustrates the final profiling results using NVIDIA's profiling tools. On the initial profiling \ref{fig:initial}, the Laplacian evaluation consummend about half of the overall simulation time. However, on the final optimization results\ref{fig:final}, the Laplacian was reduced from 44.37$\%$ to 26.24 $\%$ on execution time. But more importantly the importance of the kernel was reduced, in other words more computational workload on the kernels. The same occurred for the Runge and Kutta term evaluation. The speedup mainly occurred in the Occupancy version of the code, see table \ref{tab:speed}. Furthermore, the {\listf gsdExchangeFull} incremented from 13.37$\%$. 23.35$\%$, which is not necessary good. The increment in time is due to shift in streams operators, the {\listf gsdExchangeFull} is processed in the default stream zero, while the others kernels launched concurrently in a different stream.
\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0 \textwidth]{Figures/speedup.png}
		\smallskip
	\caption[Speedup performance output]{Speedup performance output.}
	\label{fig:speedup}
\end{figure}

\begin{figure}[htbp]
	\centering
	  \begin{tabular} { |  l  |  l | l  | l | l |}
	      \hline
	    Time $\%$& Time & Calls & Avg & Kernel \\
    \hline
   35.24 & 21.33s & 216330 & 98.5us & {\listf gSolTermRK4Relaxation } \\
   \hline
   26.24 & 15.88s & 288441 & 55.0us &  {\listf glaplacianXYBroundaries }\\
   \hline
   23.35 & 14.13s & 160000 & 88.3us & {\listf gsdExchangeFull} \\
   \hline
   15.17 & 9.18s & 72108 & 127.2us & {\listf gSolTerm4RK4Relaxation }\\ 
   \hline
    \end{tabular}
	\caption[Optimization results with the Profiler]{Final optimization results using NVIDIA's profiler, on the Tesla K20m}
	\label{fig:final}
\end{figure}
    
The Tesla K20m was the only GPU which in every code modification it did not lose performance over the course of the optimization process. However, the other GPUs drop performance in the stream optimization stage. The stream process is where each kernel was divided into three separated kernels. Doing this we were able to calculate the x, y, z coordinates independently. In addition, this enable room to implement shared memory across possible kernels. The GPU table specifications \ref{tab:gpus} illustrates that the Tesla K20m is the only GPU card with CC of 3.5. Therefore, has access to Hyper-Q technology. which is able to synchronize automatically concurrent kernels, just by activated a steam process to the kernel.

The SOA optimization, improved overall dramatically the performance of the application, obtaining a 1.2x - 2.0x speedup in all GPU cards, see table \ref{tab:speed} and figure \ref{fig:speedup}. The final version, Occupancy, improved up-to 0.7x speedup on the 580 GPU. However, for the Teslas cards only 0.2x-0.25x speedup. The speedup difference, 0.5x,  is due to the process cycle of the GeForce GPU. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/speed.png}
		\smallskip
	\caption[Optimization speedup overview]{Optimization speedup overview}
	\label{fig:speeduplast}
\end{figure}


We expected that the newest card, the Tesla k20m, would obtain the highest speedup overall, certainly because it has more CUDA cores, the highest compute capabilities. However, if falls behind the GeForce 580 with about 2.5x speedup difference (table \ref{tab:speed}). In addition, the Tesla K20 only had a difference of 0.3x speedup compared with the Teslas Cards, see figure \ref{fig:speedup} and \ref{fig:speeduplast}. The GeForce compared with the others GPUs specifications has most Processor clock (GHz), more mathematical calculations per cycle, se table \ref{tab:gpus} for GPU specifications comparison. The results demonstrate the increase of workload and occupancy on the device. Increasing the computational process per thread and per block.


  \vspace{4.0em}

Finally, the techniques and practices from chapter \ref{Heterogeneous Performance Analysis and Practices} were used to archive speedup and increase performance on the initial CUDA implementation. Methods such as: increase the use of constant memory, shared memory, changed the memory allocating access, analyzed thread branching and finally analyzed kernel occupancy. The highest performance of all GPUS did not occurred on the newest NVIDIA card, the K20m, thus, the most expensive of all the GPUs available in the cluster. The actual improvement appeared on the GeForce 580 (the more GHz of all GPUs) with a 2.32x speedup difference in comparison with the Tesla K20m, table \ref{fig:speedup}. If the problem data set is to be scaled, for example, to a simulation of 8 days, the speedup performance of 8.0x will drastically decrease the time execution in only one day. Moreover, is possible to execute more simulations on the initial time step.
  
