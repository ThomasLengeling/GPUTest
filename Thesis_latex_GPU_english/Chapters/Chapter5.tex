% Chapter Template

\chapter{Optimization Results} % Main chapter title

\label{Optimization Results} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Optimization Results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title


In this chapter we present the results of the CUDA code implementation launched on a single GPU device. The tests were performed on various GPUs architectures. The application is analyzed on various stages using NVIDIA's Visual Profiler. In addition, the CUDA kernels were evaluated in performance, execution time, occupancy and concurrent kernels. Furthermore, the results, are analyzed and optimized using the schemes from Chapter \ref{Heterogeneous Performance Analysis and Practices}. The code is executed remotely on the supercomputer Piritakua at the Department of Multidisciplinary Studies at Yuriria, Guanajuato of the University of Guanajuato. The last section is an overview of all the optimization results achieved in the physics simulation.

\section{Supercomputer Piritakua}

The experiments were carried out using the supercomputer Piritakua. Dr. Claudio from the University of Guanajuato is in charge of maintaining and administrating the GPU Cluster. The supercomputer is located at Yuriria, a small town in the center of Mexico. Nonetheless, it is possible to access to cluster and make test from any part of the world.

The cluster has the CentOS (Community Enterprise Operating System) 64 bits as operating system. The OS is a free operating system and one of the most popular GNU$ \ $Linux distribution for web servers and supported by RHEL (Red Hat Enterprise Linux) \cite{centos}. The specifications of cluster are \ref{tab:cpus}.

\begin{table}[h]
\centering
\begin{tabular}{ | p{7.1cm}  | l | l | l |}
  \hline
  Processor & Number & Cores & RAM  \\
  \hline
  Server Dell Intel(R) Xeon(R) E5620 2.4 GHz & 1 & 8 & 12 GB \\
  \hline
  Server HP Proliant SL 350s Gen3 Intel(R) Xeon(R) X5650 2.67 GHz & 2 & 24 & 32 GB \\
  \hline
   Server HP Proliant SL 250s Gen8 Intel Xeon E5-2670 2.60 GHz & 3 & 48 & 104 GB \\
   \hline
  \end{tabular}
      \caption{CPU technical specifications.}
  \label{tab:cpus}
  \end{table}

The host code was executed on only two types of CPUs: on an eight core Intel i7-3630QM and on a high-end eight core Intel Xeon CPU E5620. In addition, the Xeon was used in all the simulation test, except when testing on the GeForce 670mx. Lastly, the code was executed on laptop to show the performance comparison between a lightweight GPU and a server based GPU.

The device code was executed using the CUDA Toolkit 5.5 version. The main advantage of the 5.5 version are the improvements in MPI(Message Passing Interface) and HyperQ. The MPI implementation was not used for the current simulation. However, we obtained benefits from using the HyperQ when applying concurrent kernels to the implementation. 

When accessing Piritakua remotely, it is possible to use all the GPUs nodes available on the cluster. The specifications of the GPU connected to the back-end are as follow, CC stands for compute capability.

\begin{table}[h]
\centering
  \begin{tabular}{ |  l  |  l  |  l  |  l  |  l  | l | l | l |l | }
    \hline
    Model & Core& RAM& DP GF& SP GF& Bandwidth& GHz& CC & Power\\
    \hline
    Tesla K20m & 2496 & 5GB & 1,170 & 3,520 & 208GB/s & 0.73 & 3.5 & 225W \\
   \hline
    Tesla M2070 & 448 & 6GB & 515 & 1,030 & 150GB/s & 1.15 &  2.0 & 225W\\
   \hline
     Tesla C2050 & 448 & 2.5GB & 512 & 1,030 & 144GB/s & 1.15  & 2.0 & 238W \\
   \hline
      GeForce 580 & 512 & 1.5GB & 520 & 1,154 & 192.2GB/s & 1.5 & 2.0 & 244W \\
   \hline
   GeForce 670mx & 960 & 3GB & 520 & 1,154 & 67.2GB/s & 0.6 & 3.0 &  - \\
   \hline
  \end{tabular}
    \caption{GPU technical specifications.}
  \label{tab:gpus}
  \end{table}
  
The code was launched on all Piritakua's GPUs and on the GeForce GTX 670m. The "m" stands for the mobil graphic cards. In addition, the 670m card is designed for less power usage, but high graphics power. It has more cores than some Tesla models, however, they have less Bandwidth than standard versions. The 670m card was used as comparison between the laptop GPUs and the high-end desktop/servers GPUs.
     
\subsection{Architecture Differences}

  NVIDIA's GPU have constantly been improved over the years. Nonetheless, most programming paradigms have stayed the same. There are currently three main architectures: Fermi, Kepler and Maxwell. For example, a new GPU a streaming processor is able to handle up-to 2048 threads at a time, but the maximum block size stayed at 1024. Another example is the use of Shared Memory. Maxwell has 64KB dedicated Shared Memory. The maximum amount of Shared Memory per Block is 48KB for all three architectures \cite{hoermanngpu}. Table \ref{tab:arch} provides information on the models.
   
  There are two GPU architectures where the implementation was launched: the Fermi and the Kepler. The Tesla K20m and the GeForce 670mx are based on the Kepler GPU architecture. The Tesla M2070, M2050 and the GeForce GTX 580 were implemented on the Fermi architecture. The Kepler architecture is newer than the Fermi. More information about the architectures is provided in the Table \ref{tab:arch}. The Maxwell architecture was not used for the simulation. However, it is showed for future reference.
  
\begin{table}[h]
\centering
  \begin{tabular} { | l | l  | l | l | l  |  l  | l |}
    \hline
    Name & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} &  \multicolumn{2}{|c|}{Maxwell} \\
    \hline
    Compute Capability & 2.0 & 2.1 & 3.0 & 3.5 & \multicolumn{2}{|c|}{5.0}\\
   \hline
    Single Precision Operation per Clock/SM & 32 & 48 & \multicolumn{2}{|c|}{192} & \multicolumn{2}{|c|}{128}\\
   \hline
    Double Precision Operation per Clock/SM & $4/16^1$ & 4 & 8 & $8/64^2$ & \multicolumn{2}{|c|}{$1^3$}\\
   \hline
    Max Number of Threads per SM / SM & \multicolumn{2}{|c|}{16} & \multicolumn{4}{|c|}{32}\\
   \hline
    Max Number of Registers per Thread/SM & \multicolumn{3}{|c|}{1536} & \multicolumn{3}{|c|}{2048}\\
   \hline
       Max Number of Threads per Block & \multicolumn{6}{|c|}{1024}\\
   \hline
   Active Thread Blocks per SM / SM & \multicolumn{2}{|c|}{8} & \multicolumn{2}{|c|}{16} & \multicolumn{2}{|c|}{32}\\
   \hline
   Max Warps per Multiprocessor/ SM & \multicolumn{2}{|c|}{48} & \multicolumn{4}{|c|}{64}\\
   \hline
   Registers / SM & \multicolumn{2}{|c|}{32K} & \multicolumn{4}{|c|}{64K}\\
   \hline
   Level 1 Cache & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Shared Memory / SM & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Warp Size & \multicolumn{6}{c|}{32}  \\
   \hline
  \end{tabular}
  \caption{GPU Architecture Specifications.}
  \label{tab:arch}
  \end{table}
 
\section{Optimization}

The cluster has two  main architecture types Fermi and Kepler. Furthermore, the initial results of 1.00x speedup were tested on a Kepler architecture, the GeForce GT 650M. The Figure \ref{fig:iniresults} illustrates the executing time for all the GPUs in the server.

We used NVIDIA's Visual Profiler to obtain kernel metrics of the Tesla K20m, see Table \ref{tab:nvprof}. The output is organized by kernel importance and performance during the simulation. The Laplacian kernel evaluation ({\listf glaplacinay}, {\listf gLaplacianx} and {\listf gLapBoundaries}) uses up-to 44.37$\%$ of the overall simulation. The $gsolution$ kernel, which solves Zhang-Li Model \ref{eq:zhang} consumes up-to 14.04$\%$. The RK4 integration only exhausts a minor part of the overall simulation. However, the {\listf gSolution}, {\listf gsdExchange} and Laplacian calculation are part of the RK4 integration, which overall is about 99$\%$.

The throughput was not reviewed on the current application. Only two stages of the simulation transfer of the CPU data occurs: on the initial stage where the CPU data is sent to the GPU, and the final stage where the data is retrieved from the GPU

 The optimization focus is to give the GPUs as much work as possible, using at the fullest the GPU hardware capabilities. In addition, reducing the overall performance time of each kernel by eliminating the computational hover-head process on the highest consumed kernel \ref{tab:nvprof}.
 
\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | l |}
    \hline
    Time$\%$& Time & Calls & Avg & Min & Max & Kernel \\
    \hline
    23.50 & 3.6s & 26521 & 137.5us & 96.0us & 597.1us& {\listf gLaplaciany} \\
    \hline
    17.04 & 2.6s & 26521 & 99.7us & 57.0us & 561.1us& {\listf gSolution} \\
    \hline
    16.75 & 2.6s & 26522 & 98.0us & 62.8us & 400.6us& {\listf  gLaplacianx} \\
     \hline
      13.37 & 2.0s & 26522 & 78.2us & 40.8us & 453.8us& {\listf gsdExchange} \\
      \hline
    7.22 & 1.1s & 26522 & 42.2us & 23.4us & 326.0us & {\listf gsfRelaxation} \\
       \hline
    6.22 & 965.2ms & 6630 & 145.6us & 79.2us & 722.6us & {\listf gTerm4RK4}  \\
       \hline
    4.12 & 640.3ms & 26522 & 24.1us & 21.8us  &138.7us & {\listf gLapBoundaries} \\
       \hline
    3.41  & 529.2ms & 6630 & 79.8us & 41.6us  & 478.8us & {\listf gTerm2RK4} \\
       \hline
    3.36 & 520.8ms & 6630 & 78.5us & 41.5us & 372.2us & {\listf gTerm3RK4} \\
       \hline
    3.35 & 519.5ms & 6631 & 78.3us & 41.1us & 372.2us & {\listf gTerm1RK4} \\
   \hline
  \end{tabular}
  \caption{Kernel time executing and on the Tesla K20}
  \label{tab:nvprof}
  \end{table}
 
  \begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/gpu_initial.png}
		\smallskip
	\caption[Initial GPU results]{Initial implementation benchmarking on several different GPUs nodes, with a initial speedup of 1.0x.}
	\label{fig:iniresults}
\end{figure}

Figure \ref{fig:iniresults} illustrates the GeForce GTX 580 which is the card with the least amount of execution time, and the Tesla K20m is the fastest amongst the Tesla Cards. 

The following sections are the optimization techniques and methods applied to the application as well as comparing the performance between the initial implementation and the modified versions. The optimization is broken down into five steps; Branching, Occupancy, Concurrent Kernels, Shared Memory and Structure of Arrays. Branching refers to how kernels and threads are executed in the application. Occupancy refers to the number of threads per devices being used. Concurrent Kernels is the execution of several kernels at once. Shared Memory is using as much shared memory as possible. Finally, Structure of Arrays is the modification of the memory allocation in the device.

 \subsection{Branching}
 
 CUDA follows the Single Instruction Multiple Thread architecture, which means, all threads on the device execute the same code. Each thread is able to operate on its own data and has its individual address counter. Moreover, threads are free to use a different path. Each thread launches the same operation at the same time. However, they have to wait for all the threads in the kernel to finish their task. In other words, some threads can finish their job before  groups of threads are executing their tasks. Therefore, a thread within a warp/block branches differently the other threads get deactivated \cite{hoermanngpu}. The method is described in the following code \ref{lst:branch} and illustrated in Figure \ref{fig:threads}.

\begin{lstlisting}[language=C++, label={lst:branch}, caption={Branching threads.}]
__global__ void kernel(int* out){
idx = threadIdx.x;
int result;
if(idx == 0){
	result = foo();
} else {
	result = bar();
	out[idx] = result;
}
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/threads.png}
		\smallskip
	\caption[Branching execution flow]{The execution flow of a branching code, with a warp size of eight. Black arrows are active threads, and the grey ones are disabled.}
	\label{fig:threads}
\end{figure}

The branching problem occurred in the section where boundary condition for Laplacian was being analyzed \ref{lst:branchlap}. Only a single kernel was used to check the boundary condition. In addition, a bottleneck occurred. The implementation gets the job done with only one kernel. However, a minor part of the threads are only working, which is a waste of computation resources and energy.

\begin{lstlisting}[language=C++, label={lst:branchlap}, caption={Branching problem in the Laplacian boundary condition evaluation.}]
__global__ void glaplaciany(...); //Compute Laplacian in Y direction
__global__ void glaplacianx(...); //Compute Laplacian in X direction

__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
  	}
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
}
\end{lstlisting}

To solve the branching issue, we included more work on the laplacian boundary condition kernel. The new kernel evaluates the boundary condition in a single kernel. Therefore, this refers to eliminating branching threads, more importantly, reducing global memory calls, see Listing \ref{lst:newcde}.

\begin{lstlisting}[language=C++, label={lst:newcde}, caption={More workload on a single kernel execution.}]
__global__ void glaplacianyboundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries
    }
    glaplaciany(...); //Compute Laplacian in Y direction
	glaplacianx(...);  //Compute Laplacian in X direction
}
\end{lstlisting}

The technique was applied to all parts of the code. Therefore, this eliminated inactive threads and activated threads for more computational process. The technique increased the occupancy percentage of active threads within the kernels. The results of the modified version are in the final section of the chapter.

\subsection{Concurrent Kernels}

Initially, each kernel was launched on the default steam zero. Therefore, every kernel was consequently launched in a serial way. Figure \ref{fig:streams} illustrates such results using the NVIDIA's Visual Profiler. Each kernel that is being launched is not able to run simultaneously, because, each kernel needs previous data to compute the next data. In other words, the kernels are not independent from each other. Therefore, we changed the implementation to be able to launch parallel kernels.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/ini_steams.png}
		\smallskip
	\caption[Initial streams]{Kernels running on the default Stream zero.
}
	\label{fig:streams}
\end{figure}

 Kernels by default cannot run in parallel with others kernels. Furthermore, CUDA does not provide an automatic parallel kernel executing. In addition, the programmer needs to tell the CUDA compiler that some portion of the code or kernel should be run in parallel. However, the compiler does not always know when to use concurrent kernels and this depends on the hardware capabilities and as well the number of threads per block and the number of SM available. If the compiler finds available space to run another kernel simultaneously, it will do so. 

For example, the kernel {\listf gsolution} from the Listing \ref{lst:concurrent} computes the Zhang-Li Model for x, y, z coordinates, which extensively uses the global memory of the device. To accomplish concurrent kernels, the streams should be able to access memory blocks that are pinned to a specific stream. Therefore, each memory block, which correspond to x, y, z coordinates is mapped to three independent streams. Furthermore, all the matrices corresponding to the coordinate x are mapped to the stream 1, coordinate y to stream 2 and coordinate z to stream 3.

\begin{lstlisting}[language=C++, label={lst:concurrent}, caption={Evaluation of x, y, z coordinates of the Zhang-Li Model in a single kernel.}]
__global__ void zhangLiComplete(...)
{
	deltamX[index] = sfrelaxX[index] + sdexX[index] + laplX[index] - smX[index];
	deltamY[index] = sfrelaxY[index] + sdexY[index] + laplY[index] - smY[index];
	deltamZ[index] = sfrelaxZ[index] + sdexZ[index] + laplZ[index] - smZ[index];
}
\end{lstlisting}
 
The single kernel {\listf zhangLi} from the Listing \ref{lst:concurrent} is divided into three separated kernels. In addition, the new generic kernel is able to launch parallel kernels, see Listing \ref{lst:consingle}. Instead of running one big kernel, three individual kernels are able to launch simultaneously. With the new implementation, shared memory is no feasible. Perviously this was not possible, due to that the matrices were pinned to different locations of the GPU hardware. 

\begin{lstlisting}[language=C++, label={lst:consingle}, caption={Evaluation of individual coordinates of the Zhang-Li Model.}]
__global__ void zhangLi(  MatrixCoordinate )
{
	int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
	int j = blockIdx.y * blockDim.y + threadIdx.y;
	int index = j * NXCUDA_CONST + i;

	if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
		deltam[index] = sfrelax[index] + sdex[index] + lapl[index] - sm[index];
}
\end{lstlisting}

We applied the current method to all of the kernels which was possible to separate into three kernels calls. Some kernels were not viable to be separated, such as the cross product, which uses memory allocations from different streams.

\begin{lstlisting}[language=C++, caption={Evaluate Zhang-Li Model.}]
for (int i  = 0; i < 3; i++)
	gsolution<<<blocks, threads, 0, stream[i]>>>(spinAccXYZ[i]->getDev_deltam(),
												 spinAccXYZ[i]->getDev_sfrelax(), 
												 spinAccXYZ[i]->getDev_sm(), 
											 	 spinAccXYZ[i]->getDev_sdex(),
											 	 spinAccXYZ[i]->getDev_lapl());
\end{lstlisting}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/concurent.png}
		\smallskip
	\caption[Streams kernels Tesla K20]{Concurrent kernels in the Tesla K20 using NVIDIA's Visual Profiler.}
	\label{fig:concurrent}
\end{figure}

In Chapter \ref{Implementation of Domain Wall Dynamics under Nonlocal STT} we mentioned that the input data set was 480 x 120 and mapped to a CUDA square grid of 512 x 128. Therefore, limiting the number executing threads within each kernel. Because of this, the extra threads were able to launch a different kernel in parallel with the current one, see Figure  \ref{fig:concurrent}.

Concurrent kernels demonstrate a very promising technique to achieve a huge performance increment in the current simulation. In theory it is possible to have multiple kernels executing at the same. However, there are some downsides to the implementation; correctly synchronize kernels, waiting time and hardware resources are among the  problems \cite{practices}. The timeline of the application \ref{fig:waittime} illustrates the waiting times between kernel execution. However, the waiting time are very small time steps between 0.01ms and 0.01ms, but waiting time occurs for each step of the RK4 and appears approximate 45,00 times. Furthermore, branching the kernel execution process should eliminate the issue.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/waittime.png}
		\smallskip
	\caption[Waiting time in concurrent kernels]{Waiting time between each concurrent kernel execution.}
	\label{fig:waittime}
\end{figure}


\subsection{Shared Memory}

Shared memory is faster than global memory but it is very limited, see Chapter \ref{Heterogeneous Performance Analysis and Practices}. To be able to implement shared memory in the kernels, we needed the kernels to be separated in their x, y and z coordinated, as mentioned in the previous section. In addition, this allows us to implement shared memory across each kernel, otherwise it would not be possible. Shared memory was applied in all the kernels where global memory was used extensively, eliminating the number clock cycles per thread.

The idea behind shared memory is to reduce the amount of global memory calls, which has about 400-600 latency clock cycles, while the shared memory only has 1-32 latency clock cycles \ref{fig:memory}. The shared memory implementation is accomplished by allocating the data from the thread block into a temporary array, in other words the shared memory. In addition, the kernel is able to perform calculations on the temporary array and write the values onto the global memory. The implementation code is illustrated in the Listing \ref{lst:shared}. There is no guarantee that threads will execute at the same order. Using {\listf \_\_syncthreads()} will wait until all threads have completed their task which is in this case loading global memory into the shared memory array. Chapter \ref{Heterogeneous Performance Analysis and Practices} section thread synchronization, has more information about thread synchronization and shared memory. Once all the operations on the shared memory array are finished, the final part is to write the shared memory values back to the global memory.

\begin{lstlisting}[language=C++, label={lst:shared}, caption={Shared memory.}]
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
int index = j * NXCUDA_CONST + i;

if (i > 1 && i < NX + 2 && j >= 0 && j < NY){
     int cacheIdx = threadIdx.y * blockDim.x + threadIdx.x;
     __shared__ double deltamS[THREADS_SHARED * THREADS_SHARED];

	 //load memory into shared memory
     deltamS[cacheIdx] = operationGlobal(globalMemory);
     __syncthreads();

	 //copy back the shared memory to global memory
     deltam[index]  = deltamS[cacheIdx];
}
\end{lstlisting}

To calculate the Laplacian, we need to access a great amount of global memory, located near the value of interest. In this case this is region of 4x4x4 grid space. Figure \ref{fig:shared} illustrates what part of the block is used for allocating shared memory and global memory. The global memory is used for the boundary conditions of the block, while the shared memory is for all the values inside the block.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.4\textwidth]{Figures/shared.png}
		\smallskip
	\caption[Shared memory strategy]{Shared Memory Strategy for the Laplacian evaluation.}
	\label{fig:shared}
\end{figure}

Listing \ref{lst:lapsh} demonstrates how  to calculate the Laplacian from Equation \ref{eq:nn} with shared memory. Firstly, we load all the global memory into a temporary array, the shared memory. Then we performed the calculation on the shared memory as mentioned before. The last step is to return data to the global memory.

\begin{lstlisting}[language=C++, label={lst:lapsh}, caption={Laplacian evaluating using shared memory with boundaries condition.}]
if (i >= 0 && i < NX && j >= 0 && j < NY){
    __shared__ double lapS[ THREADS_SHARED * THREADS_SHARED];
    lapS[sIdx] = deltam[Index];
    __syncthreads();

    if (threadIdx.x >= 2 && i < threadIdx.x - blockDim.x -2){ //shared
       lapy[idx] = - lapS[sIdx + 2] / 12.0 + 4.0 * lapS[sIdx + 1] / 3.0
			  	   - 5.0 * lapS[sIdx] / 2.0
			  	   - lapS[sIdx - 2] / 12.0 + 4.0 * lapS[sIdx - 1] / 3.0;
	else{ //global memory
		lapy[idx] = - deltam[idx + 2] / 12.0 + 4.0 * deltam[idx + 1] / 3.0
			  		- 5.0 * deltam[idx] / 2.0
			  		- deltam[idx - 2] / 12.0 + 4.0 * deltam[idx - 1] / 3.0;
	}
}
\end{lstlisting}

The current approach seems very promising for reducing global memory. However, a great amount of time is spent on loading data onto the shared memory array, In consequence, this delays threads executing and results in a decrease in performance. Therefore, fast allocating shared memory data is the optimal solution to ensure the optimal use of this type of memory. The results of such implementation are in the following section, optimization results.

\subsection{Structure of Arrays, SAO}

AoS and SoA refer to Array of Structures and Structure of Arrays respectively. These two terms refer to the two different ways of laying out data in the device or host. This is illustrated in Figure \ref{fig:aos} and \ref{fig:sao} respectively. The AOS approach, groups up properties of an object together and makes an array of those objects in the memory. The SOA would be a single structure in which you make an array for each property. The structure of arrays can allow for better cache utilization, make easier to access continues data, make better use of each read you make from memory, provide a more effective route to memory. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/aos.png}
		\smallskip
	\caption[Array of structures (AOS)]{AOS memory layout.}
	\label{fig:aos}
\end{figure}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/soa.png}
		\rule{35em}{0.2pt}
	\caption[Structure of arrays (SAO)]{SAO memory layout.}
	\label{fig:sao}
\end{figure}

The initial implementation of the  x, y, z data was allocated in separated blocks. Furthermore, when accessing blocks of the same coordinates, the register access the data. Review Listings \ref{lst:aos} and \ref{lst:sao}.

\begin{lstlisting}[language=C++, label={lst:aos}, caption={AOS implementation.}]
deltam_x = (double **)calloc(NYCUDA, sizeof(double *));
deltam_y = (double **)calloc(NXCUDA, sizeof(double *));
deltam_z = (double **)calloc(NXCUDA, sizeof(double *));
\end{lstlisting}

To solve the issue, a custom class GPUMatrix was programmed. The class contained all the matrices for the device. Moreover, the classes allocated the data for each Matrix and freed the memory automatically when the simulation was over. This was allocated in a structure that was easier for the device to access common elements. For example, when evaluating operations only on the x coordinate, the kernel physically accesses matrices that are near by and eliminate unnecessary shift in registers or hierarchy memory access.

\begin{lstlisting}[language=C++, label={lst:sao}, caption={SOA implementation.}]
    GPUMatrix<T> *dev_deltam;
    GPUMatrix<T> *dev_sdex; //Exchange term
    GPUMatrix<T> *dev_sfrelax;
    GPUMatrix<T> *dev_m; 
\end{lstlisting}

The current approach eliminates unnecessary data shift in registers, and it is able to stack more values per registers. In theory, there are more computational time per threads. The results of such implementation are illustrated in the last section of the chapter. With the new implementation, the code becomes more reusable for future improvements. 

\subsection{Occupancy}
 
Firstly, we increased the use of constant memory in the device. thus, eliminating redundant evaluations of variables and operations. The results are an increase in performance and more computational workload on each thread. In addition, constant memory modifications are illustrated in the Listing \ref{lst:constant}. 

The matrices \ref{eq:matrix3} and \ref{eq:matrix3} evaluates the boundary conditions, which do not change over the course of the application, They are only calculated at the initial stage of the application. Therefore, it is possible to implement those matrices using constant memory and  thus, eliminating unnecessary calls in the device.
 
 \begin{lstlisting}[language=C++,  label={lst:constant}, caption={Constant Memory changes.}]
 gsource << <blocks, threads >> >(u_val, dev_sm_z, dev_mz, NXCUDA);
 
 sfrelax_y[index] = -deltam_y[index] / tau_sf;
     
 DELTAX = (double)TX / (double)NX;
\end{lstlisting}
 
The different numbers of threads per block and as well the number of blocks per grid can dramatically increase or decrease the performance of the application. Table \ref{tab:ocu} illustrates the different threads per block configuration on the GeForce GTX 580. 


\begin{table}[h]
\centering
  \begin{tabular} { | l | l | l | l | l | l | }
    \hline
    Threads & Shared & speedup & time & Occupancy \\
    \hline
     8x8 &  8x8 & 7.217x & 52318.3  & 56.6\% \\
    \hline
     16x16 & 8x8 & 7.625x & 49517.3 & 86.6\% \\
    \hline
    16x16 & 16x16 & 7.978x & 47329.2 & 100.0\% \\
    \hline
    32x32 & 16x16 & 7.356x & 51333.4 & 66.6\% \\
    \hline
    32x32 & 32x32 & \multicolumn{3}{|c|}{Failed}\\
    \hline
  \end{tabular}
  \caption{Threads per block configuration and occupancy on the Fermi architecture}
  \label{tab:ocu}
  \end{table}


Using NVIDIA's Profiler made it possible to obtain the occupancy percentage of threads the device. The initial configuration for the Fermi and the Kepler was 32x32 threads per block for global memory and 16x16 threads per block for the shared memory. We found, that the optimal configuration for the Fermi cards was 16x16 threads per block and as well for the shared memory and for the Kepler cards was 32x32 threads per block for both memory types. 

\section{Optimization results}

This section is an overview of the optimization results compared with the initial CUDA implementation. Each version of the code is compared with the first test results, which were done using the GeForce GT 650M. Figures \ref{fig:gpuop} and \ref{fig:speedup} illustrate the time execution and the speedup in Tables \ref{tab:time} and \ref{tab:speed}. The final version of the code is the Occupancy. Moreover, the greatest performance occurred on the GeForce GTX 580 Card with 8.0x speedup \ref{fig:speedup}.

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy \\
    \hline
    Tesla K20m & 107322.7 & 101513.4 & 97106.0 & 90201.7 & 68988.2 & 66456.0\\
   \hline
    Tesla M2070 & 110912.3 & 103212.4 & 130754.1 & 97343.4 & 73938.1 & 70299.3\\
    \hline
    Tesla C2050 & 109635.1 & 101212.4 & 128516.6 & 96762.0 & 72964.5 & 69358.1\\
   \hline
    GeForce GTX 580 & 70002.7 & 68712.2 & 76481.9 & 68567.1 & 51603.7 & 47213.2\\
   \hline
    GeForce 650m & 244372.9 & 237371.9 & 227237.8 & 279804 & 181217.4 & 174419\\
   \hline
  \end{tabular}
    \caption{GPU Optimization time.}
  \label{tab:time}
  \end{table}
  
  Table \ref{tab:time} displays the overall executing time for all the version of the code, original, constant, streams, shared memory, SAO and Occupancy. We can see the comparison of the initial time and the final. There is a difference between 40s and 50s time decrease. The time reduction is relatively low. There is no large difference in waiting 100s or 66s for a simulation to be completed. However, if we increase the data set to five decimal points, the simulation could take up-to a couple of hours or days to process, thus with an the optimal implementation we will reduce the time by a factor of eight.
  
  Finally, the speedup comparison in Table \ref{tab:speed} and Figure \ref{fig:speedup} illustrates how much performance increase we could obtain in a new simulation using the new implementation.
  
  \begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l | l  |  l  | l | l | l |}
    \hline
    GPU & Original & Constant & Streams & Shared & SAO & Occupancy\\
    \hline
    Tesla K20m & 3.517x & 3.718x & 3.888x & 4.186x & 5.473x & 5.682x\\
   \hline
    Tesla M2070 & 3.403x & 3.534x & 2.888x & 3.879x & 5.107x & 5.371x\\
    \hline
    Tesla C2050 & 3.442x & 3.571x & 2.938x & 3.902x & 5.175x & 5.444x\\
   \hline
    GeForce GTX 580 & 5.391x & 5.521x & 4.937x & 5.551x & 7.317x & 8.0x\\
   \hline
    GeForce 670MX & 1.544x & 1.598x & 1.662x & 1.349x & 2.084x & 2.163x\\
   \hline
  \end{tabular}
    \caption{GPU Speedup performance.}
  \label{tab:speed}
  \end{table}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.98\textwidth]{Figures/gpuOptimization.png}
		\smallskip
	\caption[Overall simulation time]{Overall simulation time.}
	\label{fig:gpuop}
\end{figure}


Based on the initial profiling in Table \ref{tab:nvprof}, the Laplacian evaluation consumed about half of the overall simulation time. However, on the final optimization results \ref{fig:final}, the Laplacian was reduced from 44.37$\%$ to 26.24 $\%$ on execution time. But more importantly the importance of the kernel was reduced, in other words more computational workload on the other kernels. The same occurred for the Runge and Kutta term evaluation. The speedup mainly occurred in the Occupancy version of the code, see Table \ref{tab:speed}. Furthermore, the {\listf gsdExchangeFull} incremented from 13.37$\%$. 23.35$\%$, which is not necessary good. The increment in time is due to shift in streams operators. The {\listf gsdExchangeFull} is processed in the default stream zero, while the others kernels launched concurrently in a different stream.  Table \ref{fig:final} illustrates the final profiling results using NVIDIA's profiling tools.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0 \textwidth]{Figures/speedup.png}
		\smallskip
	\caption[GPU speedup comparison]{Speedup performance output.}
	\label{fig:speedup}
\end{figure}

\begin{figure}[htbp]
	\centering
	  \begin{tabular} { |  l  |  l | l  | l | l |}
	      \hline
	    Time $\%$& Time & Calls & Avg & Kernel \\
    \hline
   35.24 & 21.33s & 216330 & 98.5us & {\listf gSolTermRK4Relaxation } \\
   \hline
   26.24 & 15.88s & 288441 & 55.0us &  {\listf glaplacianXYBroundaries }\\
   \hline
   23.35 & 14.13s & 160000 & 88.3us & {\listf gsdExchangeFull} \\
   \hline
   15.17 & 9.18s & 72108 & 127.2us & {\listf gSolTerm4RK4Relaxation }\\ 
   \hline
    \end{tabular}
	\caption[Optimization results using NVIDIA's Profiling tool]{Final optimization results using NVIDIA's profiler, on the Tesla K20m.}
	\label{fig:final}
\end{figure}
    
Tesla K20m was the only GPU which in all the code modifications did not lose performance over the course of the optimization process. However, the other GPUs dropped performance in the stream optimization stage. The stream process is where each kernel was divided into three separated kernels. Doing this, we were able to calculate the x, y, z coordinates independently. In addition, this enabled room to implement shared memory across possible kernels. The GPU table specifications \ref{tab:gpus} illustrates that the Tesla K20m is the only GPU card with CC of 3.5. Therefore, it has access to Hyper-Q technology. which is able to synchronize automatically concurrent kernels, just by activated a steam process to the kernel.

The SOA optimization, improved overall dramatically the performance of the application, obtaining a 1.2x - 2.0x speedup in all GPU cards, see Table \ref{tab:speed} and Figure \ref{fig:speedup}. The final version, Occupancy, improved up-to 0.7x speedup on the 580 GPU. However, the Teslas cards improved only 0.2x-0.25x speedup. The speedup difference, 0.5x,  is due to the process cycle of the GeForce GPU. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/speed.png}
		\smallskip
	\caption[Final optimization speedup overview]{Optimization speedup overview.}
	\label{fig:speeduplast}
\end{figure}


We expected that the newest card, the Tesla k20m, would obtain the highest speedup overall, certainly because it has more CUDA cores and the highest compute capabilities. However, it falls behind the GeForce GTX 580 with about 2.5x speedup difference, see Table \ref{tab:speed}. In addition, the Tesla K20 performance results only have a difference of 0.3x speedup compared with the other Teslas cards, see Figures \ref{fig:speedup} and \ref{fig:speeduplast}. If we compare the GTX 580 card with the others GPUs specifications, it has the most Processor clock (GHz) and more mathematical calculations per cycle, see Table \ref{tab:gpus}. The results demonstrate the increase workload and occupancy on the device as well the increment in computational process per thread and per kernel.


  \vspace{4.0em}

Finally, the techniques and practices from Chapter \ref{Heterogeneous Performance Analysis and Practices} were used to archive speedup and increase performance on the initial CUDA implementation. Methods used were: increased the use of constant memory, shared memory, changed the memory allocating access, analyzed thread branching and finally analyzed kernel occupancy. The highest performance of all GPUS did not occurr on the newest NVIDIA card, the K20m, thus, this was the expensive of all the GPUs available in the cluster. The actual improvement appeared on the GeForce 580 (the more GHz of all GPUs) with a 2.32x speedup difference in comparison with the Tesla K20m, see Table \ref{fig:speedup}. If the problem data set is to be scaled, for example, to a simulation of eight days, the speedup performance of 8.0x will drastically decrease the time execution in only one day. Moreover, it is possible to execute more simulations on the initial time step.
  
