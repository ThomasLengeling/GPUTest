% Chapter Template

\chapter{Heterogeneous Performance Analysis and Practices} % Main chapter title

\label{Heterogeneous Performance Analysis and Practices} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 2. \emph{Heterogeneous Performance Analysis and Practices}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

When working with GPUs hardware challenges emerges. How can we make the best usage of the GPU hardware. In the conventional CPU model we have what is called linear or flat memory model. This appears to the programmer as a single contiguous address space. The CPU can directly address all the available memory, in other words there is almost no efficiency penalty in creating global data, local data, or even access data that is located on a opposite memory location, all of this can be access as a contiguous block. \cite{cook} Meanwhile on the GPU there are expectations, their exists different memory hierarchies which dramatically change the performance output. By allocation the optimal memory types, seed-up and increase throughput can be accomplished, but also analyzing. To ensure optimization, some analysis should be done, like comparing latency, memory hierarchies and data bandwidth between CUDA kernels, The study of the performance of the CUDA code can be done by using NVIDIA's Visual Profiler.


% techniques, programming models and hierarchies.
\section{Practices}

There are three rules based on NVIDIAs standers to follow for creating a high performance GPGPU (General-purpose on the GPU) program.\cite{design}

\begin{enumerate}
  \item Get the data on the GPU device and keep it there
  \item Process all the data en the GPU, give it enough work to do.
  \item Focus on data reuse within the GPU context, to avoid memory bandwidth limitations
\end{enumerate}

GPUs are plugged into the PCI Express bus of the host computer. The PCIe bus has extremely slow bandwidth compared with the GPU. This is why is important to store the data on the GPU and keep it busy. And minimize the data transfer to the host and back to the device. We can see this in the following table \ref{fig:PCI}.  Because CUDA-enable GPUs can carry out petaFLOP performance, they are fast enough to compute large amount of data. So each Kernel launch needs to use all the available resources of the GPU and avoid wasting compute cycles. If a single Kernel doesn't use all of the available bandwidth, multiple kernels can be launched at the same time on a single GPU.
For example a DP vectors require 8 bytes of storage per vector element this will double the bandwidth requirement. So is important to take advantage of the memory usage, take advantage of the memory types, use less memory copies between the GPU. \cite{design}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.7\textwidth]{Figures/PCI.png}
		\rule{35em}{0.5pt}
	\caption[PCIe Bandwidth]{PCIe bus and GPU bandwidth comparison }
	\label{fig:PCI}
\end{figure}

Some practices should keep in mind to rapidly identify the portions of code where it would be beneficial for GPU acceleration.\cite{practices}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/apod.png}
		\rule{35em}{0.5pt}
	\caption[Different memory types]{Different memory type and penalties usage}
	\label{fig:apod}
\end{figure}

\begin{description}

 \item{Asses} \hfill \\
 The first step is to locate the part of the code where the majority of the execution time occurs. The programmer can evaluate memory bottlenecks for GPU parallelization.
 \item{Parallelize} \hfill \\
 Increase parallelization from the original code, could be either adding GPU-optimized libraries such as cuBLAS, cuFFT, or including more amount of parallelism exposure though the use of CUDA code.
 \item{Optimize} \hfill \\
The developer can optimize the implementation performance through a number of considerations, overlapping kernel executing, kernel profiling, memory handling and fine-tuning floating-point operations.
 \item{Deploy} \hfill \\
 Compare the outcome with the original expectation. Determinate the potential speedup by accelerating a given section. First a partial parallelization should be implementation before carrying out a complete change.
 \end{description}

\section{Performance Metrics}

Before trying to make your simulation run faster, you should understand how it currently performs and where the bottlenecks are. There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time and increase the throughput by a giving kernel. Throughput is how many operations completed per cycle.

\subsection{Timing}

Timing a launched kernel can be done on either the GPU or the CPU. Is important to remember that the CPU and GPU are not synchronized. So its necessary to synchronize the CPU thread with the GPU kernels launches. CUDA provides the required  functions to synchronize the CPU with the GPU calling immediately before starting the timer.\cite{practices}.  CUDA also can handle timers within the GPU, and record times in a floating-point value in milliseconds. This is done with $cudaEventRecord()$, just by including $start$ and $stop$ in the function inputs. Note that the timing are measured on the GPU clock, so the timing is independent from the OS. \cite{cook}.

\subsection{Bandwidth}

The bandwidth refers to the rate at which data can be transferred between host and device and vi-versa. The bandwidth is one of the most important factors for testing performance o the GPUs. Choosing the right type of memory could dramatically increase performance and bandwidth. There are two main memory to indicate performance, theoretical bandwidth and effective bandwidth. The theoretical bandwidth is base on the hardware specifications that is available by NVIDIA. This is calculated using the following formula:

$$ theoretical bandwidth = (clock rate * (bit-wide-memory-interface / 8 )*2) / 10^9 $$

For example the NVIDIA GeForce GTX 280 uses DDR RAM with a memory clock rate of 1,105 MhZ and a 512-bit-wide memory interface

$$ (1107 * 10^6 * (512/8.0) *2 )/10^9 = 141.6 Gb /sec$$

The GTX 280 has a theoretical bandwidth of $141.6Gb/sec$.
The effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the application. \cite{practices}

$$effective-bandwidth = ((Br - Bw) / 109 )/time$$

Where Br is the number of bytes read per kernel, Bw is the number of bytes written per kernel and  t is the elapsed time given in seconds.  \cite{fortran}

In practice the difference between theoretical bandwidth and effective bandwidth indicated how much bandwidth is wasted on accessing memory and calculations. If the effective bandwidth is low compared to the theoretical bandwidth is one indication that there is not enough work being done in the GPUs. There a several solutions, analyze the code to make more parallelize instructions, execute more computational instructions on the GPUs, finally analyze the number of threads per block that are executing on execute kernels .

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Memory Handling with CUDA}

In this section four types of memory handling are going to be explained, shared memory, global memory (device memory) and finally host memory. In figure \ref{fig:memory} each memory type has it's bandwidth penalty of used and latency in cycles. Each one can be used in different applications to maximize the memory used. The shared Memory is very limited so it cannot be handler for all the kernels, when performed wrong on the device there is a huge latency and bandwidth penalty, instead having a gain in performance \cite{cook}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.85\textwidth]{Figures/memory.png}
		\rule{35em}{0.5pt}
	\caption[Different memory types]{Different memory type and penalties usage}
	\label{fig:memory}
\end{figure}


\subsection{Global Memory}

Understanding how to efficiently use global memory is essential in CUDA memory management.
Focusing on data reuse within the SM and caches avoids memory bandwidth limitations. Global memory on the GPU is designed to quickly stream memory blocks of data into the SM.

\begin{itemize}
\item Get the data on to the Device, keep it there.
\item Give the GPU enough workload, this using all the resources available from the GPU.
\item Focus on data reuse within the GPGPU to avoid memory bandwidth limitations.
\end{itemize}

In other words the global memory resides on the device, and it can be anything from 1 byte to 8GB, depending on the GPU. Also the memory is visible to all the threads of the grid. Any thread can read and write to any location of the global memory, The memory is always allocated with \textit{cadaMalloc}. And only global memory can be passed to the kernels and are called with \twoline global \twoline. \cite{design}


%\ref{fig:smi}

%\begin{figure}[htbp]
%	\centering
%		\includegraphics[width=0.85\textwidth]{Figures/smi.png}
%		\rule{35em}{0.5pt}
%	\caption[GPU and CPU]{Different architecture between the GPU and CPU}
%	\label{fig:smi}
%\end{figure}

\subsection{Shared Memory}

CUDA C compiler treats variables differently than a typical variable, it creates a copy of the variable for each block that is launched on the GPU, now every thread in that block can access the memory, this is why is called shared memory. This memory reside physically on the GPU, because the memory is very close the cache, the latency is typical very low.\cite{example}. One thing comes to mind, if the threads can communicate with others threads, so there should be way to synchronize all the threads. A simple case should be if thread A writes a value into the shared memory, and Thread B wants to access we need to synchronize, when thread A finish writing then Thread B can access it. This is typical case when shared memory with synchronize thread is needed. \cite{cook}
Shared memory is magnitudes faster to access than global memory, essentially is like a local cache for each threads of a block. While  the shared memory is limited to 48K a block, the global memory is the amount of DRAM on the device. The duration of the shared memory on the device is the lifetime of the thread block. Using \twoline shared \twoline in-front of the kernel call will invocate shared memory.

   %There is also one other very useful case with shared memory and that is where every thread in a warp reads the same bank address

\subsection{Constant Memory}

Is an excellent way to store and broadcast read-only data to all the threads on the GPU. One thing to keep in mind is that the constant memory is limited to 64KB. \cite{design}. A simple analogue is the  \textit{\#define} or \textit{const} attribute in the c++ programming language, the variable performed like a variable that cannot be modified. On CUDA is  excitability the same, the value can only be read and not written, the value will not change over the course of a kernel execution and only the host can write the constant memory.\cite{example}

\subsection{Texture Memory}

Like constant memory, texture memory is another variety of read-only memory that can improve performance and reduce memory traffic when reads have certain access patterns. . Traditionally Texture memory id used for computer graphics applications, but it can also be use for HPC. The main idea of this read-only memory is that threads are likely to read from address ''near' the address they nearby threads.\cite{example}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.55\textwidth]{Figures/texture.png}
		\rule{35em}{0.5pt}
	\caption[Texture Memory]{Mapping of threads into a two dimensional array of texture memory}
	\label{fig:texture}
\end{figure}

The texture Memory in a form works like the GPU graphics Texture, when you want to use the texture bind with some sort of data is necessary and when you finish using it unbind the texture from the data. The usage can be summarized in the following table:

\begin{itemize}
\item Allocate global memory in the Host.
\item Create Texture reference and bind it to memory object.
\item On the device obtain the reference from the texture.
\item  Use Texture memory operations on the device
\item  When the work is done on the Texture, unbind the texture reference on the host.
\end{itemize}

\subsection{Thread Synchronization}

This refers to synchronizing threads operations. For efficiency, a pipeline can be created by queuing a number of kernels to keep the GPGPU busy for as long as possible. Further, some form of synchronization is required so that the host can determine when the kernel or pipeline has completed. \cite{design} Commonly used synchronization mechanisms are:

\begin{itemize}
  \item Explicitly calling cudaThreadSynchronize(), which acts as a barrier causing the host to stop and wait for all queued kernels to complete.
  \item Performing a blocking data transfer with cudaMemcpy() as cudaThreadSynchronize() is called inside cudaMemcpy().
\end{itemize}

The basic unit of work on the GPU is a thread. It is important to understand from a software point of view that each thread is separate from every other thread. Every thread acts as if it has its own processor with separate registers and identity. Will wait for all threads to finish there job. \cite{design}

\section{Kernel Analysis}

As said before, kernels are the essential part of CUDA programming, threads are launched automatically throughout each thread blocks of the device. Furthermore it millions of threads execute the same code parallel, which is  When execution the 

\begin{description}

 \item{Memory Bandwidth Bound} \hfill \\
 This refers when the code/application is limited by memory access. Most GPUs card have 1GB- 6GB of memory, this is used to process the data on the GPU, while the CPU has massively amount of memory available for use. A solution to this is to reuse the data, change the type of memory used in the GPU. A multi-GPU approach, launching kernels in several GPUs at once. This will dramatically increase the amount of memory in the application.

  \item{Compute Bound} \hfill \\
Refers to the computation time execution, in other works calculations done in the device, under the assumption that  theres is enough memory for the calculations. what is actually the analysis time operations on the kernels. Theoretical bandwidth vs  effective Bandwidth can measure performance for a compute-bound Kernel. Therefore its possible to increase the FLOPS per device.

 \item{Latency Bound} \hfill \\
 Is one whose predominate stall reason is due to memory fetches. This is actually the saturating the global memory, or any type, but still have to wait to get the data into the kernel. Physically can be the data being sent from one part of the Device to the other. Also depends the time required to perform an operation, and are counted in cycles of operations. A way to reduce the latency is to increase the number of parallel instructions (more  calls per thread), in other words more work per thread and fewer threads.
 \end{description}

The performance of relatively simple kernels, which perform computations across a large number of data elements, is more a function of the GPU's memory system performance than the processing performance. It can be beneficial for such memory-bound kernels to decrease the amount of memory access required by increasing the complexity of the computation. \cite{cook}

\section{Hardware constraints}

This refers to the limit how many threads per block a kernel launch can have. If exceed this values they kernel will never run. The threads per block really depends of the hardware capabilities. In a roughly summarized as:

\begin{itemize}
\item Each block cannot have more than 512/1024 threads in total. (Capability 1.x or 2.x-3.x)
\item The Maximum dimensions of each block are limited to [512,512, 64]/[1024, 1024, 64](compute 1,1.2)
\item Each block cannot consume more than to 8k, 16k, 32K registers total
\item Each block cannot consume more than 16kb/48kb of shared memory
\end{itemize}

SM Resources, improve performance of an application by trading one resource usage for another.  \cite{practices}

Another inefficiency that can cause low performance to the applications is the number transfers memory calls between the CPU and GPU. The GPU communicates with the CPU via a \textit{PCIe} bus, in addition all of the massive FLOPS per second that can be achieve cannot actually be sent to CPU. The GPU should be filled with the enough workload at the beginning of the application and at the end only return the memory to the CPU.

\subsection{Thread Division}

The hardware has its limits, how much thread per block a kernel can handle. Launching a kernel with the hardware constrains for above can only ensure us that the kernel will actually be executed in the device, nonetheless not 100$\%$ optimal. Furthermore is necessary to launch kernels with the amount of threads per block base on the hardware contains and the problem. The block size that is chosen will determine how faster the code will run. By Benchmarking, is possible to find what configuration is the best for the problem. One thing to notice is that thread blocks should be a multiple number of SMs, with this idea is possible to obtain optimal thread block configuration.

\section{Visual Profiler}

Is a hard task to keep track of each individual thread. This becomes difficult for debugging highly parallel applications. The NVIDIA's Visual Profiler is a profiling tool that can be used to measure performance and find potential opportunities for optimization in order to archive maximum performance on the GPUs. The Profiler provides metrics in the form of plots and graphs, which describes opportunities to fully utilize the compute and data movements capabilities of the GPU, as well of each kernel launch in the application. See Figure \ref{fig:visualgraph}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.9\textwidth]{Figures/visualgraph.png}
		\rule{35em}{0.2pt}
	\caption[Visual Profiler metrics graphs and plots]{Profiler provides optimization metrics}
	\label{fig:visualgraph}
\end{figure}

NVIDIA's profiling tools comes in various flavors; a standalone profiler through the visual profiler compiler nvvp, integrated in a GUI Nsight Eclipse Edition as nsight command (Visual Profiler), and as a command-line profiler though nvprof command. Each one has its disadvantages and advantages. The command-line profiler is useful for remotely access, where a GUI is not available, while the Nsight can show graphs, plots and timeline of the application. The Profiler support CUDA applications as well as openCL applications, however both have some exceptions. 

The Visual Profiler, by default, will execute the entire application, nonetheless typically only some parts of application only need performance optimization. This enables to determine kernels, code where critical performances is needed. The common situations where profiling a region of the application is helpful.\cite{tool}

\begin{itemize}
  \item Analyze data initialization and movement in the CPU and GPU, as well as evaluating CUDA calls.
  \item The application operates in phases, where a algorithm operates throughout each region. The application can be optimized independently from other phases of the code.
  \item The application contains algorithms that operate though a large number of iterations. In this case is possible to collect data from a portion of the iterations.
\end{itemize}

The Visual Profiler provides a step-by-step optimization guidance, where is possible to evaluate the GPU usage, examine individual kernels and analyze timeline of the application which the profiler shows memory movements and usage, CUDA calls, number of threads and performance. The figure \ref{fig:visual01} shows, each Kernel has its own percentage of execution time of the overall application.\cite{practices}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.35\textwidth]{Figures/visual01.png}
		\rule{35em}{0.5pt}
	\caption[Visual Profiler example]{Visual Profiler kernel execution}
	\label{fig:visual01}
\end{figure}

\subsection{Profiler Kernel Report}

The profiler will execute several times the application for it to collect data from each kernels. This enables to precisely optimize phases of the application\cite{example}. The profiling tools can verify how long the application spends executing each kernel as well the number of used blocks and threads. Through this is possible to obtain various memory throughput measures, like global load throughput and global store throughput, indicate the global memory throughput requested by the kernel and therefore corresponding to the effective bandwidth mentioned in the last section.

As we know the profiler executes the application several time to collect data about each kernel. The information obtained by each kernel can be sum-up in-to a report that can be exported in a pdf file, which has the following information.

\begin{enumerate}
  \item \textbf{Compute, Bandwidth, or Latency Bound} \hfill \\
      The performance determines if the kernel is bounded by computation, memory bandwidth, or instructions/memory latency. It shows how is limiting the performance respectively.
  
  \item \textbf{Instructions and Memory Latency} \hfill \\
Instruction and memory latency limit the performance of a kernel when the GPU does not have enough work to keep busy. The performance of latency-limited kernels can often be improved by increasing occupancy. Occupancy is a measure of how many warps the kernel has active on the GPU, relative to the maximum number of warps supported by the GPU.
  
  \item \textbf{Compute Resources} \hfill \\
GPU compute resources limit the performance of a kernel when those resources are insufficient or poorly utilized. Compute resources are used most efficiently when instructions do not overuse a function unit. 
  \item \textbf{Floating-Point Operation Counts} \hfill \\
  floating-point operations executed by the kernel, can be either single precision or double precision.
  
  \item \textbf{Memory Bandwidth} \hfill \\
  Memory bandwidth limits the performance of a kernel when one or more memories in the GPU cannot provide data at the rate requested by the kernel.
\end{enumerate}

\subsection{Collect Data On Remote System}

Is also possible to collect data from a remote system where a GUI is not available. 

There are three common remote profiling use cases that can be addressed by using nvprof and nvvp.


Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed. There are two ways to perform remote profiling. You can profile your remote application directly from nsight or nvvp. Or you can use nvprof to collect the profile data on the remote system and then use nvvp on the host system to view and analyze the data.

\vspace{3.2em}

Finally, this chapter gives a overview of practices and performance studies for GPGPU. A better understanding of the hardware and memory management, as well as hardware limitation, will help us how to determinate the best usage of the GPUs. As we can see NVIDIA's profiling tools is useful to analyze different stages of our application, moreover to determined which parts of the CUDA code is better to optimize from others, to gain more performance.


