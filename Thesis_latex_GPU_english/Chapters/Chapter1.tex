

\chapter{Heterogeneous Computing} % Main chapter title

\label{Heterogeneous Computing} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 1. \emph{Heterogeneous Computing}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

Heterogeneous computing refers a system that combines several processor types to gain more performance. Typically using a single or multi-core computer processing units (CPUs) and a  graphics processing units (GPUs).
Frequently GPUs are know for 3D graphics rendering, video games and video editing, but GPUs are becoming increasingly popular for accelerating computing applications and scientific research due to their low price, high performance and relatively low energy consumption per FLOPS (floating point operations per second) when compared with the CPUs. This chapter provides an overview of GPUs within the High Performance Computing (HPC) context, their advantages and disadvantages and how they can be integrated in to a scientific software and research.

%per watt better than cpu's


\section{Motivation}

The GPU has been essential part of personal computer since the early use. Over the course of 30 years the graphics architecture has evolve form drawing a simple 3D scene to be able to program each part of the GPU graphics pipeline. Their role became more important in the 90s with the first-person shooting video game DOOM by id Software. The demanding video game industry has brought year by year more realistic 3D graphics. Consequently new innovated hardware capabilities has been developed to increase the graphics pipeline and the render output. This lead to a more sophisticated programming environment with a massive parallel capabilities.

The fixed graphics pipeline (fixed functions on the GPU) was introduced in the early 90s, allowed various customization of the rendering process. However only allowed some modifications of the GPU output. Specific adjustment were extremely complicated did not allow custom algorithms. In 2001 NVIDIA and ATI (AMD) introduced the first programmability to the graphics pipeline. Which could control millions pixels and vertex output in a single frame, moreover it out-performed the CPU in rending video. In addition graphics shift from the CPU to the GPU. This was the beginning of GPU parallel capabilities \cite{Nickolls}.

At first the GPUs where only used for general-purpose computing (GPGPU) like computer graphics, but in resent years the GPU has been used to accelerate scientific research, analytics, engineering, robotics and consumer applications.

GPUs are attractive for certain type of scientific computation as they offer potential seed-up of multi-processors devices with the added advantages of being low cost, low maintenance, energy efficient, and relative simple to program. Many algorithms in applied physics are using GPUs to improve their performance over the CPU. Some area of scientific research that obtain the benefit of heterogenous computing are: Molecular Dynamics, Quantum Chemistry, Computational Structural Mechanics and Computational Physics \cite{applications}.

In any case, for a given simulation a compromise between speed and accuracy is always made. The current tendency of the CPU relies on increasing the clock speed, decreasing the size of transistor and finally adding more cores per unit and be able to work and a parallel manner. Therefore there are limitations to this paradigm \cite{quantitative}.

\begin{description}
  \item[Power Wall] \hfill \\
  The CPUs single core has not gone beyond the 4GHz barrier, a paradigm shift from a single core to a multi-core CPUs, also  the power use of CPUs is very high per Watt. The figure \ref{fig:gpu_cpu_s} shows the comparison of performance between the GPU and CPU.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.75\textwidth]{Figures/GPU_CPU_s.png}
		\smallskip
	\caption[GPU and CPU]{GPU and CPU peak performance in gigaflops}
	\label{fig:gpu_cpu_s}
\end{figure}


  \item[Memory Wall] \hfill \\
  This refers to the growing disparity of speed between CPU  and the memory outside the CPU chip. Some applications have become memory bound, that is to say computing time is bounded by the transfer memory between the CPU and all the hardware devices connected to the CPU, commonly to the Peripheral Component Interconnect (PCI) chip. In conclusion, the computing time is bounded by the memory and not by the time calculations performed on the CPU.

  \item[Parallelism Wall] \hfill \\
  This indicates a law that indicates the number of parallel processes. The number N parallel processes is never ideal and always depends on the problem.  The speedup is described by Amdahl's Law in terms of the fraction of parallelized work $f$ \cite{quantitative}.

  $$Speedup \leq \frac{N}{f + N(1-f)}$$


\end{description}

The current paradigm of using CPUs for computing is growth unsustainable. In 2012, Japan among the countries with elite supercomputers, builded the machine "K Computer", with 705,024 multi-core CPUs, achieving up-to 11.3 petaflops ($10^5$ flops). Furthermore, the computer is one of the most power efficient supercomputer in the world with a total of 12.66 megawatts (MW), in other words 830 Mflops/watt. This is this is enough to power a small town of 10,000 homes. If the current thread of power use continues, the next supercomputer would require 200 MW of power, this would require a nuclear power reactor to run it \cite{whatexascale}. However, in 2013 Oak Ridge Nacional Laboratory (U.S) built a supercomputer that combines CPUs and GPUs, the Titan. It obtains an astonishing 24 petaflops theoretical peak performance. Moreover, with a power consumption of 8.2 MW. They demonstrated that is possible to built a supercomputers that combines CPUs and GPUs, which enables a  higher performance and lower power consumption compared to a CPU based supercomputer \cite{titan}.

As said the GPU exceeds the CPU in calculations per second FLOPS with a low energy consumption. However, the GPU is designed to launch small amounts of data in parallel with only several instructions, in other words the GPU swap, switch threads very fast and they are extremely lightweight. In a typical GPU system, thousands of threads are waiting for call request to start working. While on the CPU only run up-to 24 threads on a hex-core processor. They can execute a single operation on comparatively large set of data with only one instruction. Although this can be extremely cost-wise operation on the GPU.

\section{GPUs as computing units}


The GPU, unlike its CPU cousin, has thousands for registers per SM (Streaming Multiprocessor), which are arithmetic processing units. An SM, in other words, is similar to a multi-thread CPU core. On a typical CPU has two, four, six or eight cores. On a GPU as many as $n$ SM core. The SM are configured in such a way that they are able to access memory location close by. We can see this in the figure \ref{fig:gpu_cpu}. For a particular calculation, all the stream processors within a group execute exactly the same instruction on a particular data stream, then the data is sent to the upper level, the host (CPU) \cite{cook}.

CUDA cores, are the number of processors in a single NVIDIA GPU chip. For example one of the first GPU capable of running CUDA code was the NVIDIA 9800 GT, which had 112 cores, while the latest high-end GPU GTX 980 has 2048 cores.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.5\textwidth]{Figures/GPU_CPU.png}
		\smallskip
	\caption[Architecture of a SM]{SM Architecture \cite{cook}.}
	\label{fig:gpu_cpu}
\end{figure}


Each CUDA core can execute a sequential thread, just like a CPU thread, which NVIDIA calls it Single Instruction, Multiple Thread (SIMT). In addition, all cores in the same group execute the same instruction at the same time, much like classical SIMD (Single instruction, multiple data) processors. SIMT handles conditionals somewhat differently than SIMD, though the effect is much the same, where some cores are disabled for conditional operations, in other word a single instruction is executed throughout the device.

Being able to efficiently use a GPU for an application requires to expose the inherent data-parallelism Optimized for low-latency, serial computation. This can be seen in contrast with a CPU, which is optimized for sequential code performance, fast switching registers  and sophisticated control logic allowing to run single complex programs as fast as possible, which is not possible on the GPU. Memory management is very important for GPUs. This refers how to allocate memory space and transfer data between host (CPU) and device (GPU). While the CPU memory hierarchy is almost non-existent, on the GPU inherent data is important \cite{hwu}. The figure \ref{fig:arch} illustrates the memory hierarchy of the device. In addition the global memory is huge in comparison with the L1/Cache and the texture memory. However, the access to the global memory is slow in comparison to the other. We can observed in the figure how the data is sent from the host to the device and vi-versa.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.85\textwidth]{Figures/arch.png}
		\smallskip
	\caption[GPU archiqutecture]{Unified programmable processor array of the GeForce 8800 GT graphics pipeline \cite{hwu}.}
	\label{fig:arch}
\end{figure}

On the GPU, floating point precision and optimization are very important. However, there is a penalty for choosing either performance or precession. All the GPUs are optimized for single precision floating operations, a 24 bit size. NVIDIA, also provides a double precision point, size of  53 bits. Which is a standard based on  IEEE 754 notation. Normally the GPU uses the single precession(SP) by default, if chosen a double precision (DP), normally there is a penalty between 2x and 4x speedup\cite{precision}. Libraries such as CUBLAS and CUFFT provides useful information how NVIDIA handles floating point operations under the hood.

\section{Programming on GPUs}

There exist among many, two main computing platforms, NVIDIA's Compute Unified Device Architecture (CUDA), and Khronos's Open Computing Language (OpenCL). NVIDIA's CUDA provides the necessary tools, frameworks and library to program parallel application. While, the OpenCL is a open standard framework meaning that is possible to do parallel computing on other GPUs, like on AMD cards. Programmers can easily port their code to others graphics cards.  However, CUDA has more robust debugging and profiling for GPGPU computing. The two frameworks are developed to be close to the hardware layer, using the C programming language as primarily programming language. Furthermore, CUDA provides both a low level API and a higher level API. Those who are familiar to OpenCL and CUDA, can easily modify their code to work on either platform \cite{hwu}.

CUDA programming model views the GPU as an accelerator processor which calls parallel programs throughout all the SM \cite{handbook}. In addition, the CUDA parallel programs are only launched on the device (GPU) and are named as kernels. The kernels are executed across a large amount of threads, which contains the CUDA code. The basic idea of programming on a GPU is simple, the following steps explains the procedure \ref{fig:cycle}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.3\textwidth]{Figures/cycle.png}
		\smallskip
	\caption[Programming Cycle]{Programming Cycle between the CPU and GPU \cite{example}}
	\label{fig:cycle}
\end{figure}

\begin{itemize}
\item Create memory(data) for the host (CPU) and devices (GPUs)
\item Send the data host memory to the highly parallel device.
\item Do something with data on the device, e.g. matrix multiplication,calculation, parallel algorithm.
\item Return the data from the device to the host.
\end{itemize}

The structure of CUDA reflects the coexistence of CPU and GPUs. The CUDA code is a mixture of both host code and device code. Moreover, the device code is an extension of the C compiler with additional namaspaces/CUDA keywords for parallel code, the CUDA compiler is called NVCC. The host code is the standard low level ANSI C language. However, is possible to program applications in C++, Python and Fortran. While the standard C code has extension marked as .c for source and .h for headers files, the CUDA code has extensions of .cu for source files and .cuh.

Kernels are launched or executed on a large amount of threads in the SM. They can be configured by threads per block and by block per grid. The thread and block configuration is illustrated in the figure \ref{fig:grid}. A thread is the simplest executing process. It consists of the code of the program, the particular point where the code is being executed \cite{hwu}. In addition all the threads in a kernel can access the global memory (RAM), figure  \ref{fig:arch} illustrates the physical position. Moreover, many threads form a block, and many blocks form a grid. CUDA handles the execution of the random-access threads, which take up-to very few clock cycles in comparison to CPU threads.

Each of the threads can be access by a implicit variable that identifies its position within the thread block and its grid. The thread access for the x coordinate is showed in the code \ref{lst:1d}. This is only the case for 1D block, which is widely used for shared memory access (see chapter 4) \cite{example}.

\begin{lstlisting}[language=C++, label={lst:1d}, caption={1D block}]	
	int index = blockIdx.x * blockDim.x + threadId.x;
\end{lstlisting}

For the case of a 2D thread block, the code \ref{lst:2d} describe such configuration. In addition, the 2d thread block is the most common thread block configuration. Is also possible to configure a 3D thread block just by adding the z coordinate to the threadIdx index. However, is very limited.

\begin{lstlisting}[language=C++, label={lst:2d}, caption={2D block}]	
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    
    int index = j * YSIZE + i;
\end{lstlisting}

$$blockIdx.x \times blockDim.x + threadId.x$$

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.66\textwidth]{Figures/grid.png}
		\smallskip
	\caption[CUDA's 2D thread grid]{Thread per block division and block per grid \cite{hwu}.}
	\label{fig:grid}
\end{figure}

In CUDA, host memory and device memory have separate memory spaces. Both of them have physically a separated location, for example, the RAM for either the CPU o the GPU. Furthermore, the programmer requires to send data from the host memory to the device's global memory (RAM) and vice-versa. The process is illustrated in figure \ref{fig:arch}. Memory which is allocated in the device needs to be freed on the device, the same occurs for the host memory. Moreover, the process is accomplish with similar devices operations, free or delete in C/C++. Some of the operations are performed by CUDA's Application Programming Interface (API) on behalf of the programmer \cite{hwu}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.62\textwidth]{Figures/memorySpace.png}
		\smallskip
	\caption[Memory transfer between CPU and GPU]{Memory transfer between CPU and GPU \cite{hwu}}
	\label{fig:memorySpace}
\end{figure}

\subsection{CPU and GPU multithread comparison}

Current CPUs are typically multicore systems, which are capable of parallelizing code fairly easy. In addition suggest a parallel system. However, we would require a large infrastructure  \cite{example}. For example, if we want to implement a simple vector addition using the CPU cores, we would require to compute a portion of the code on each code, one core the evens and the other core the odds, see figure \ref{fig:cpu}. Furthermore, this makes the implementation difficult to scale and require many cores, which are not so easily available after a 8 core system. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.72\textwidth]{Figures/cpu.png}
		\smallskip
	\caption[CPU Core process]{CPU Core process \cite{hwu}.}
	\label{fig:cpu}
\end{figure}


On the GPU we are able to accomplish the same result of the CPU with lest amount of code. The code \ref{lst:gpu} demonstrate how to evaluate a addition of two matrices. $a$ and $b$, then return the result in the matrix $c$. The difference between the codes, is that the GPU executes the kernel across all threads configured by the kernel call. Moreover, enables a highly parallel process with just a couple lines of code. 

\begin{lstlisting}[language=C++, label={lst:gpu}, caption={GPU Kernel threads launch}]
__global__ void add( int *a, int *b, int *c ) {
	int tid = blockIdx.x;
	if (tid < N)
		c[tid] = a[tid] + b[tid];}
\end{lstlisting}

For example, to execute a kernel with 32 x 32 threads per block and 15 x 4 blocks per grid, we just include the block and the threads dimensions when calling the kernel in the main loop \ref{lst:kernel}. Finally, the kernel will spam the CUDA code across all the configured threads.

\begin{lstlisting}[language=C++, label={lst:kernel}, caption={kernel call}]
dim3 blocks(15, 4);
dim3 threads(32, 32);
add<<< blocks2, threads, 0 >>>(A, B, C);
\end{lstlisting}


\vspace{3.2em}


To conclude, this chapter provided a overview of heterogeneous programming in a modern context. CUDA enhance the C language with parallel computing support. Which is possible to launch  enormous amounts of parallel threads, oppose of few threads on the CPU. The number of GPU cores will continue to increase in proportion to increase in available transistors as silicon process improve. In addition, GPUs will continue to go through vigorous architectural evolution. Despite their demonstration high performance on data-parallel applications.







