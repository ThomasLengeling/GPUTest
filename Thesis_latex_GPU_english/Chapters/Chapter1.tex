

\chapter{Heterogeneous Computing} % Main chapter title

\label{Heterogeneous Computing} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 1. \emph{Heterogeneous Computing}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

Heterogeneous computing refers a system that combines several processor types to gain more performance. Typically using a single or multi-core computer processing units (CPUs) and a  graphics processing units (GPUs).
Frequently GPUs are know for 3D graphics rendering, video games and video editing, but GPUs are becoming increasingly popular for accelerating computing applications and scientific research due to their low price, high performance and relatively low energy consumption per FLOPS (floating point operations per second) when compared with the CPUs. This chapter provides an overview of GPUs within the High Performance Computing (HPC) context, their advantages and disadvantages and how they can be integrated in to a scientific software and research.

%per watt better than cpu's


\section{Motivation}


The GPU has been essential part of personal computer since the early use. Over the course of 30 years the graphics architecture has evolve form drawing a simple 3D scene to be able to program each part of the GPU graphics pipeline. Their role became more important in the 90s with the first-person shooting video game DOOM by id Software. The demanding video game industry has brought year by year more realistic 3D graphics. Consequently new innovated hardware capabilities has been developed to increase the graphics pipeline and the render output. This lead to a more sophisticated programming environment with a massive parallel capabilities.

The fixed graphics pipeline (fixed functions on the GPU) was introduced in the early 90s, allowed various customization of the rendering process. However only allowed some modifications of the GPU output. Specific adjustment were extremely complicated did not allow custom algorithms. In 2001 NVIDIA and ATI (AMD) introduced the first programmability to the graphics pipeline. Which could control millions pixels and vertex output in a single frame, moreover it out-performed the CPU in rending video. In addition graphics shift from the CPU to the GPU. This was the beginning of GPU parallel capabilities.

At first the GPUs where only used for general-purpose computing like computer graphics, but in-till resent years the GPU has been used to accelerate scientific research, analytics, engineering, robotics and consumer applications.(GPGPU)\cite{physicsgpu}.

GPUs are attractive for certain type of scientific computation as they offer potential seed-up of multi-processors devices with the added advantages of being low cost, low maintenance, energy efficient, and relative simple to program. Many algorithms in applied physics are using GPUs to improve their performance over the CPU. Some examples are Euler Solver 16x seed-up (add Reference seed-up).

In any case, for a given simulation a compromise between speed and accuracy is always made. The current tendency of the CPU relies on increases the clock seeped, decreasing the size of transistor and finally adding more cores per unit and be able to work and a parallel manner, because of the there are some limitations\cite{quantitative}

\begin{description}
  \item[Power Wall] \hfill \\
  The CPUs single core has not gone beyond the 4GHz barrier, a paradigm shift from a single core to a multi-core CPUs, also  the power use of CPUs is very high per Watt. The figure \ref{fig:gpu_cpu_s} shows the comparison of performance between the GPU and CPU.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.70\textwidth]{Figures/GPU_CPU_s.png}
		\rule{35em}{0.5pt}
	\caption[GPU and CPU]{GPU and CPU peak performance in gigaflops}
	\label{fig:gpu_cpu_s}
\end{figure}


  \item[Memory Wall] \hfill \\
  This refers to the growing disparity of speed between CPU  and the memory outside the CPU chip. Some applications have become memory bound, that is to say computing time is bounded by the transfer memory between the CPU and all the hardware devices connected to the CPU, commonly to the Peripheral Component Interconnect (PCI) chip. In conclusion the computing time is bounded by the memory not by the time calculations done on the CPU.

  \item[Parallelism Wall] \hfill \\
  This indicates a law that indicates the number of parallel processes. The number N parallel processes is never ideal and always depends on the problem.  The seed-up can be described by Amdahl's Law in terms of the fraction of parallelized work (f). \cite{quantitative}.

  $$speedup \leq \frac{N}{f + N(1-f)}$$


\end{description}

The current paradigm of using CPUs for computing growth is unsustainable. In 2012, Japan among the countries with elite supercomputers, builded the machine "K Computer", with 705,024 multi-core CPUs, it can achieve 11.3 petaflops ($10^5$ flops). Furthermore the computer is one of the most power efficient supercomputer in the world with a total of 12.66 megawatts (MW), in other words 830 Mflops/watt. This is this is enough to power a small town of 10,000 homes. If the current thread of power use continues, the next supercomputer would require 200 MW of power, this would require a nuclear power reactor to run it.\cite{whatexascale}. However in 2013 Oak Ridge Nacional Laboratory (U.S) built a supercomputer that combines CPUs and GPUs, the Titan. It can archive an astonish 24 petaflops theoretical peak. Moreover with a power consumption of 8.2 MW. Combining CPUs GPUs is possible to built supercomputers with a higher performance and lower power consumption. \cite{titan}


As said the GPU exceeds the CPU in calculations per second FLOPS with a low energy consumption. However the GPU is designed to launch small amounts of data in parallel with only several instructions, in other words the GPU swap, switch threads very fast, they are extremely lightweight. In a typical system, thousands of threads are waiting to work. While the CPU only run up-to 24 threads on a hex-core processor. They can execute a single operation on comparatively large set of data with only one instruction. Although this can be extremely cost-wise operation on the GPU.

\section{GPUs as computing units}

A insight of the architecture of GPU can give a idea of  why it outperforms the CPU on various benchmarking.

The GPU, unlike its CPU cousin, has thousands for registers per SM (streaming multiprocessor), this are  arithmetic processing units. An SM can thought of like a multi-thread CPU core. On a typical CPU has two, four, six or eight cores. On a GPU as many as N SM core. We can see this in the figure \ref{fig:gpu_cpu}. For a particular calculation, all the stream
processors within a group execute exactly the same instruction on a particular data stream, then the data is sent to the upper level, the host (CPU). \cite{cook}

As commonly named CUDA cores are the number of processors in a single NVIDIA GPU chip. For example one of the first GPU capable of running CUDA code was the NVIDIA 9800 GT, which had 112 cores, while the latest high-end GPU GTX 980 has 2048 cores.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.42\textwidth]{Figures/GPU_CPU.png}
		\rule{35em}{0.5pt}
	\caption[Architecture of a GPU]{Architecture of a NVIDIA GeForce GTX 580}
	\label{fig:gpu_cpu}
\end{figure}


Each CUDA core  can execute a sequential thread, just like a CPU thread, which NVIDIA calls it Single Instruction, Multiple Thread (SIMT). In addition all cores in the same group execute the same instruction at the same time, much like classical SIMD (Single instruction, multiple data) processors. SIMT handles conditionals somewhat differently than SIMD, though the effect is much the same, where some cores are disabled for conditional operations, in other word a single instruction is executed throughout the device.

Being able to efficiently use a GPU for an application requires to expose the inherent data-parallelism Optimized for low-latency, serial computation. This can be seen in contrast with a CPU, which is optimized for sequential code performance, fast switching registers  and sophisticated control logic allowing to run single complex programs as fast as possible, which is not possible on the GPU. Memory management is very important for GPUs. this refers how to allocate memory space and transfer data between host (CPU) and device (GPU). While the CPU memory hierarchy is almost non-existent, on the GPU inherent data is important. In figure \ref{fig:arch} different levels of memory can be observer between the host and the device, which differs form the CPU \cite{hwu}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/arch.png}
		\rule{35em}{0.5pt}
	\caption[Host and Device]{Memory transfer between the host and device}
	\label{fig:arch}
\end{figure}


On the GPU precision and optimization are very important but there is a penalty for choosing performance or precession. All the GPUs are optimized for single precision floating operations, 24 bit size, Also provides double precision point, size of  53 bits. This is using the standard notation IEEE 754. Normally the GPU uses single precession(SP) by default, if choosed double precision (DP), normally there is a penalty of  2x - 4x seed-up. \cite{precision}
Libraries such as CUBLAS and CUFFT provides useful information how NVIDIA handles floating point operations under the hood.


\section{Programming on GPUs}

There exist, among many, two main computing platforms, NVIDIA's Compute Unified Device Architecture (CUDA), and Khronos's Open Computing Language (OpenCL). NVIDIA's CUDA provides the necessary tools, frameworks and library to programs parallel computing, but for there GPUs. While OpenCL is a open standard framework meaning that is possible to do parallel computing on other GPUs, like on AMD cards. Programmers can easily port their code to others graphics cars.  However CUDA has more robust debugging and profiling for GPGPU computing. This two frameworks are developed to be close to the hardware layer, using C programming language. CUDA provides both a low level API and a higher level API. Those who are familiar to OpenCL and CUDa, can easily modify their code to work on either platform.\cite{hwu}


The CUDA programming model views the GPU as an accelerator processor which calls parallel programs  throughout all the SMI. This programs are only called on the device and are called kernels, which launch a large amounts of threads to execute CUDA code. The basic idea of programming on a GPU is simple. We can observer this in the figure \ref{fig:cycle}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.3\textwidth]{Figures/cycle.png}
		\rule{35em}{0.5pt}
	\caption[Programming Cycle]{Programming Cycle between the CPU and GPU}
	\label{fig:cycle}
\end{figure}


\begin{itemize}
\item Create memory(data) for the host (CPU) and devices (GPUs)
\item Send the data host memory to the highly parallel device.
\item Do something with data on the device, e.g. matrix multiplication,calculation, parallel algorithm.
\item Return the data from the device to the host.
\end{itemize}

The structure of CUDA reflects the coexistence of CPU and GPUs. The CUDA code is a mixture of both host code and device code. The CUDA C compiler is called NVCC. The host code is the standard low level ANSI C language. The device code is marked is CUDA keywords for identifying data-parallel functions and has a extension file .cu.

When a kernel is launched, executed by a large amount of threads, where they are organized as a one, two or three dimensional grid of thread blocks. A thread is the simplest executing process. It consists of the code of the program, the particular point where the code is being executed. \cite{hwu}. Many threads form a block, and many blocks form a grid. CUDA handles the execution of the random-access threads, which take up-to very few clock cycles in comparison to CPU threads. The threads per block can be observer in figure \ref{fig:grid}. All the threads in a kernel can access the global memory, figure \ref{fig:arch}.

Each of the threads can be access by implicit variable that identifies its position within the thread block and its grid. In a case of 1-D block. \cite{example}

$$blockIdx.x \times blockDim.x + threadId.x$$

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/grid.png}
		\rule{35em}{0.5pt}
	\caption[Part of the CUDA's 2D grid]{Part of a 2D CUDA's thread grid, divided in blocks, each block with it’s own respective threads.}
	\label{fig:grid}
\end{figure}

In CUDA. host and device have separate memory spaces. This can bee seen on the host and device with the DRAM(Dynamic random-access memory) data. For example a NVIDIA GTX 660m comes with 2GB of memory, which is the global memory for the device. As told the host and device allocates data. The programmer needs to send data from the host memory to the device's global memory. We can see this in the figure  \ref{memorySpace:arch}.  Once the memory is transfer back to the host, is completely necessary to free the memory from the device and host. This is typically done with free or delete on C/C++. The CUDA's Application Programming Interface (API) functions performs this activities on behalf of the programmer.   \cite{hwu}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/memorySpace.png}
		\rule{35em}{0.5pt}
	\caption[Memory Space GPU and CPU]{Separate memory spaces for the CPU and GPU}
	\label{fig:memorySpace}
\end{figure}

\subsection{Vector Addition Example}

A simple example of a vector addition to show the comparison between the GPU and CPU, input, two list of number which is sum up each corresponded element to produce a final output with the addition of both list. Figure \ref{fig:sum} shows this process. \cite{example}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{Figures/sum.png}
		\rule{35em}{0.3pt}
	\caption[Vector Addition Example]{Simple Vector Addition Example}
	\label{fig:sum}
\end{figure}

\subsubsection{CPU Code}

This first example illustrates the CPU code executed in a single thread. The code is straight forward to understand. First create the memory for each array, A, B and C with size N. Then calculate the sum of the two vectors with the function \textit{add}. As we can see in the function \textit{add}, we use the while loop to go through each element of the arrays A and B, which are added into a single array C.

\begin{lstlisting}[language=C++, caption={CPU Vector Addition}]
#include <iostream>

#define N 100

void add( int *a, int *b, int *c );

int main()
{
    int A[N], B[N], C[N];
    
    //fill the arrays with values
    for(int i = 0; i < N; i++){
        A[i] = 1;
        B[i] = i;
    }
    
    add(A, B, C);
    
    //Display the results
    for (int i = 0; i < N; i++) {
        std::cout << A[i] << ", " << B[i] << ", " << C[i] << std::endl;
    }
    
    return 0;
}

void add( int * A, int * B, int * C )
{
    int index = 0;
    
    //go through each index of the arrays and make the operation
    while(index < N){
        C[index] = A[index] + B[index];
        index++;
    }
}

\end{lstlisting}

We can notice if we set N to be a large number, the function \textit{add} could take a large amount of time to execute. But the example only illustrates the used of the CPU as a single core, however nowadays CPUs commonly have around 4-8 cores. To be able to execute the previous code on all the cores available in the CPU, threads are needed to be implemented. But you would need reasonable amount of code and debugging to make that happen. Also is a complicated task to schedule all the threads in the CPU. 

\subsubsection{GPU Code}

We can accomplish the same operation very similar in the GPU with CUDA. First create CPU and GPU memory with there corresponded code. Send the CPU memory to the device, make calculations on the highly parallel GPU, finally return the results the CPU.

\begin{lstlisting}[language=C++, caption={GPU Vector Addition}]
#include <iostream>

#define N 100

// CUDA KERNEL
__global__ void add( int *a, int *b, int *c );

int main()
{
    int a[N], b[N], c[N];
    int *dev_a, *dev_b, *dev_c;

    // allocate the memory on the GPU
    cudaMalloc( (void**)&dev_a, N * sizeof(int) ) );
    cudaMalloc( (void**)&dev_b, N * sizeof(int) ) );
    cudaMalloc( (void**)&dev_c, N * sizeof(int) ) );
    
    //allocate the memory on the CPU
    for(int i = 0; i < N; i++){
        A[i] = 1;
        B[i] = i;
    }
    
    //calculate the vector addition in the GPU
    add<<<N,1>>>( dev_a, dev_b, dev_c );
    
    //copy back the result from the GPU to the CPU 
    cudaMemcpy( c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost ) );
     
     
    //Display the results
    for (int i = 0; i < N; i++) {
        std::cout << A[i] << ", " << B[i] << ", " << C[i] << std::endl;
    }
    
    cudaFree( dev_a );
    cudaFree( dev_b );
    cudaFree( dev_c );
    
    return 0;
}
\end{lstlisting}

As we can see the function \textit{cudaMalloc} and \textit{cudaFree} are very similar to the C code functions \textit{malloc} and \textit{fee} for allocating memory and deleting memory.

CUDA automatically spams the threads to it correspondent block, so we only need to access the index of the block and pass it to the index arrays. To parallel code will stop in-till the block index reaches the number of elements of the arrays, N.

\begin{lstlisting}[language=C++, caption={GPU Vector Addition}]
void add( int * A, int * B, int * C )
{
    // handle the data at this index if (tid < N)
    int index = blockIdx.x; 
    if(index < N)
        c[index] = a[index] + b[index];
}
\end{lstlisting}

The biggest difference between the CPU code and the GPU code is how threads are managed with-in the process. The CPU code has only one thread, thrust one loop, however if we want to expand the CPU code to multiples threads, extra loops are required for each additional thread. Based on this idea, the GPU code launches to code throughout every threads accessible by the device chip. 

\vspace{3.2em}

Finally, this chapter provided a quick overview of heterogeneous programming in a modern context. CUDA  enhance the C language with parallel computing support. Which is possible to launch  enormous amounts of parallel threads, oppose of few threads on the CPU. The number of GPU cores will continue to increase in proportion to increase in available transistors as silicon process improve. In addition, GPUs will continue to go through vigorous architectural evolution. Despite their demonstration high performance on data-parallel applications. \cite{hwu}








