% Chapter Template

\chapter{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque} % Main chapter title

\label{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

This chapter is the explanation of the CUDA implementation.




\section{Simulation}

The simulation is divided into two calculations parts, the host code and the device code. As the figure \ref{fig:flow} illustrates the data flow of the simulation. Each step of the simulation is going to explain in the following section with more detail. First that data is read or created on the host. Then the initial calculations of the simulation are calculated. Notice that only once is calculated. Afterwards the simulation is started by calculating the fourth order Runge and Kutta integration several times. When RK4 is finish the effective values are calculated to produce the final beta variation. As the figure \ref{fig:flow} shows all the intense computation is done on the GPU side. While on the  CPU only minor intense computation are done such as I/O data, memory allocation and final beta variation. 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{Figures/flow.png}
		\rule{35em}{0.2pt}
	\caption[Control flow]{Control flow of the simulation}
	\label{fig:flow}
\end{figure}

\subsection{Data allocation and threads}

The first section of the implementation starts by allocation data into several matrices for both host memory and device memory. The initial magnetization data can be either self generated by the application or by reading a file which contains information related to the magnetization. In both cases the data is divide into two blocks of data. Both blocks have 57600 (480 x 120) rows of information The first 57600 rows contains initial magnetization x, y coordinates. The next block of 57600 rows is the initial magnetization in x, y, and z. The data being read is stored on a three temporary 2D matrix, that corresponds to the x, y, and z coordinate. In addition the three matrices are flatten into three continuous memory blocks, as showed in figure \ref{fig:flaten}.


\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.4\textwidth]{Figures/flaten.png}
		\rule{35em}{0.2pt}
	\caption[2D Flatten array]{Converting 2D array to a single continuous block of memmory}
	\label{fig:flaten}
\end{figure}


To ensure optimal memory allocation on the GPU side is best to assign square matrices to the device. The calculation for this operation is done using the first two operations in \ref{lst:blocks}.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l | }
\hline
Matrix size X & 480 & Device allocation X & 512\\
\hline
Matrix size Y & 120 & Device allocation Y & 128 \\
\hline
\end{tabular}
\end{table}

The magnetization data is stored on matrices of x, y, and z, which has 56700 values, in other words 480 times 120. Base on this information we want to calculate the optimal number of grids that will ensure complete use of the hardware resources. The number of blocks per grid corresponds dividing  the dimensions of the array by the number of threads. The last two operations in \ref{lst:blocks}.

\begin{lstlisting}[language=C++, label={lst:blocks}, caption={Device capacity calculation and number of block per grid}]	
NXCUDA = (int)powf(2,ceilf(logf(NX)/logf(2)));
NYCUDA = (int)powf(2,ceilf(logf(NY)/logf(2)));

//Setup optimum number of blocks
XBLOCKS_PERGRID = (int)ceil((float)NX/(float)XTHREADS_PERBLOCK); 
YBLOCKS_PERGRID = (int)ceil((float)NY/(float)YTHREADS_PERBLOCK);
\end{lstlisting}

Depending on the hardware properties, each GPU can allocate different number of threads per block and as well as different shared memory size. The Shared memory in this implementation relays on the number of threads per block. In addition the number of blocks depends on the input matrix and the number of threads.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l |}
\hline
 & Fermin & Kepler \\
\hline
Threads per block X  & 16 & 32 \\
\hline
Threads per block Y  & 16 & 32 \\
\hline
Number of blocks X & 30 & 15 \\
\hline
Number of blocks Y & 8 & 6 \\
\hline
Shared memory & 16 * 16 & 32 * 32 \\
\hline
\end{tabular}
\end{table}

The initial values for the simulation are

\begin{table}[h]
\centering
\begin{tabular}{| l | l |}
\hline
TX     & 1200.0                                         \\
\hline
TY     & 300.0                                          \\
\hline
TZ     & 5.0                                            \\
\hline
u      & 1                                              \\
\hline
D      & 1.0e\textasciicircum 3 nm\textasciicircum 2/ns \\
\hline
tau sd & 1.0e\textasciicircum -3 ns                     \\
\hline
tau sf & 25.0e-3 ns      \\
\hline
\end{tabular}
\end{table}

\subsection{Initial Calculations}



\subsection{Fourth order Runge and Kutta}

 this method is implementation to numerically solve the differential equation. Intuitively the implementation on CUDA code is done with 4 kernels, where each kernel calculates respectively the order of the integrator. In the last term calculation is where all the magic occurs, the sum of the previous 3 calculated terms. The code \ref{lst:rkcuda} computes the Runge and Kutta terms for the 1st, 2nd, and 3rd. For the 4th term is sum of the previous 3 calculations.

\begin{lstlisting}[language=C++, label={lst:rkcuda}, caption={CPU Vector Addition}]
    int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    // map the two 2D indices to a single linear, 1D index
    int index = j * grid_width + i;

    if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
    {
        k3x[index] = dt * deltam_x[index];
        deltam_x[index] = tempx[index] + 0.5 * k3x[index];   
    }
\end{lstlisting}

The integration of Runge and kutta is the main of the simulation \ref{lst:rkiter}

\begin{lstlisting}[language=C++, label={lst:rkiter}, caption={Runge y Kutta integration}]
	//cross podruct
    gsd_exchange<<<blocks, threads>>>(...);

	//Evaluates laplacian with boundary condition
    glaplacianx<<<blocks, threads>>>(...);
    glaplacianyboundaries<<<blocks, threads>>>(...);
    glaplaciany<<<blocks, threads>>>(...);

    //This call evaluates dm / dt at t = n
    gsolution<<<blocks, threads >>>(...);

    //This call evaluates the first term of the RK4 integrator
    gterm1_RK4<<<blocks, threads >>>(...);
}
\end{lstlisting}


The integration can be sum up as follow were
\ref{lst:rk4}
\begin{lstlisting}[language=C++, label={lst:rk4}, caption={Runge y Kutta integration}]	

for(int i = 0; i < 4; i++)
    gsd_exchange<<<blocks, threads>>>(i);
    glaplacianx<<<blocks, threads>>>(i);
    glaplacianyboundaries<<<blocks, threads>>>(i);
    glaplaciany<<<blocks, threads>>>(i);
    gsolution<<<blocks, threads >>>(i);
    gterm_RK4<<<blocks, threads >>>(i);
}
\end{lstlisting}


\subsection{Calculate effective beta}


\begin{lstlisting}[language=C++, label={lst:rk4}, caption={Runge y Kutta integration}]	
    gm_x_sm << <blocks, threads >> >(...); //Calculate
    gu_eff << <blocks, threads >> >(...);  //Calculate 
    gu_eff_beta_eff << <blocks, threads >> >(...); 	//Calculate
    gbeta_eff << <blocks, threads >> >(..); 	//Calculate
    gbeta_diff << <blocks, threads >> >(...);  //Calculate             
\end{lstlisting}

\subsection{Calculate beta variation}

At the end 

\section{Validation}

The validate the code, that is obtained from the simulation

Once obtain the results from the simulation, the results are written into two separated data files; .eff and .spin. depending of the configuration of the application is possible to obtain the uVW or the. Because CUDA framework is highly parallel system is fairly easy to obtain erroneous data from the calculations, even setting up the threads per block incorrectly is possible to get data set that a wrong, or results that don't diverge. It is necessary that when finishing making changes to the code validating the results with a valid data set is done.

The validation is done by checking the output the simulation with a valid data set, the output of the validation application tells us the error factor of the current data with the valid set. So for each data set there is a threshold value, that can tell if the that is close enough to the results. A example of the validation performed.



The performance validations 

This folder contains scripts used for the different tests of the spin accumulation code 
between versions, so far we provide a script to validate the $*$-spin.dat and $*$-eff.dat
files produced by your improved code at the tests folder.

The unittest.py file contains a script for reading the parameters.h file under include 
directory, the $*$-spin.dat file computed with your code from the src file which has been
compiled with the appropriate make option, and the reference DW $*$-spin.dat and $*$-eff.dat
files found in this directory.

According to our results, new code shouldn't produce errors in the $*$-spin.dat data 
greater than 7.0e-17, in other words valid code don't lead to differences greater than
the precision expected from computations with double precision 1.0e-16 in the case of
eff data the errors are in the order of 1.0e-11 and no greater than 6e-11



\subsection{Finite Precision}


We all know summing one-third three times yields to one.

$$\dfrac{1}{3} + \dfrac{1}{3} + \dfrac{1}{3} = 1$$

 However this is not always true on a computer. It actually depends of the floating-point precision.
 
 
  
  
validation float error precision





