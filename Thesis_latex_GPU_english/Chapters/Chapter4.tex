% Chapter Template

\chapter{Optimization Results} % Main chapter title

\label{Optimization Results} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Optimization Results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

This chapter is the results of the CUDA code implementation launched on several different GPUs nodes. The test are performed on various GPUs architectures, which, has different hardware characteristics. Each GPU node is analyzed using the NVIDIA's Visual Profiler, in addition the CUDA kernels are evaluated in performance; throughput, bandwidth, executing and parallel time. Furthermore the results, are analyzed and optimized using the schemes from chapter 3. Lastly the code is executed remotely on the supercomputer ``Piritakua'' at the Department of Multidisciplinary Studies Yuriria, University of Guanjauato

\section{Supercomputer ``Piritakua''}

The experiments are carried out using the supercomputer “Piritakua”. The massive GPU cluster was design and built by Dr. Claudio from the University of Guanajuato Multidisciplinary Studies Yuriria. The GPU cluster is located at a small town of Mexico, Yuriria. The supercomputer at the Front-end has a eight core Intel Xeon at 2.4 Ghz, at the back-end several GPU are connected, one NVIDIA Tesla K20, two Tesla M2070 and a GTX 580.

A GNU$ \ $LINUX distributionis installed on the system, the CentOS 64 bits version 6.4. CentOS stands for Community Enterprise Operating System which is free operating system and one of the most popular GNU$ \ $Linux distribution for web servers and as well is supported by RHEL (Red Hat Enterprise Linux). \cite{centos} 

The specifications of the front-end cluster.

\begin{tabular}{ | p{7.1cm}  | l | l | l |}
  \hline
  Processor & Number & Cores & RAM  \\
  \hline
  Servidor Dell Intel Xeon E5620 2.4 GHz & 1 & 8 & 12 GB \\
  \hline
  Servidores HP Proliant SL 350s Gen3 Intel Xeon X5650 2.67 GHz & 2 & 24 & 32 GB \\
  \hline
   Servidores HP Proliant SL 250s Gen8 Intel Xeon E5-2670 2.60 GHz & 3 & 48 &104 GB \\
   \hline
   CPU Xeon Phi  5110p & 1 & 8 & 8 GB\\
   \hline
   CPU Xeon Phi 7120p  & 1 & 8 & 16 GB\\
   \hline
  \end{tabular}


 The CUDA Code was launched on only two CPUs, a laptop with a eight core intel i7-3630QM and a high-end CPU Xeon Phi 7120p from the cluster. In addition the Xeon Phi was used for all the experiments for the Cluster's GPUs. The Xeon Phi 720p is capable of achieveing f 1.2 teraflops of double precision floating point instructions with 352 GB/sec memory bandwidth at 300 W.

When accessing ``Piritakua'' remotely is possible to use all the GPUs available on the cluster.
 The specifications of the GPU connected to the front-end are as follow.

  \begin{tabular}{ |  l  |  l  |  l  |  l  |  l  | l | }
    \hline
    Model & Cores & RAM & DP & SP & Bandwidth \\
    \hline
    Tesla K20m & 2496 & 5GB & 1.17 Tflops & 3.52 Tflops & 208 GB/s \\
   \hline
    Tesla M2070 & 448 & 6GB & 515 Gflops & 1030 Gflops & 150 GB/s \\
   \hline
     Tesla C2050 & 448 & 2.5GB & 512 Gflops & 1030 Gflops & 144 GB/s \\
   \hline
      GeForce GTX 580 & 512 & 1.5GB & 520 Gflops & 1,154 Gflops & 192.2 GB/s \\
   \hline
   GeForce GTX 670MX & 960 & 3GB & 520 Gflops & 1,154 Gflops & 67.2 GB/s \\
   \hline
  \end{tabular}

   The code was launched on all Piritakua's GPUs and on external GeForce GTX 670m, located on a laptop. The "m" stands for the mobil graphic card version. In addition the 670m card is design for less power usage, but with high graphics power, it even has more cores than some Tesla models, however this types of cards has way more less Bandwidth than standard versions.

There are two GPU architectures that code was launched, the Fermi and the Kepler. The Tesla K20m is base on ``Kepler'' GPU architecture and Tesla M2070, Tesla M2050 and GeForce GTX 580 on the Fermi architecture. The Kepler is a newer architecture than the Fermi. The big difference between the is the number of CUDA cores per SM.

  
\subsection{Architecture Differences}

  architecture dependent technical differences of Nvidia GPUs. During CUDA development a lot of internal features have been improved, but most paradigms for the programmer stayed the same. For example a streaming processor can now handle 2048 threads at a time, but the maximum block size stayed at 1024. This results in a 100$\%$ theoretical occupancy for block sizes of 1024 compared to 66$\%$ of Fermi. Another example is the use of Shared Memory. Maxwell has 64KB dedicated Shared Memory. The maximum amount of Shared Memory per Block is 48KB for all three architectures.
  
\begin{table}[h]
\centering
  \begin{tabular} { | l | l  | l | l | l  |  l  | l |}
    \hline
    Name & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} &  \multicolumn{2}{|c|}{Maxwell} \\
    \hline
    Compute Capability & 2.0 & 2.1 & 3.0 & 3.5 & \multicolumn{2}{|c|}{5.0}\\
   \hline
    Single Precision Operation per Clock and SM & 32 & 48 & \multicolumn{2}{|c|}{192} & \multicolumn{2}{|c|}{128}\\
   \hline
    Double Precision Operation per Clock and SM & $4/16^1$ & 4 & 8 & $8/64^2$ & \multicolumn{2}{|c|}{$1^3$}\\
   \hline
    Maximum Number of Threads per SM / SM & \multicolumn{2}{|c|}{16} & \multicolumn{4}{|c|}{32}\\
   \hline
    Maximum Number of Registers per Thread/ SM & \multicolumn{3}{|c|}{1536} & \multicolumn{3}{|c|}{2048}\\
   \hline
       Maximum Number of Threads per Block & \multicolumn{6}{|c|}{1024}\\
   \hline
   Active Thread Blocks per SM / SM & \multicolumn{2}{|c|}{8} & \multicolumn{2}{|c|}{16} & \multicolumn{2}{|c|}{32}\\
   \hline
   Maximum Warps per Multiprocessor/ SM & \multicolumn{2}{|c|}{48} & \multicolumn{4}{|c|}{64}\\
   \hline
   Registers / SM & \multicolumn{2}{|c|}{32K} & \multicolumn{4}{|c|}{64K}\\
   \hline
   Level 1 Cache & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Shared Memory / SM & \multicolumn{2}{|c|}{16/48 KB} & \multicolumn{2}{|c|}{16/32/48 KB} & \multicolumn{2}{|c|}{64 KB}\\
   \hline
   Warp Size & \multicolumn{6}{c|}{32}  \\
   \hline

  \end{tabular}
  \end{table}
  

\subsection{Experiment metrics}

The initial implementation was launched on several GPU nodes as well as several times on the same node. The implementation was launched 10 times on each node, For example the following table are the results of the initial implementation launched on the NVIDIA Tesla K20m.

\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l  |  }
    \hline
    number & time  \\
    \hline
    1 &  64846.0 \\
   \hline
    2 & 59010.4 \\
   \hline
    3 & 67332.4\\
   \hline
    4 & 68171.8 \\
   \hline
    5 & 61818.0 \\
   \hline
    6 & 65111.3\\
    \hline
    7 &  67346.4\\
    \hline
    8 & 65666.6 \\
    \hline
    9 & 67343.3 \\
    \hline
    10 & 65681.8      \\
   \hline
  \end{tabular}
  \end{table}
  
As we can see each time the code is executed, different time results.  . The order in which operations are evaluated can significantly affect the final value of a floating point computation; t
It's possible that the first issued block might be issued to different multiprocessor each time, and if there are an uneven number of blocks across multiprocessors, then the execution order could vary slightly between runs \cite{cook}.

As we know there is no garanty that threads will execute at the same order. To accomplish such task thread sycronization is needed.



\section{Results}

The CUDA code was launched on each GPU of the Piritakua supercomputer. As we know the supercomputer has different GPU, as well as several architectures and different number of CUDA cores.


metion cuda versions. as well as the compiler used for the test.


\begin{table}[h]
\centering
  \begin{tabular} { |  l  |  l  |  l  | l |}
    \hline
    Node & initial Avg time & 1st optimization & 2nd optimization \\
    \hline
    Tesla K20m &  64846.0 & 55 & 33\\
   \hline
    Tesla M2070 & 64846.0 & 55 & 33\\
   \hline
    GeForce GTX 580 & 64846.0 & 55 & 33\\
   \hline
    GeForce 670m & 64846.0 & 55 & 33\\
   \hline
  \end{tabular}
  \end{table}
  
  

  
  
\subsection{Initial Test}

The initial implementation was launched on several GPU nodes, as previous mentioned.  As we know .Se figure.\ref{fig:iniresults}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.65\textwidth]{Figures/initial_results.png}
		\rule{35em}{0.2pt}
	\caption[Initial GPU results]{Initial resutls of the implementation runing on several differen GPU nodes}
	\label{fig:iniresults}
\end{figure}


\subsection{Finite Precision}

We all know summing one-third three times yields to one.

$$\dfrac{1}{3} + \dfrac{1}{3} + \dfrac{1}{3} = 1$$

 However this is not allways true on a computer. It actually dependes of the floating-point precition.
 
 
  


\subsubsection{Visual profiler}

The visual profiler.

The visual profiler was used on Laptop with GeForce GTX 670m with the intel eight core i7-3630QM.


the

command

 nvprof –o nvprof.log ./command

\subsection{Concurrent Kernels}

\subsection{Shared Memory}

load reduction, reference to

http://www.bu.edu/pasi/files/2011/07/Lecture31.pdf


\subsection{Obtimized}


%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Subsection 2}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Main Section 2}
