% Chapter Template
\chapter{Heterogeneous Performance Analysis and Practices} % Main chapter title

\label{Heterogeneous Performance Analysis and Practices} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Heterogeneous Performance Analysis and Practices}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
When working with GPUs hardware new challenges emerges. How can we make the best usage of the GPU hardware. In the conventional CPU model, we have what is called linear or flat memory model. In addition this appears to the programmer as a single contiguous address space. Furthermore, the CPU can directly address all the available memory, in other words there is almost no efficiency penalty in creating global data, local data, or even access data that is located on a opposite memory location, all of this can be access as a contiguous block \cite{cook}. Meanwhile, on the GPU there are exceptions, their exists different memory hierarchies which dramatically change the performance output. By allocation the optimal memory types, speedup and increase throughput can be accomplished. To ensure optimization, some analysis should be done, like comparing latency, memory hierarchies and data bandwidth between CUDA kernels. The debug of parallel CUDA code can be accomplish using the NVIDIA's Visual Profiler. This chapter demonstrated techniques, practices and methods to debug and analyzed parallel process.

% techniques, programming models and hierarchies.
\section{Practices}

There are three rules for developing high performance GPGPU (General-purpose on the GPU) program, which are based on NVIDIAs GPU standards \cite{design}.

\begin{enumerate}
  \item Get the data on the GPU device and keep it there
  \item Process all the data en the GPU, give it enough work to do.
  \item Focus on data reuse within the GPU context, to avoid memory bandwidth limitations
\end{enumerate}

As we know the GPUs are plugged into the PCI Express bus of the host computer, in other words the CPU. The PCIe bus has extremely slow bandwidth compared with the GPU. This is why is important to store the data on the GPU and keep it busy. In addition minimize the data transfer from the host and back to the device. We can see this in the table \ref{fig:PCI}. CUDA enables the GPU to carry out petaFLOP performance in a single device \cite{cook}. In addition they are fast enough to compute a large amount of data. To accomplish such high performance, each CUDA Kernel needs to use all the available resources of the GPU. Furthermore, avoid wasting compute cycles. Finally if a single Kernel doesn't use all of the available bandwidth, multiple kernels can be launched at the same time on a single GPU, which are streams \cite{design} 

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.72\textwidth]{Figures/PCI.png}
		\smallskip
	\caption[PCIe Bandwidth]{PCIe bus and GPU bandwidth comparison \cite{cook}}
	\label{fig:PCI}
\end{figure}

The practices should be taken in consideration to identify the portions of code where it would be beneficial for improving GPU acceleration \cite{practices}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{Figures/apod.png}
		\smallskip
	\caption[GPU application practices]{GPU application practices \cite{practices}.}
	\label{fig:apod}
\end{figure}

\begin{description}

 \item{Asses} \hfill \\
 The first step is to locate the part of the code where the majority of the execution time occurs. The programmer can evaluate memory bottlenecks for GPU parallelization.
 \item{Parallelize} \hfill \\
 Increase parallelization from the original code, could be either adding GPU-optimized libraries such as cuBLAS, cuFFT, or including more amount of parallelism exposure though the use of CUDA code.
 \item{Optimize} \hfill \\
The developer can optimize the implementation performance through a number of considerations, overlapping kernel executing, kernel profiling, memory handling and fine-tuning floating-point operations.
 \item{Deploy} \hfill \\
 Compare the outcome with the original expectation. Determinate the potential speedup by accelerating a given section. First a partial parallelization should be implementation before carrying out a complete change.
 \end{description}

\section{Performance Metrics}

There are many possible approaches to profiling the code, but in all cases the objective is the same:  identify the kernel or kernels in which the application is spending most of its execution time and increase the throughput by a giving kernel. Throughput is how many operations completed per cycle.

\subsection{Timing}

Timing a launched kernel can be done on either the GPU or the CPU. Is important to remember that the CPU and GPU are not synchronized. So its necessary to synchronize the CPU thread with the GPU kernels launches. CUDA provides the required  functions to synchronize the CPU with the GPU calling immediately before starting the timer \cite{practices}. CUDA is able to handle timers within the GPU, which records times in a floating-point value in milliseconds. This is done with $cudaEventRecord()$, just by including $start$ and $stop$ in the function inputs. Note that the timing are measured on the GPU clock, so the timing is independent from the OS \cite{cook}. The timing performed on the application is showed in Chapter \ref{Optimization Results}.

\subsection{Bandwidth}

The bandwidth refers to the rate at which data can be transferred between host and device and vi-versa. The bandwidth is one of the most important factors for testing performance o the GPUs. Choosing the right type of memory could dramatically increase performance and bandwidth. There are two main bandwidth types to indicate performance, theoretical bandwidth and effective bandwidth. The theoretical bandwidth is base on the hardware specifications that is available by NVIDIA. This is calculated using the following formula:

$$ theoretical bandwidth = (clock rate * (bit-wide-memory-interface / 8 )*2) / 10^9 $$

For example the NVIDIA GeForce GTX 280 uses DDR RAM with a memory clock rate of 1,105 MhZ and a 512-bit-wide memory interface

$$ (1107 * 10^6 * (512/8.0) *2 )/10^9 = 141.6 Gb /sec$$

The GTX 280 has a theoretical bandwidth of $141.6Gb/sec$.
The effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the application \cite{practices}.

$$effective-bandwidth = ((Br - Bw) / 109 )/time$$

Where Br is the number of bytes read per kernel, Bw is the number of bytes written per kernel and  t is the elapsed time given in seconds \cite{fortran}.

In practice the difference between theoretical bandwidth and effective bandwidth indicated how much bandwidth is wasted on accessing memory and calculations. If the effective bandwidth is low compared to the theoretical bandwidth is one indication that there is not enough work being done in the GPUs. In addition there a several solutions; analyze the code to make more parallelize instructions, execute more computational instructions on the GPUs, finally analyze the number of threads per block that are executing on execute kernels.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Memory Handling with CUDA}

In this section four types of memory handling are going to be explained, global memory (device memory), shared memory, texture memory and constant memory. The figure \ref{fig:cores} illustrates physically the position of the different memory types inside the device chip.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.93\textwidth]{Figures/cores.png}
		\smallskip
	\caption[Schematic cache hierarchy of a CUDA GPU]{The schematic cache hierarchy of a CUDA GPU with 4 Streaming Multiprocessors and 8 CUDA Cores each \cite{cook}.}
	\label{fig:cores}
\end{figure}

Global memory is very large in comparison to the shared memory, which is on the L1 cache. However, the global memory is far away from the registers and from the CUDA core locations. Moreover, the memory access is very slow in comparison to the shared memory \cite{cook}.

The table \ref{fig:memory} illustrates the five different memory types that are available in CUDA. But more interesting the bandwidth penalty and the latency in computer cycles for each on of them. Moreover, different memory types can be used in different applications to maximize performance, hence memory usage. The Shared Memory is very limited so it cannot be handler across all situations. Furthermore, when implementation wrong memory type on the device there can be latency penalty and bandwidth drop, instead of having a gain in performance.
 
\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.87\textwidth]{Figures/memory.png}
		\smallskip
	\caption[Different memory types]{Different memory type and penalties usage \cite{cook}}
	\label{fig:memory}
\end{figure}

\subsection{Global Memory}

Understanding how efficiently use global memory is essential part of CUDA memory management. Focusing on data reuse within the SM and caches avoids memory bandwidth limitations. Global memory on the GPU is designed to quickly stream memory blocks of data into the SM.

\begin{itemize}
\item Get the data on to the Device, keep it there.
\item Give the GPU enough workload, this using all the resources available from the GPU.
\item Focus on data reuse within the GPGPU to avoid memory bandwidth limitations.
\end{itemize}

In other words the global memory resides on the device, and it should be anything from 1 byte to 8GB, depends on the GPU RAM available. Furthermore, the memory is visible to all the threads of the grid. Every thread at a given location is possible to read and to write global memory, The memory is always allocated with the keyword \textit{cadaMalloc}. In addition, the global memory is only used by passing to the kernel call the keyword \twoline global \twoline \cite{design}. Global memory is widely used for the current implementation.

\subsection{Shared Memory}

CUDA C compiler treats variables differently than a typical c variable, it creates a copy of the variable for each block that is launched on the GPU, now every thread in that block can access the memory, this is why is called shared memory. This memory reside physically on the GPU, because the memory is very close the cache, the latency is typical very low \cite{example}. One thing comes to mind, if the threads can communicate with others threads, so there should be way to synchronize all the threads. A simple case should be if thread A writes a value into the shared memory, and Thread B wants to access we need to synchronize, when thread A finish writing then Thread B can access it. This is typical case when shared memory with synchronize thread is needed \cite{cook}.
Shared memory is magnitudes faster to access than global memory, essentially is like a local cache for each threads of a block. While  the shared memory is limited to 48K a block, the global memory is the amount of DRAM on the device. The duration of the shared memory on the device is the lifetime of the thread block. Using \twoline shared \twoline in-front of the data type will innovate shared memory.

Shared memory is widely used for applications were the kernels access a great amount of global memory. In addition, using shared memory eliminates the use of clock cycles per kernel which increases performance on a single kernel call. The optimizations results of increasing the use of shared memory are located in Chapter \ref{Optimization Results}.

   
\subsection{Constant Memory}

Is an excellent way to store and broadcast read-only data to all the threads on the GPU. One thing to keep in mind is that the constant memory is limited to 64KB \cite{design}. A simple analogue is the \textit{\#define} or \textit{const} attribute in the C++ programming language, the variable performs like a variable that cannot be modified. On CUDA this is exactly the same, the value can only be read and not written. Furthermore, the value will not change over the course of a kernel execution and only the host can write the constant memory \cite{example}.

\subsection{Texture Memory}

Like constant memory, texture memory is another variety of read-only memory that can improve performance and reduce memory traffic when reads have certain access patterns. Traditionally texture memory is used for computer graphics applications, but it can also be use for HPC. The main idea of this read-only memory is that threads are likely to read from address ''near' the address they nearby threads \cite{example}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.52\textwidth]{Figures/texture.png}
		\smallskip
	\caption[Texture Memory]{Mapping of threads into a two dimensional array of texture memory \cite{hwu}}
	\label{fig:texture}
\end{figure}

The texture Memory in a form works like the GPU graphics Texture, when you want to use the texture bind with some sort of data is necessary and when you finish using it unbind the texture from the data. The usage can be summarized in the following table:

\begin{itemize}
\item Allocate global memory in the Host.
\item Create Texture reference and bind it to memory object.
\item On the device obtain the reference from the texture.
\item  Use Texture memory operations on the device
\item  When the work is done on the Texture, unbind the texture reference on the host.
\end{itemize}


The texture memory is not used on the current implementation, for obvious reasons, it is a read only memory. For the application we need to constantly read an write blocks of memory.

\subsection{Thread Synchronization}

This refers to synchronizing threads operations. For efficiency, a pipeline can be created by queuing a number of kernels to keep the GPGPU busy for as long as possible. Further, some form of synchronization is required so that the host can determine when the kernel or pipeline has completed \cite{design}. Commonly used synchronization mechanisms are:

\begin{itemize}
  \item Explicitly calling $cudaThreadSynchronize()$, which acts as a barrier causing the host to stop and wait for all queued kernels to complete.
  \item Performing a blocking data transfer with $cudaMemcpy()$ as $cudaThreadSynchronize()$ is called inside $cudaMemcpy()$.
\end{itemize}

The basic unit of work on the GPU is a thread. It is important to understand from a software point of view that each thread is separate from every other thread. Every thread acts as if it has its own processor with separate registers and identity. Will wait for all threads to finish there job \cite{design}.

Threads synchronization can also be accomplish inside of the kernels calls. The idea is the same, the kernel will wait until all the threads have completed there task. Threads synchronization is general used when loading data into shared memory. The implementation of such process is in Chapter \ref{Optimization Results}, section optimizations.

\section{Concurrent Kernels}

Kernels are executed in a sequential form with parallel instructions. In addition, with CUDA's streams is possible to launch several kernels in parallel, in other words, overlap kernel in the same launch sequence. As the figure \ref{fig:streams} illustrates.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.68\textwidth]{Figures/streams.png}
		\smallskip
	\caption[Concurrent Kernels]{Overlapping kernel execution using CUDA streams}
	\label{fig:streams}
\end{figure}

A stream in CUDA is a sequence of operations that execute on the device in the order in which they are issued by the host code. Every kernel is launch on the default stream zero. Hence, to overlap kernel execution, non-default streams should be used for every kernel launch. To accomplish concurrent kernels, streams should be pinned to a non-default stream (non zero)\cite{hwu}. 

Using two or more CUDA streams, we can allow the GPU to simultaneously execute a kernel while performing a copy between the host and the GPU. We need to careful about two things. First, the host memory involved needs to be allocated, since we will queue our memory copies, we need to synchronize those copies. Second. we need to be aware that the order in which we add operations to our streams will affect out capacity to achieve overlapping of copies and kernel execution. The general guideline involves a breadth-first, or round robin, to assign work and queue work to the kernels \cite{example}.

Concurrent kernels is technique which is implemented in this application. In addition, the order of kernel execution effected the operations of the streams. Moreover,  Chapter \ref{Optimization Results} illustrates the path taken to accomplish such task. 

\section{Kernel Analysis}

 Kernels are the essential part of CUDA programming, threads are launched automatically throughout each thread per blocks of the device. Furthermore, millions of threads execute the same code in parallel. However, the parallel code can be bound by three factors memory, compute and latency \cite{cook}.

\begin{description}

 \item{Memory Bandwidth Bound} \hfill \\
  Refers to code/application is limited by memory access. Most GPUs card have 1GB- 6GB of memory, which is used to process the data on the GPU. Different solutions are: reuse data, use different GPU memory types, implement a multi-GPU approach to increase the memory.

  \item{Compute Bound} \hfill \\
Refers to the computation time execution, in other works calculations done in the device, under the assumption that theres is enough memory for the calculations. This is the number of operations per cycle on the kernel. Theoretical bandwidth vs effective Bandwidth can measure performance for a compute-bound Kernel. Therefore, it is possible to increase the FLOPS per device.

 \item{Latency Bound} \hfill \\
 Is one whose predominate stall reason is due to memory fetches. This is actually the saturating the global memory, or any type, but still have to wait to get the data into the kernel. Physically can be the data being sent from one part of the Device to the other. Also depends the time required to perform an operation, and are counted in cycles of operations. A way to reduce the latency is to increase the number of parallel instructions (more  calls per thread), in other words more work per thread and fewer threads.
 \end{description}
 
Depending on the problem, the application can be bound by the previous three factors. The next chapter we are going to explain how and why the current implementation is bounded by memory, compute and latency.


\section{Hardware constraints}

Depending on the hardware capabilities limits how many threads per block a kernel launch can have. If exceed this values, the kernel will run incorrectly. The threads per block depends of the GPU hardware capabilities. The compute capabilities of a device is represented by a version number, also sometimes called its "SM version". This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU \cite{tool}. In a roughly summarized as:

\begin{itemize}
\item Each block cannot have more than 512/1024 threads in total. (Capability 1.x or 2.x-3.x)
\item The Maximum dimensions of each block are limited to [512,512, 64]/[1024, 1024, 64](compute 1,1.2)
\item Each block cannot consume more than to 8k, 16k, 32K registers total
\item Each block cannot consume more than 16kb/48kb of shared memory
\end{itemize}

Another inefficiency that can cause low performance in the CUDA application, is the number transfers memory calls between the CPU and the GPU. The GPU communicates with the CPU via a \textit{PCIe} bus as mentioned before. In addition, all of the massive FLOPS per second that can be achieve in the CPU cannot be sent back to the CPU. Because of the physical connection between the GPU and CPU. The GPU should be filled ip with enough workload at the beginning of the application and only at the end  return the memory back to the CPU. However, this is not always possible, a technique to increase more throughput from this operations is to pin the memory in the host. But also send as much data as possible in a single kernel call by using the maximum the GPU hardware capabilities \cite{practices}.

\subsection{Thread Division}

There a hardware limitations in how much threads per block a kernel can handle. Launching a kernel with the hardware constrains of the device will only ensure us that the kernel will actually be executed in the device, Nonetheless, not 100$\%$ optimal and the results can be incorrect. Furthermore, is necessary to launch kernels with the amount of threads per block base on the hardware settings. The block size will determine how faster the code will run. However, not the biggest block will run faster, depends con the problem and the data set. By Benchmarking the application, is possible to find the optimal configuration that best fits the problem. One thing to keep in mind, thread blocks should be a multiple number of SMs, with this idea is possible to obtain optimal thread block configuration. See Chapter \ref{Optimization Results} for the optimal thread configuration for the application.

\section{Visual Profiler}

Is a hard task to keep track of each individual thread. This becomes difficult for debugging highly parallel applications. The NVIDIA's Visual Profiler is a profiling tool that can be used to measure performance and find potential opportunities for optimization in order to archive maximum performance on the GPUs. The Profiler provides metrics in the form of plots and graphs, which describes opportunities to fully utilize the compute and data movements capabilities of the GPU, as well of each kernel launch in the application. See Figure \ref{fig:visualgraph}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{Figures/visualgraph.png}
		\smallskip
	\caption[Visual Profiler metrics]{Profiler provides optimization metrics necessary to improve the application.}
	\label{fig:visualgraph}
\end{figure}

NVIDIA's profiling tools comes in various flavors; a standalone profiler through the visual profiler compiler nvvp, integrated in a GUI NSight Eclipse Edition as NSight command (Visual Profiler), and as a command-line profiler though nvprof command. Each one has its disadvantages and advantages. The command-line profiler is useful for remotely access, where a GUI is not available, while the NSight can show graphs, plots and timeline of the application. The Profiler support CUDA applications as well as openCL applications. However, there are exceptions. 

The Visual Profiler, by default, will execute the entire application, nonetheless typically only some parts of application only need performance optimization. This enables to determine kernels, code where critical performances is needed. The common situations where profiling a region of the application is helpful \cite{tool}.

\begin{itemize}
  \item Analyze data initialization and movement in the CPU and GPU, as well as evaluating CUDA calls.
  \item The application operates in phases, where a algorithm operates throughout each region. The application can be optimized independently from other phases of the code.
  \item The application contains algorithms that operate though a large number of iterations. In this case is possible to collect data from a portion of the iterations.
\end{itemize}

The Visual Profiler provides a step-by-step optimization guidance, where is possible to evaluate the GPU usage, examine individual kernels and analyze timeline of the application which the profiler shows memory movements and usage, CUDA calls, number of threads and performance. The figure \ref{fig:visual01} shows, each Kernel has its own percentage of execution time of the overall application \cite{practices}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.9\textwidth]{Figures/pofiler.png}
		\smallskip
	\caption[Visual Profiler timeline and stream process]{Visual Profiler kernel execution, and timeline execution}
	\label{fig:visual01}
\end{figure}

\subsection{Profiler Kernel Report}

The profiler will execute several times the application for it to collect data from each kernels. This enables to precisely optimize phases of the application\cite{example}. The profiling tools can verify how long the application spends executing each kernel as well the number of used blocks and threads. Through this is possible to obtain various memory throughput measures, like global load throughput and global store throughput, indicate the global memory throughput requested by the kernel and therefore corresponding to the effective bandwidth mentioned in the last section.

As we know the profiler executes the application several time to collect data about each kernel. The information obtained by each kernel can be sum-up in-to a report that can be exported in a pdf file, which has the following information.

\begin{enumerate}
  \item \textbf{Compute, Bandwidth, or Latency Bound} \hfill \\
      The performance determines if the kernel is bounded by computation, memory bandwidth, or instructions/memory latency. It shows how is limiting the performance respectively.
  
  \item \textbf{Instructions and Memory Latency} \hfill \\
Instruction and memory latency limit the performance of a kernel when the GPU does not have enough work to keep busy. The performance of latency-limited kernels can often be improved by increasing occupancy. Occupancy is a measure of how many warps the kernel has active on the GPU, relative to the maximum number of warps supported by the GPU.
  
  \item \textbf{Compute Resources} \hfill \\
GPU compute resources limit the performance of a kernel when those resources are insufficient or poorly utilized. Compute resources are used most efficiently when instructions do not overuse a function unit. 
  \item \textbf{Floating-Point Operation Counts} \hfill \\
  floating-point operations executed by the kernel, can be either single precision or double precision.
  
  \item \textbf{Memory Bandwidth} \hfill \\
  Memory bandwidth limits the performance of a kernel when one or more memories in the GPU cannot provide data at the rate requested by the kernel.
\end{enumerate}

\subsection{Collect Data On Remote System}

As mention before, is possible to collect data from a remote system where a GUI is not available, using the command-line nvprof. Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed. Once the data is collected is possible to access the data using the Visual profiler, which enables a GUI and more compressive information about the application. There are two ways to perform remote profiling. To use nvvp remote profiling you must install the same version of the CUDA Toolkit on both the host and remote systems. It is not necessary for the host system to have an NVIDIA GPU \cite{tool}.

\vspace{3.2em}

Finally, this chapter gives a overview of practices and performance studies for GPGPU. In addition a better understanding of the hardware and memory management on the GPU. As well as hardware limitation, which determinate the best usage of the GPUs. As we can see NVIDIA's profiling tools is useful to analyze different stages of our application, moreover to determined which parts of the CUDA code is better to optimize from others to gain more performance.


