% Chapter Template

\chapter{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque} % Main chapter title

\label{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Implementation of DW Dynamics under Nonlocal Spin-Transfer Torque}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

This chapter is the study of the heterogenous implementation of Domain Wall Dynamics under Nonlocal Spin-Transfer Torque. We use the massively parallel capabilities of a single GPU to numerically solve a mathematical equation, known as the Zhang-Li model. The numerical method used for the solution is a the method known as Finite Differences in the Time Domain (FDTD) whose integration is done using the 4th order Runge - Kutta. The integration is done on a 3D grid space which outputs a single value, the effective beta.

\section{Simulation}

The simulation consist of the integration of the equation \ref{eq:zhang}, know as the Zhang and Li model for $1ns$ in a grid of 57,600 cells. The sample considered is a 300nm wide in y direction and 5nm thick in z direction NiFe (nickelâ€“iron alloy) soft nanostrip, a material and size widely uses in experiments with this characteristics. Using this size the asymmetric transverse wall \ref{fig:atw}, and the vortex wall. have nearly equal energies. The numerical mesh size is 3 x 3 x 5 $nm^3$, and the calculation box has a length (x direction) of 1,200 or 3,172 nm. The table \ref{tab:mesh} illustrates mesh information and calculation box for the simulation \cite{claudio}.

The table \ref{tab:drk} shows the constant values for the equation \ref{eq:zhang} such as $\mu$, $D_{0}$, $\tau_{sd}$ and $\tau_{sf}$.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
Mesh size & value & calculation box & value \\
\hline
Cell NX & 480 & Box TX  & 1200.0   \\
\hline
Cell NY & 120 & Box TY  & 300.0  \\
\hline
Cell NZ &	1 & Box TZ  & 5.0   \\
\hline
\end{tabular}
\caption{Mesh size and calculation box}
\label{tab:mesh}
\end{table}

The simulation is divided into two calculations parts, the host code and the device code. As the figure \ref{fig:flow} illustrates the data flow of the simulation. Each step of the simulation is going to explain in the following section with more detail. First the initial values are read from a data file. Then those values are used to calculate the initial matrices for the simulation. This values are only calculated once. Afterwards the simulation begins with a time step of 1.0  iterations. In addition each time step the fourth order Runge and Kutta(RK4) integration is calculated to solve \ref{eq:zhang}. Each 50,000 iterations the Beta difference is evaluated, if the beta difference convergence to 1.0e-9 the simulation stops. As the figure \ref{fig:flow} shows all the intense computation is done on the GPU side. While on the  CPU only minor intense computation are done such as I/O data, memory allocation and final beta variation.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l |}
\hline
Diffusion parameters& Value & Runge - Kutta 4th & Value \\
\hline 
$\mu$ & 1 &  time step (dt) &   $25.0e^{-6}$   \\
\hline
$D_{0}$ & $1.0e^{3}$ nm mm$^2$/ns  & tmax  & 1.0  \\
\hline
$\tau_{sd}$ & $1.0e^{-3}$ ns  & beta difference & $1.0e^{-9}$ \\
\hline
$\tau_{sf}$ & $25.0e^{-3}$ ns  & Iterations & 50,000 \\
\hline
\end{tabular}
\caption{Diffusion parameters and Runge - Kutta 4th}
\label{tab:drk}
\end{table}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.42\textwidth]{Figures/flow.png}
		\rule{35em}{0.2pt}
	\caption[Control flow]{Control flow of the simulation}
	\label{fig:flow}
\end{figure}

\subsection{Data allocation and threads}

The first section of the implementation starts by allocation data into several matrices for both host memory and device memory. The initial magnetization data can be either self generated by the application or by reading a file which contains information related to the magnetization. In both cases the data is divide into two blocks of data. Both blocks have 57600 (480 x 120) rows of information The first 57600 rows contains initial magnetization x, y coordinates. The next block of 57600 rows is the initial magnetization in x, y, and z. The data being read is stored on a three temporary 2D matrix, that corresponds to the x, y, and z coordinate. In addition the three matrices are flatten into three continuous memory blocks, as showed in figure \ref{fig:flaten}. The device code \ref{lst:flatten} flattens the 2d index into a single linear 1D index.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.4\textwidth]{Figures/flaten.png}
		\rule{35em}{0.2pt}
	\caption[2D Flatten array]{Converting 2D array to a single continuous block of memmory}
	\label{fig:flaten}
\end{figure}

\begin{lstlisting}[language=C++, label={lst:flatten}, caption={Kernel Flatten}]	
    int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    // map the two 2D indices to a single linear, 1D index
    int index = j * grid_width + i;
\end{lstlisting}

To ensure optimal memory allocation on the GPU side is best to assign square matrices to the device. The calculation for this operation is done using the first two operations in \ref{lst:blocks}.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l | l | }
\hline
Matrix size X & 480 & Device allocation X & 512\\
\hline
Matrix size Y & 120 & Device allocation Y & 128 \\
\hline
\end{tabular}
\caption{Matrix allocation size}
\label{tab:cuda}
\end{table}

The magnetization data is stored on matrices of x, y, and z, which has 56700 values, in other words 480 times 120. Base on this information we want to calculate the optimal number of grids that will ensure complete use of the hardware resources. The number of blocks per grid corresponds dividing  the dimensions of the array by the number of threads. The last two operations in \ref{lst:blocks}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/block.png}
		\rule{35em}{0.2pt}
	\caption[Grid layout]{Memory allocation in terms of the blocks per threads and grids}
	\label{fig:block}
\end{figure}

\begin{lstlisting}[language=C++, label={lst:blocks}, caption={Device capacity calculation and number of block per grid}]	
NXCUDA = (int)powf(2,ceilf(logf(NX)/logf(2)));
NYCUDA = (int)powf(2,ceilf(logf(NY)/logf(2)));

//Setup optimum number of blocks
XBLOCKS_PERGRID = (int)ceil((float)NX/(float)XTHREADS_PERBLOCK); 
YBLOCKS_PERGRID = (int)ceil((float)NY/(float)YTHREADS_PERBLOCK);
\end{lstlisting}

Depending on the hardware properties, each GPU can allocate different number of threads per block and as well as different shared memory size. The Shared memory in this implementation relays on the number of threads per block. In addition the number of blocks depends on the input matrix and the number of threads. See figure \ref{fig:block}.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l |}
\hline
 & Fermin & Kepler \\
\hline
Threads per block X  & 16 & 32 \\
\hline
Threads per block Y  & 16 & 32 \\
\hline
Number of blocks X & 30 & 15 \\
\hline
Number of blocks Y & 8 & 4 \\
\hline
Shared memory & 16 * 16 & 32 * 32 \\
\hline
\end{tabular}
\caption{Threads, blocks size}
\label{tab:threads}
\end{table}

\subsection{Initial Calculations}

The initial calculation is only a minor part of the whole simulation. However only once at the begining of the simulation the calculations are performed, \ref{lst:init}.

\begin{lstlisting}[language=C++, label={lst:init}, caption={Initial calculations}]
    gsource << <blocks, threads >> >(...); //Compute x, y and z component of source term
    gsource << <blocks, threads >> >(...);
    gsource << <blocks, threads >> >(...);

    //Project source term on magnetization components by computing
    //a cross product twicse
    gm_x_source << <blocks, threads >> >(...);
    gm_x_source << <blocks, threads >> >(...);
\end{lstlisting}

In \ref{lsd:init} the kernel \textit{gsource} evaluates once the matrix \textit{sm}, which is used to compute the Zhang-Li model in the RK4 integration. 

///shift error
// in implementation

As mention before the matrices for CUDA are square matrices 512 x 128, however the data set is 480 x 120. Because of the matrices size difference we need to limit the number of threads execution of the CUDA kernels. To limit such threads branching with a simple if within the kernels solves this issue \ref{lst:if}. Due to the boundaries condition in the implementing of the FDTD a small shift in the x direction is necessary. 

\begin{lstlisting}[language=C++, label={lst:lpay}, caption={Laplacian X using global memory}]
    if (i > 1 && i < NX + 2 && j >= 0 && j < NY)
    {
    //calculations
    }
    
\end{lstlisting}

\subsection{Numerical Methods}

micromagnetic simulation and demonstrate how to advance the configuration of the system through time. we do not know for how long we need to integrate the system until it stops in a local energy minimum configuration.

This step is where all the computational intense operations are performed. The figure \ref{fig:sim} demonstrates the operations 

\begin{algorithm}[H]
 \KwData{deltam}
 \KwResult{how to write algorithm with \LaTeX2e }
 initialization\;
 \While{$b_{eff} < 1.0e^{-9}$}{
  Runge and Kutta 4th\;
  \For{$i = 1; i <= 4; i+=1$ }{  
  sdex $\gets$ crossProduct(deltam, mag); calculate cross product \\
  FDTD with boundary condition\\
  laplacian $\gets$ laplacianXYBoundary(deltam);\\
  evaluate Zhang-Li model \\
  solveZhangLi  $\gets$ solveZhang(sfrelax, sdex, laplacian, sm); \\
  
  RK4 evaluation \\
  rkterm(i) $\gets$  rktime(i, solveZhangLi, dt); \\
  
  \eIf {$i == 4$}{
    deltam $\gets$ rk4(solveZhangLi, temp, dt, rkterm(1), rkterm(2), rkterm(3), rkterm(4))
    }{
	deltam $\gets$ rk4(solveZhanLi, temp, dt)\\
   }
  evaluate RK4 term\\
  deltam $\gets$ rk4(solveZhanLi, temp, dt) \\
  sfrelax $\gets$ relaxation(deltam, tau) \\
  
  \If {$i == 4$}{
    temp $\gets$ copy(rkterm(4));\\
   }
	
  }
 }
 \caption{Runge and Kutta 4th integration implementation}
\end{algorithm}

\subsubsection{Finite differences in the time domain}

The finite differences method requires the domain of interest to be broken down into small regions. Such a subdivision of space is known as a mesh or grid, cell division is explained in the table \ref{tab:mesh}, whose implementation in CUDA threads and blocks are done with \ref{tab:cuda} and \ref{tab:threads}. Based on the equations from chapter three \ref{eq:nn}. The first and second derivate are based on seconds nearest neighbors expands are calculated. The base idea is showed on the figure	 \label{fig:laplacian}. The equation \ref{eq:nn} is evaluated for three coordinates x, y and z.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.6\textwidth]{Figures/laplacian.png}
		\rule{35em}{0.2pt}
	\caption[Laplacian block calculation]{Laplacian XY block calculation and boundaries condition}
	\label{fig:laplacian}
\end{figure}

As the Figure \ref{fig:laplacian} demonstrate the calculation the nearest neighbors expansion by evaluation a neighborhood of $[-2, 2]$ values in the x direction. In addition the index begins at $i = 2$ and finishes at $NX + 2$. The same occurs for the laplacian in the Y direction, however it begins at $j = 2$ and finish at $NY -2$.  The implementation \ref{lst:lpay} uses the equation from chapter 3 \ref{eq:nn} as previous mention. The calculation \ref{lst:lpay} is done for each three x, y and z coordinates. 

\begin{lstlisting}[language=C++, label={lst:lpay}, caption={Laplacian X using global memory}]
int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
int j = blockIdx.y * blockDim.y + threadIdx.y;
// map the two 2D indices to a single linear, 1D index
int idx = j * grid_width + i;

lapy[idx] = - deltam[idx + 2] / 12.0 + 4.0 * deltam[idx + 1] / 3.0
			  - 5.0 * deltam[idx] / 2.0
			  - deltam[idx - 2] / 12.0 + 4.0 * deltam[idx - 1] / 3.0;	
\end{lstlisting}


However the calculation of the nearest neighbors only work for values points inside  inside a grid of [-2, 2] in x, y, and z. In addition we need to calculate the boundaries values for those values that are in the edge of the grid, such as the beginning points and the end points. To solve the boundaries issue we use two square matrices matrices from chapter three, The equation \ref{eq:matrix3} for boundaries at $j = 0$ and $j = NY - 1$ for all values in the i cell, and the equation \ref{eq:matrix4} for the condition $j = 1$ and $j = NY - 2$ for all i values. The code \ref{lst:lap} demonstrate how the implementation was done, to achieve the correct values first the laplacian in the y direction is calculated, then the boundary condition, and finally the in the x direction. 

\begin{lstlisting}[language=C++, label={lst:lap}, caption={Evaluation of Laplacian X, Y with boundary condition}]
__global__ void glaplaciany(...){} //Compute laplacian in Y direction

__global__ void glaplacianBoundaries(...){
    if (i > 1 && i < NX + 2 && j == 0){
     	// Update Laplacian Boundaries Equation 3.10
    }
    else if (i > 1 && i < NX + 2 && j == 1){
  		// Update Laplacian Boundaries Equation 3.9
    }
    else if (i > 1 && i < NX + 2 && j == NY - 2){
        // Update Laplacian Boundaries Equation 3.9
    }
    else if (i > 1 && i < NX + 2 && j == NY - 1){
        // Update Laplacian Boundaries Equation 3.10
    }
}
__global__ void glaplacianx(...){} //Compute laplacian in X direction
\end{lstlisting}

The three CUDA kernels \textit{glaplaciany}, \textit{glaplacianx} and \textit{glaplaciany} are evaluated for each x, y and z coordinate. In addition the three coordinate sum up to 172,800 cell points to be calculated.

\subsubsection{Zhang and Li Model}

The equation of Zhang - Li Model \ref{eq:zhang} is solved using the following following code \ref{lst:rkcuda}. This function is used for the \ref{eq:rk4} integration equation. The first term of the equation \textit{sfrelax} is calculated in the in the relaxation process at the end of the RK4 integration process \ref{fig:sim}. The term \textit{sdex} is evaluated once at the beginning the RK term integration, in the Cross Product. The matrix \textit{sm} is only calculated once at the initial calculations process, in the kernel \textit{gsource}. Finally the lapl matrix is evaluated in the Laplacian X, Y and boundary condition. In addition this process is the most computational expensive  operation.

\begin{lstlisting}[language=C++, label={lst:zhangcuda}, caption={Runge and Kutta 4th Terms}]
		sfrelax[idx] = -deltam[idx] / tau_sf;
		sdex[idx] = -(deltam[idx] * m[index] - deltam[idx] * m[idx]) / tau_sd;
		
		//Evaluate Zhang - Li Method
        solveZhangLi[idx] = sfrelax[idx] + sdex[idx] + lapl[idx] - sm[idx];
\end{lstlisting}


The output  \textit{solveZhangLi} of the Zhang - Li Model evaluation \ref{lst:zhangcuda}  (last term), past to the calculation of the i term of the Runge Kutta Integration.

\subsubsection{Runge and Kutta}
 
  Intuitively the equation \ref{eq:rk4} implementation code is done by using 4 CUDA kernels, each kernel calculates the step of the integration. The first part of \ref{lst:rkcuda} computes the Runge and Kutta's 1st, 2nd, and 3rd  term \ref{eq:rksplit}. The final fourth term, is the sum of the previous 3 terms, the last two lines of code in \ref{lst:rkcuda} show the calculation.

\begin{lstlisting}[language=C++, label={lst:rkcuda}, caption={Runge and Kutta 4th Terms}]
     rk1[idx] = dt * deltam[idx];  // Terms k1, k2, k3
     deltam[idx] = temp[idx] + 0.5 * rk1[idx];
     
     rk4[idx] = dt * deltam[idx]; //final k4
     deltam[idx] = temp[idx] + (rk1[idx] + 2.0 * (rk2[idx] + rk3[idx])
                                  + rk4[idx]) / 6.0;
\end{lstlisting}


\subsubsection{Final evaluation}

The integration can be sum up as two for cycles. The first \textit{for} calculating the RK4 and the second one each x, y, and z coordinate \ref{lst:rk4}.

\begin{lstlisting}[language=C++, label={lst:rk4}, caption={Summarize of Runge and Kutta Integration}]	

for(int term = 0; term < 4; term++)
	for(int coord = 0; coord < 3; coord++)
    	gsd_exchange<<<blocks, threads>>>(term, coord);
    	glaplacianx<<<blocks, threads>>>(term, coord);
    	glaplacianyboundaries<<<blocks, threads>>>(term, coord);
    	glaplaciany<<<blocks, threads>>>(term, coord);
    	gsolution<<<blocks, threads >>>(term, coord);
    	gterm_RK4<<<blocks, threads >>>(term, coord);
}
\end{lstlisting}


\subsection{Calculate effective beta}

The final calculation of the simulation, which calculates the effective beta. The following kernels are launched only when the RK4 integration is done, for this implementations is 50,000 iterations of RK4. \ref{fig:flow}

\begin{lstlisting}[language=C++, label={lst:beta}, caption={Calculate effective beta}]	
    gm_x_sm << <blocks, threads >> >(...); //Calculate
    gu_eff << <blocks, threads >> >(...);  //Calculate 
    gu_eff_beta_eff << <blocks, threads >> >(...); 	//Calculate
    gbeta_eff << <blocks, threads >> >(..); 	//Calculate
    gbeta_diff << <blocks, threads >> >(...);  //Calculate             
\end{lstlisting}


The final step of the simulation is calculating on the host the single value beta variation.


Note that the execution time can generally be reduced significantly by decreasing the amount of converges of the effective bete. However, this can result in wrong data.

\section{Validation}

Once obtain the results from the simulation, the results are written into two separated data files; .eff and .spin. depending of the configuration of the application is possible to obtain the uVW or the ATWpm. Because CUDA framework is highly parallel system is fairly easy to obtain erroneous data from the calculations, even setting up the threads per block incorrectly is possible to get data set that is wrong, or results that don't diverge. When making changes to the code, its is necessary to  validate the results, to continue to make changes.

The validation is done by checking the output of the simulation with a valid data set, the output of the validation application tells us the error factor of the current data with the valid set. So for each data set there is a threshold value, that can tell if the that is close enough to the results. A example of the validation performed.
According to our results, new code shouldn't produce errors in the $*$-spin.dat data 
greater than 7.0e-17, in other words valid code don't lead to differences greater than
the precision expected from computations with double precision 1.0e-16 in the case of
eff data the errors are in the order of 1.0e-11 and no greater than 6e-11. For the diffuse beta variation the precision expected to be within the double precision range of 1e-16. 

For example the validation test are done with the following output values.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | l |}
\hline
Data set & Simulation time & Diffuse beta  \\
\hline
upVW magnetization & 377590.3ms & 4.848728452719814e-02 \\
\hline
ATWpm magnetization & 377409.2ms & 4.054674178687585e-02 \\
\hline
\end{tabular}
\caption{Calculation results}
\label{tab:results}
\end{table}


\vspace{3.3em}

To conclude, the simulating at its core uses the RK4 for integration which uses as function the equation of Zhang-li \ref{eq:zhang}. In addition to solve such differential equations the FDTD method is used. As we can expected the simulation is computational expensive, because the RK4 evaluates fourth times the Zhang-Li equation. Moreoever,



