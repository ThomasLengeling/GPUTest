


A simple example of a vector addition to show the comparison between the GPU and CPU, input, two list of number which is sum up each corresponded element to produce a final output with the addition of both list. Figure \ref{fig:sum} shows this process. \cite{example}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{Figures/sum.png}
		\rule{35em}{0.3pt}
	\caption[Vector Addition Example]{Simple Vector Addition Example}
	\label{fig:sum}
\end{figure}

\subsubsection{CPU Code}

This first example illustrates the CPU code executed in a single thread. The code is straight forward to understand. First create the memory for each array, A, B and C with size N. Then calculate the sum of the two vectors with the function \textit{add}. As we can see in the function \textit{add}, we use the while loop to go through each element of the arrays A and B, which are added into a single array C.

\begin{lstlisting}[language=C++, caption={CPU Vector Addition}]
#include <iostream>
#define N 100

void add( int *a, int *b, int *c );

int main()
{
    int A[N], B[N], C[N];
    
    //fill the arrays with values
    for(int i = 0; i < N; i++){
        A[i] = 1; B[i] = i;
    }
    
    add(A, B, C);
    
    //Display the results
    for (int i = 0; i < N; i++)
        std::cout << A[i] << ", " << B[i] << ", " << C[i] << std::endl;
    
    return 0;
}

void add( int * A, int * B, int * C )
{
    int index = 0;
    
    //go through each index of the arrays and make the operation
    while(index < N){
        C[index] = A[index] + B[index];
        index++;
    }
}

\end{lstlisting}

We can notice if we set N to be a large number, the function \textit{add} could take a large amount of time to execute. But the example only illustrates the used of the CPU as a single core, however nowadays CPUs commonly have around 4-8 cores. To be able to execute the previous code on all the cores available in the CPU, threads are needed to be implemented. But you would need reasonable amount of code and debugging to make that happen. Also is a complicated task to schedule all the threads in the CPU. 

\subsubsection{GPU Code}

We can accomplish the same operation very similar in the GPU with CUDA. First create CPU and GPU memory, with there corresponded code. Send the CPU memory to the device, make calculations on the highly parallel GPU, finally return the results the CPU.

\begin{lstlisting}[language=C++, caption={GPU Vector Addition}]
#include <iostream>
#define N 100

// CUDA KERNEL
__global__ void add( int *a, int *b, int *c );

int main()
{
    int a[N], b[N], c[N];
    int *dev_a, *dev_b, *dev_c;

    // allocate the memory on the GPU
    cudaMalloc( (void**)&dev_a, N * sizeof(int) ) );
    cudaMalloc( (void**)&dev_b, N * sizeof(int) ) );
    cudaMalloc( (void**)&dev_c, N * sizeof(int) ) );
    
    //allocate the memory on the CPU
    for(int i = 0; i < N; i++){
        A[i] = 1; B[i] = i;
    }
    
    //calculate the vector addition in the GPU
    add<<<N,1>>>( dev_a, dev_b, dev_c );
    
    //copy back the result from the GPU to the CPU 
    cudaMemcpy( c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost ) );
     
    //Display the results
    for (int i = 0; i < N; i++)
        std::cout << A[i] << ", " << B[i] << ", " << C[i] << std::endl;
    
    cudaFree( dev_a );
    cudaFree( dev_b );
    cudaFree( dev_c );
    
    return 0;
}
\end{lstlisting}

The function \textit{cudaMalloc} and \textit{cudaFree} are very similar to the C functions \textit{malloc} and \textit{fee}, which allocating memory and deleting memory respectively.

CUDA automatically spams the threads to it correspondent block, so we only need to access the index of the block and pass it to the index arrays. To parallel code will stop in-till the block index reaches the number of elements of the arrays, N.

\begin{lstlisting}[language=C++, caption={GPU Vector Addition}]
void add( int * A, int * B, int * C )
{
    // handle the data at this index if (tid < N)
    int index = blockIdx.x; 
    if(index < N)
        c[index] = a[index] + b[index];
}
\end{lstlisting}

The biggest difference between the CPU code and the GPU code is how threads are managed within the process. The CPU code has only one thread, thrust one loop, however if we want to expand the CPU code to multiples threads, extra loops are required for each additional thread. Based on this idea, the GPU code is launched to every threads accessible by the device chip.

\vspace{3.2em}

hus, also the magnetic processes in a racetrack memory and gyrating magnetic vortices driven by spin-transfer torque  can be described.


This has simulated research into domain wall (DW) dynamics, particularly those resulting from interactions with current passing through the DW via the phenomenon of spin momentum transfer (SMT) \cite{handbookspin}.


//beta

\subsection{Effective Beta}

We show that and effective non-adiabatic parameter $\beta_{diff}$ dependent on the DW structure, provides 
\begin{equation} \label{eq:beta}
(\beta \upsilon)^{*} = \frac{\delta\vec{m} \cdot \partial_x \vec{m}}{\tau_{sd} \| \partial_x \vec{M} \|^2}
\end{equation}


-----include fill final calculation --------------

\subsection{Procedure}

---------final small procedure test solving

The procedure is (i) compute the non-equilibrium spin density $\delta \vec{m}$ with the DW at rest, solve equation \ref{eq:zhang} to convergence or directly its time-independent version. (ii) compute the $\beta_{diff}$ distribution from equation \ref{eq:beta} and finally compute $\beta_{diff}$ by averaging with the $| \partial_x \vec{m}|^2$ weight function.

%The basic structure is computational solve rungge and kutta of for other.